

# 人工智能知识库

##  一>基础

###  1-人工智能的初步认识

####  **1.1 人工智能** 

人工智能（Artificial Intelligence）就是让机器具有人类的一样的智慧，有人类的视觉、听觉、阅读、写作等能力。人类大脑是经过了上亿年的进化才形成的复杂结构，但我们至今仍然没有完全了解其工作机理。虽然随着神经科学、认知心理学等学科的发展，人们对大脑的结构有了初步的了解，但对大脑的智能究竟是怎么产生的还是未知。我们并不理解大脑的运作原理，以及如何产生意识、情感、记忆等功能．因此，通过“复制”人脑来实现人工智能在目前阶段还不切实际。

人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。其本质是通过让计算机系统具备学习、推理、解决问题等类似人类智能的能力，来实现对复杂任务的自动化处理和优化。

![](image\1.png)

阿兰·图灵（Alan Turing） 在1950 年发表了一篇有着重要影响力的论文《Computing Machinery and Intelligence》，阐述一种“智能机器”的可能性．由于“智能”一词比较难以定义，他提出一个著名测试，后人称之为图灵测试。

图灵测试————“一个人在不接触对方的情况下，通过一种特殊的方式和对方进行一系列的问答．如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的”．图灵测试是促使人工智能从哲学探讨到科学研究的一个重要因素，引导了人工智能的很多研究方向．

![c4c39984c64a654a4957f7a3ba7e2c4c](image\c4c39984c64a654a4957f7a3ba7e2c4c.png)

目前，人工智能的主要领域大体上可以分为以下几个方面：
1.**感知**：模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感知和加工．主要研究领域包括语音信息处理和计算机视觉等．
2.**学习**：模拟人的学习能力，主要研究如何从样例或从与环境的交互中进行学习．主要研究领域包括监督学习、无监督学习和强化学习等．
3.**认知**：模拟人的认知能力，主要研究领域包括知识表示、自然语言理解、推理、规划、决策等

![fac63594763ead05208e9585b2f38ecf](image\fac63594763ead05208e9585b2f38ecf.png)

####  1.2 机器学习

机器学习 是指从有限的观测数据中学习 （ 或 “ 猜 测”） 出具有一般性的规律 ， 并利用这些规律对未知数据进行预测的方法 ． 机器学习是人工智能的一个重要分支，并逐渐成为推动人工智能发展的关键因素．

传统的机器学习主要关注如何学习一个预测模型．一般需要首先将数据表示为一组特征，特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作浅层学习．

浅层学习的一个重要特点是**不涉及特征学习**，其特征主要靠人工经验或特征转换方法来抽取．
在实际任务中使用机器学习模型一般会包含以下几个

数据预处理： 对原始形式的数据进行初步的数据清理（例如去掉一些冗余的数据特征等）和加工（对相关数值进行缩放和归一化等），并构建成可用于训练机器学习模型的数据集．

特征提取： 从数据的原始特征中提取一些对特定机器学习任务有用的价值高的特征．比如在图像分类中提取边缘特征、尺度不变特征、变换特征，在文本分类任务中去除停用词等．

特征转换： 对特征进行进一步的加工，比如降维和升维． 降维包括特征 抽取和特征选择两种途径．常用的 特征转换方法有主成分分析、线性判别分析

机器学习是让计算机像人一样学习和进步的技术，以下将从其概念、学习方式、应用场景等方面进行通俗易懂的简述：

 • 概念：可以把机器学习想象成一个特别聪明的学生，它能从大量的数据中学习知识和规律。比如，给它很多张猫和狗的图片，它看了之后就能自己总结出猫和狗的特点，下次再看到新的猫或狗的图片，就能准确地分辨出来。 

• 学习方式 

◦ 有监督学习：好比有个老师在旁边指导。给计算机一些带答案的数据，例如告诉它一些房子的面积、房间数量等信息，以及这些房子对应的价格，让计算机学习面积、房间数量和价格之间的关系，之后它就能根据新的房子面积和房间数量等信息，预测出大概的价格。

 ◦ 无监督学习：就像让学生自己去探索。给计算机一堆数据，没有告诉它这些数据应该怎么分类，让计算机自己去发现数据中的规律和特点，比如把一群人按照消费习惯分成不同的群体。 

◦ 强化学习：有点像让计算机做游戏闯关。计算机通过不断尝试不同的行为，根据每次行为得到的奖励或者惩罚来调整自己的策略，比如让机器人学习走迷宫，它每走对一步就给它一个小奖励，走错了就给它一个小惩罚，慢慢地机器人就能学会最快走出迷宫的方法。  

 • 应用场景

 ◦ 图像识别：现在的很多手机都有面部识别解锁功能，这就是机器学习在起作用，它能识别出你的脸和其他人的脸不一样，从而判断是不是你本人来解锁手机。 

◦ 语音助手：像我们用的小爱同学、Siri 等，它们能听懂我们说的话并帮我们做事情，也是因为机器学习让它们学会了理解人类的语言。

◦ 推荐系统：我们在淘宝、抖音上看到的各种推荐商品和视频，也是机器学习根据我们之前的浏览、购买等行为数据，分析出我们可能喜欢什么，然后给我们推荐的。

####  1.3 深度学习

​      为了让机器学到更好的特性，需要构建具有一定“深度”的模型，并通过学习算法来让模型自动学习出好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测模型的准确率．所谓“深度”是指原始数据进行非线性特征转换的次数．如果把一个表示学习系统看作一个有向图结构，深度也可以看作从输入节点到输出节点所经过的最长路径的长度．这样我们就需要一种学习方法可以从数据中学习一个“深度模型”，这就是深度学习．深度学习是机器学习的一个子问题，其主要目的是从数据中自动学习到有效的特征表示．

深度学习是机器学习的一个分支领域，下面将从定义、原理、网络结构、应用等方面进行简单介绍： - 

**定义**：深度学习就像给计算机装上了一个超级大脑，让它能够自动地从大量数据中学习到非常复杂的模式和特征。它利用神经网络这种特殊的数学模型，模拟人类大脑神经元之间的信息传递方式，让计算机能够像人类一样对数据进行理解、分析和决策。 

**原理**    

**数据输入**：把大量的数据，如图像、语音、文字等，转化成计算机能够理解的数字形式，输入到深度学习模型中。   

**特征提取**：模型会自**动地从输入数据中提取各种特征**。例如在处理图像时，它会先识别出线条、颜色等简单特征，然后逐渐组合这些特征，识别出更复杂的物体部分，最后识别出整个物体。    

**模型训练**：通过不断地调整模型中的参数，使得模型的输出结果尽可能地接近真实的结果。这个过程就像我们学习新知识一样，通过**不断地练习和纠正错误，来提高自己的能力。**    

**预测和决策**：经过训练的模型可以对新的、未见过的数据进行预测和决策。比如在图像识别中，它可以判断一张新的图片里是什么物体；在语音识别中，它可以将听到的语音转换成文字。

**常见网络结构**    

- **卷积神经网络（CNN）**：特别擅长处理**图像数据**，它通过卷积层、池化层等结构，自动提取图像的特征，大大减少了计算量，提高了识别效率，在图像识别、目标检测等领域应用广泛。    

- **循环神经网络（RNN）**：适合处理**序列数据**，如语音、文字等。它能够记住之前的信息，并利用这些信息来处理当前的任务，比如在语言翻译中，它可以根据前面已经翻译的内容，更好地理解和翻译后面的句子。    

  **长短时记忆网络（LSTM）**：是RNN的一种改进版本，能够更好地处理**长序列**中的信息，解决了RNN在处理长期依赖问题时的困难，在**自然语言处理、时间序列预测**等方面表现出色。 - 

  **应用领域**    - 

  **医疗领域**：可以帮助医生诊断疾病，通过分析大量的医学影像，如X光、CT等，识别出病变区域，辅助医生进行更准确的判断。    - 

  **自动驾驶**：让汽车能够识别道路、交通标志和其他车辆、行人等，做出正确的驾驶决策，实现自动驾驶功能。    - 

  **金融领域**：用于风险评估、股票预测等。通过分析大量的金融数据，预测市场趋势，帮助投资者做出决策。 学习深度学习需要掌握以下内容：

1、学习深度学习，首先要掌握线性代数、概率论和数理统计等基础知识，这样才能理解深度学习算法的原理；

2、学习深度学习，要掌握神经网络、卷积神经网络（CNN）、循环神经网络（RNN）、深度强化学习等深度学习模型；

3、学习深度学习，要掌握深度学习框架，如TensorFlow、Pytorch等；

4、学习深度学习，要掌握数据预处理、模型训练和评估等技术。

### 2-一元一次函数感知器

####  1定义

一元一次函数感知器是一种简单的神经网络，也可以叫做rosenblatt感知器，其中只有一个输入和一个输出。它使用一个简单的线性函数来模拟人类神经系统的行为，该函数是由一个系数和一个偏置值组成的，系数用于控制输入对输出的影响，而偏置值用于控制输出的整体大小。rosenblatt感知器可以用来解决简单的分类和回归问题。

![a219bfa0569064600e29c54e0b7d74c3](image\a219bfa0569064600e29c54e0b7d74c3.png)

假设有一个数据集包含一组输入x和输出y，其中x可以看作是一个实数，而y是一个变量。一元一次函数感知器可以通过调整个参数k来拟合这个数据集，并用它来预测新的输入。具体来说，它将定义一个函数，用于将输入x映射为输出y，通常使用一元一次函数，其中k是一个可调参数。通过拟合数据集中的输入和输出，可以找到一个合适的k值，用来预测新的输入x的输出y。

####  2.简单的分类问题代码：

```
def perceptron(x):
    w = 0.5  # 初始化权重
    b = 0.1  # 初始化偏置
    y_hat = w * x + b  # 计算预测值
    if y_hat >= 0.0:
        return 1
    else:
        return 0
 
 
# 调用perceptron()函数
prediction = perceptron(5)
 
print('预测值为：', prediction)
```

以下是简单的回归问题与参数更新：

```
import numpy as np
# X 代表输入, Y 代表输出
X = np.array([[1,2], [3,4], [5,6]])
Y = np.array([2,4,6])
 
# 定义权重向量
weights = np.array([0.5, 0.5])
 
# 定义偏置项
bias = 1
 
# 权重与偏置项输出函数
outputs = np.dot(X, weights) + bias
 
# 计算损失函数
error = Y - outputs
 
# 更新权重向量参数和偏置项参数
weights = weights + 0.1 * np.dot(X.T, error)
bias = bias + 0.1 * np.sum(error)
 
# print the results
print("Updated weights:", weights)
print("Updated bias:", bias)
```

一元一次函数感知器可以应用于各种分类问题，如文档分类、图像分类、视频分类、文本分类等，也可以应用于机器学习、人工智能等领域，用于实现数据的分类和分析。另外，一元一次函数感知器还可以用于控制系统中，用来实现模式识别、参数估计、控制调节等。

####  3.函数感知器，训练过程，代码：

```
import numpy as np
 
# 定义sigmoid函数
def sigmoid(x):
    return 1/(1 + np.exp(-x))
 
# 定义损失函数
def cost(x, y, w):
    return np.sum(np.power((sigmoid(np.dot(x, w)) - y), 2))
 
# 定义一元一次函数感知器
def linear_perceptron(x, y, w, learning_rate, epochs):
    for i in range(epochs):
        z = np.dot(x, w)
        a = sigmoid(z)
        e = a - y
        w = w - learning_rate * np.dot(x.T, e)
        cost_value = cost(x, y, w)
        print("Epoch %d, cost %f" % (i, cost_value))
    return w
 
# 设置超参数
learning_rate = 0.01
epochs = 500
 
# 构建输入输出数据
x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])
 
# 初始化权重
w = np.array([0.1, 0.1])
 
# 训练感知器
w = linear_perceptron(x, y, w, learning_rate, epochs)
 
# 打印训练结果
print("Final weight:", w)
```

总之，一元一次函数感知器是一种简单的神经网络模型，它只有一个神经元，可以用来学习线性函数。它使用一个激活函数来输出一个值，可以模拟任何线性函数的行为。

它的优点是结构简单，参数数量也很少，而且可以用于解决一些简单的分类问题。

它的缺点也很明显：它只能适用于简单的线性函数，而不能解决复杂的非线性问题；另外，它在训练过程中的收敛可能会受到影响。

`sigmoid`函数是一种常用的激活函数，其公式为 *σ*(*x*)=1/(1+e−x)。它将输入值映射到 (0,1) 区间，常用于二分类问题中，输出值可以解释为概率。

```
def sigmoid(x):
    return 1/(1 + np.exp(-x))
```

均方误差（MSE）损失函数。损失函数用于衡量模型预测值与真实值之间的差异，我们的目标是最小化这个损失函数。

```
def cost(x, y, w):
    return np.sum(np.power((sigmoid(np.dot(x, w)) - y), 2))
```

梯度下降算法更新权重

```
 w = w - learning_rate * np.dot(x.T, e)
```

该代码的核心运算原理是梯度下降算法。梯度下降是一种迭代优化算法，用于寻找函数的最小值。在这个问题中，我们的目标是最小化损失函数

### 3-方差损失函数

#### 1.概念

方差损失函数是指计算模型在训练集上的表现和在测试集上的表现之间的差异。它衡量着模型的**拟合能力**，也可以用来衡量模型的**泛化能力**。方差损失函数可以帮助我们找到最优模型，从而提高模型的泛化能力。

方差损失函数也是一种常见的回归损失函数，它的原理是，用**算法预测值减真实值，计算差值方差，方差越高，准确度越低**。因此，当算法优化时，我们可以优化方差损失函数，从而提高预测准确率。

损失函数一般有方差损失函数，绝对值损失函数，交叉熵损失函数.

方差损失函数将模型预测值与真实值之间的误差衡量为两者之间的方差，因此可以表示模型预测能力的稳定性，常用于回归问题。

而绝对值损失函数，其实质是将模型预测值与真实值之间的误差衡量为两者之间的绝对值，在计算过程中不方便.

交叉熵损失函数一般用于分类问题，对于回归问题我们更倾向于选择方差损失函数。

方差损失函数可以用来描述一个组合的偏离度，从而估计模型的不确定性。比如说，一家公司有三个部门，A、B和C，他们的收入分别是100万、200万和300万。公司的总收入是600万，每个部门平均200万收入，我们可以用方差损失函数来计算各部门收入的偏离度：

![eq](image\eq.png)

可以看出，总收入为600万的情况下，各部门收入的偏离度为200,000。这里，方差损失函数可以用来衡量各部门收入的不确定性，从而帮助我们更好地评估模型的表现。

![Snipaste_2025-03-12_15-22-50](image\Snipaste_2025-03-12_15-22-50.png)

####  2.方差损失函数与 交叉熵损失函数的python代码样例：

```
# 引入库
import numpy as np
 
# 定义均方差损失函数
def loss_mse(y_true, y_pred):
    mse_loss = np.mean(np.power(y_true - y_pred, 2))
 
    return mse_loss
    # 构造 y_true 和 y_pred
 
y_true = np.array([1,2,3,4,5])
y_pred = np.array([1.5, 2, 3, 4.5, 5])
 
# 计算和输出均方差损失函数
mse_loss = loss_mse(y_true, y_pred)
 
print('均方差损失函数：', mse_loss)
 
# 定义交叉熵损失函数
def loss_ce(y_true, y_pred):
 
  # 计算真实标签与预测标签之间的交叉熵
  ce_loss = -np.mean(np.sum(np.multiply(y_true, np.log(y_pred)), axis=1))
  return ce_loss
 
# 构造 y_true 和 y_pred
y_true = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])
y_pred = np.array([[0.6, 0.2, 0.2], [0.2, 0.3, 0.5], [0.3, 0.3, 0.4]])
 
# 计算和输出交叉熵损失函数
ce_loss = loss_ce(y_true, y_pred) 
print('交叉熵损失函数：',ce_loss)
```

### 4-梯度下降和反向传播

#### 4.1梯度下降

梯度下降（Gradient Descent）是一种最优化算法，用于求解最小化损失函数的参数值。梯度下降的基本思想是：根据当前参数的梯度，沿着梯度的反方向移动参数，从而找到损失函数的最小值。梯度下降在机器学习和深度学习中被广泛应用，用于优化模型参数。

![01a383806b1e789b311b1784eed84616](image\01a383806b1e789b311b1784eed84616.png)

梯度下降的**原理**可以用简单的话来概括：在一个高维空间中，梯度下降就是从一个点出发，根据损失函数的导数，沿着损失函数下降最快的方向，一步步朝着最优解前进，最终到达最优解处。

![5d8e28b9e2552de0dba1a39bf2e7fca2](image\5d8e28b9e2552de0dba1a39bf2e7fca2.png)

举个生动的例子：
想象一个人在爬山，需要到达山顶，而他所在的位置却在山脚下，他需要不断前进才能到达山顶。我们把山顶看做反向过来的损失函数的最小值，而山脚下则是损失函数的初始值，每次前进就相当于梯度下降中的一次迭代，人会根据山体的坡度不断调整自己的行进方向，朝着山顶的方向前进，最终到达山顶。

**梯度的方向就是函数之变化最快的方向**

#### 4.2反向传播

反向传播（Backpropagation）是一种形式化的梯度下降算法，用于训练神经网络。

反向传播的基本思想是：用输出层的梯度反向传播到隐藏层，以计算每一层的梯度，并将梯度更新到参数，以期望找到损失函数的最小值。反向传播结合了梯度下降算法和负梯度方向的求解。

反向传播的原理是：在神经网络的输出层向输入层依次**反向传播误差，在每层计算误差对每个参数的偏导，并通过梯度下降法更新权重参数，以期望最小化误差，从而提高模型的准确性**。具体而言，它有以下几个步骤：
1. 将训练数据输入到神经网络，并计算输出；
2. 计算输出层的误差；
3. 将误差反向传播到每一层，并计算每一层的误差对每个参数的偏导；
4. 更新参数，使误差最小化；
5. 重复执行上述步骤，直到模型训练完毕。

反向传播就像一个探索迷宫的小朋友，反向传播就是从迷宫的出口开始，一步一步回溯，找到最初进入迷宫的位置，也就是迷宫的入口。

在深度学习中，**反向传播的过程就是从输出层开始，回溯将每一层的误差反向传播到输入层，从而更新权重，以最小化误差**。
1、梯度下降案例:

```
# 构造数据
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]
# 模型参数
w = 1.0  # 初始化w
# 模型定义
def forward(x):
    return x * w
# 损失函数
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) ** 2
# 梯度
def gradient(x, y):  # d_loss/d_w
    return 2 * x * (x * w - y)
# 更新参数
def update():
    global w
    w = w - 0.01 * gradient(x_data[0], y_data[0])
# 打印结果
for epoch in range(100):
    for x_val, y_val in zip(x_data, y_data):
        grad = gradient(x_val, y_val)
        w = w - 0.01 * grad
        print("\tgrad: ", x_val, y_val, round(grad, 2))
        l = loss(x_val, y_val)
    print("progress:", epoch, "w=", round(w, 2), "loss=", round(l, 2))
```

2.反向传播

```
import torch
# 定义一个简单的线性模型
model = torch.nn.Linear(1, 1)
# 定义优化器
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# 定义损失函数
criterion = torch.nn.MSELoss()
# 训练数据
x_data = torch.tensor([[1.0], [2.0], [3.0]])
y_data = torch.tensor([[2.0], [4.0], [6.0]])
# 训练模型
for epoch in range(1000):
    # 计算模型输出
    y_pred = model(x_data)
    # 计算损失
    loss = criterion(y_pred, y_data)
    # 清空梯度
    optimizer.zero_grad()
    # 反向传播
    loss.backward()
    # 更新参数
    optimizer.step()
    # 打印训练结果
    if (epoch+1) % 20 == 0:
        print(f'epoch {epoch+1}: w = {model.weight.item():.3f}, loss = {loss.item():.8f}')
 
# 训练完成
w = model.weight.item()
print(f'Predict after training: f(5) = {5*w:.3f}')
 
#模型保存
torch.save(model.state_dict(), 'model.pth')
 
#模型加载
state_dict  = torch.load('model.pth')
model.load_state_dict(state_dict )
 
#模型预测
predict = model(torch.tensor([5.0]))
print(predict.detach().numpy()[0])
```

训练过程的通俗描述（一听就懂）：

当进行训练时，我们首先将模型给定的输入数据传递给模型，并计算出预测的输出值。然后，我们使用损失函数来计算出模型的预测值与正确的标签之间的差别，这也是模型的错误量。接下来，我们使用反向传播来计算出模型中参数的梯度，并使用优化器来更新参数以最小化损失函数。每次完成反向传播后，我们都会记录下当前损失值，以评估模型的优化情况。当损失值足够低时，我们就可以认为训练完成了。

以上简述了pytorch框架下训练线性模型的过程，清晰地描述了pytorch框架下模型训练的步骤，包括模型保存，模型加载，模型预测，全流程，掌握这个过程相当于pytorch框架学习了主心骨部分，大家可以尝试训练的过程。


```
from numpy import *

# 数据集大小 即20个数据点
m = 20
# x的坐标以及对应的矩阵
X0 = ones((m, 1))  # 生成一个m行1列的向量，也就是x0，全是1
X1 = arange(1, m+1).reshape(m, 1)  # 生成一个m行1列的向量，也就是x1，从1到m
X = hstack((X0, X1))  # 按照列堆叠形成数组，其实就是样本数据
# 对应的y坐标
Y = array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m, 1)
# 学习率
alpha = 0.01


# 定义代价函数
def cost_function(theta, X, Y):
    diff = dot(X, theta) - Y  # dot() 数组需要像矩阵那样相乘，就需要用到dot()
    return (1/(2*m)) * dot(diff.transpose(), diff)


# 定义代价函数对应的梯度函数
def gradient_function(theta, X, Y):
    diff = dot(X, theta) - Y
    return (1/m) * dot(X.transpose(), diff)


# 梯度下降迭代
def gradient_descent(X, Y, alpha):
    theta = array([1, 1]).reshape(2, 1)
    gradient = gradient_function(theta, X, Y)
    while not all(abs(gradient) <= 1e-5):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, Y)
    return theta


optimal = gradient_descent(X, Y, alpha)
print('optimal:', optimal)
print('cost function:', cost_function(optimal, X, Y)[0][0])


# 根据数据画出对应的图像
def plot(X, Y, theta):
    import matplotlib.pyplot as plt
    ax = plt.subplot(111)  # 这是我改的
    ax.scatter(X, Y, s=30, c="red", marker="s")
    plt.xlabel("X")
    plt.ylabel("Y")
    x = arange(0, 21, 0.2)  # x的范围
    y = theta[0] + theta[1]*x
    ax.plot(x, y)
    plt.show()


plot(X1, Y, optimal)

```

### 5-激活函数

#### 5.1概念

激活函数（Activation Function）是神经网络中的一个重要组成部分，它是把一个神经元的输出映射到另一个值域，它可以将输入信号转换为非线性输出信号。激活函数是一种特殊的函数，它通**过计算输入信号的线性组合，然后根据某种特定的函数来生成输出信号**。激活函数的主要作用是对输入的特征进行组合，以便模型能够学习输入特征之间的非线性关系。

我们可以将激活函数比喻为一个按钮，每次按下按钮，就会发出一个信号，以此来激活神经网络中的神经元和层。当神经元接收到足够的激活信号时，它才会发挥它的功能，产生出有用的信息。

激活函数也可以比喻成一个水池，如果水位超过一定的高度，水就会从溢出口溢出，激活函数就是这样的溢出口。

#### 5.2功能

（1）非线性化
激活函数可以将线性的输入信号转换为非线性的输出信号，从而使神经网络具有非线性的学习能力，使其能够拟合更复杂的函数。
（2）抑制过拟合
激活函数可以有效地抑制过拟合，因为它们可以限制信号的流动，使得输出不会过大或过小。
（3）促进反向传播
激活函数可以促进反向传播，因为它们可以把输入信号转换为可以进行反向传播的输出信号。

####  5.3常见的激活函数

![Snipaste_2025-03-13_10-33-37](image\Snipaste_2025-03-13_10-33-37.png)

![Snipaste_2025-03-13_10-36-00](image\Snipaste_2025-03-13_10-36-00.png)

#### 5.4代码实现

```
import numpy as np
import matplotlib.pyplot as plt
# Sigmoid函数
def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))
x = np.linspace(-10, 10, 100)
y = sigmoid(x)
plt.plot(x, y)
plt.show()
 
 
# Tanh函数
def tanh(x):
    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
x = np.linspace(-5, 5, 100)
y = tanh(x)
plt.plot(x, y)
plt.show()
 
 
# ReLU函数
def relu(x):
    return np.maximum(0, x)
x = np.linspace(-10, 10, 100)
y = relu(x)
plt.plot(x, y)
plt.show()
 
 
# Leaky ReLU函数
def leaky_relu(x, alpha = 0.05):
    return np.maximum(alpha * x, x)
x = np.linspace(-10, 10, 100)
y = leaky_relu(x)
plt.plot(x, y)
plt.show()
```

####  5.5总结

激活函数是神经网络中的一类非线性映射函数，它能够将输入数据转化为非线性形式，以便网络可以拟合训练数据中的非线性关系。常用的激活函数有sigmoid、tanh、ReLU、Leaky ReLU、Softmax等，每种激活函数都有不同的特性，应用于不同的神经网络结构，可以改善网络的性能。此外，激活函数还可以防止网络出现梯度消失或梯度爆炸等问题。

###  6-神经网络初步认识

####  **一、神经网络案例**

神经网络就像一个人，它可以从外部获取输入信息并将其转换为内部结构，以便在不同的情况下做出正确的决定。神经网络也像一个掌握语言的机器，它能够接受输入，模仿人类的学习方式，从而学习和记忆特定的输入和输出之间的关系。

例如，当一个人面对一棵大树时，他可以从外部视觉输入中获取信息，并基于这些信息来做出决定，例如攀爬树或不攀爬树。这就是神经网络所做的，它们可以收集外部信息，转换它们，并基于这些信息做出正确的决定或预测。

假设你正在教给一个神经网络英语语法，它可以接受单词的输入，比如“dog”，并输出正确的单复数形式，即“dogs”。它可以通过模仿人类学习的方式，记住单词和它们的正确形式之间的关系，并继续输出正确的单复数形式。

#### **二、神经网络仿生学**

生物神经网络的仿生学原理基于人类神经系统的结构和功能，其中包括具有学习和记忆能力的神经元突触，神经元细胞之间的连接，以及神经元细胞的反应和传递信号的机制。从这个角度来看，生物神经网络的仿生学原理包括以下几点：

1. 神经元/突触：生物神经网络仿生学原理的最基本元素是神经元/突触，即神经元细胞之间的连接。这些神经元/突触可以通过权重来表示神经元之间的连接强度。
2. 学习：神经网络可以通过学习来调整连接权重，以便能够更有效地完成特定任务。
3. 记忆：人类神经系统有一种能力，可以将已经学习过的信息储存，以便在未来可以更快地推断出结果。同样，生物神经网络也可以学习和记忆信息，从而更有效地完成特定任务。
4. 传递信号：人类神经系统可以通过传递神经元信号来传递信息，以便完成特定任务。同样，生物神经网络也可以传递神经元信号，从而完成特定任务

![5bd6ff89e9a57619ca10908edadcbf6b](image\5bd6ff89e9a57619ca10908edadcbf6b.png)

####  **三、神经网络具体结构：**

**神经网络的原理主要可以分为三个层次：输入层、隐含层和输出层。**
**1.输入层**：神经网络的输入层由输入单元组成，它们接收外部环境的输入信号，并将其转化为神经网络可以理解的信号。
**2.隐含层**：隐含层是神经网络的核心，它由多个神经元组成，它们可以接收输入层传来的信号，并对其进行加工处理，以便识别出有用的信息，并将其输出给输出层。
**3.输出层：**输出层由输出单元组成，它们从隐含层接收到的信号，并将其转化为结果输出，以便外部环境对其进行处理。
神经网络通过反复调整**连接权重**和**神经元参数**，以调整信号在各个神经元之间传递的强度，从而使神经网络能够实现复杂的任务，实现自动化处理信息的目标。

![](image\fe49fbaeb70a7a5f28aabb6f7efb4a51.png)

####  **四、神经网络的数学计算：**

下面举一个案例，神经网络有有3个输入，2个输出，1个隐藏层，隐藏层有4个神经元：

![0deb6b854362eff6b7b30ab36d94eb0a](image\0deb6b854362eff6b7b30ab36d94eb0a.png)



![Snipaste_2025-03-13_10-48-34](image\Snipaste_2025-03-13_10-48-34.png)

####  **五、神经网络的实现代码：**

神经网络有有3个输入，2个输出，1个隐藏层，隐藏层有4个神经元的代码实现

```
import numpy as np
# 输入层-隐藏层的权重
weights_input_hidden = np.array([[1, 2, 3, 4],
                                [2, 3, 4, 5],
                                [3, 4, 5, 6]])
# 隐藏层-输出层的权重
weights_hidden_output = np.array([[1, 2],
                                 [2, 3],
                                 [3, 4],
                                 [4, 5]])
# 输入值
input_data = np.array([2, 3, 4])
# 定义激活函数
def sigmoid(x):
    return 1.0/(1.0 + np.exp(-x))
# 计算隐藏层的输出
hidden_layer_input = np.dot(input_data, weights_input_hidden)
hidden_layer_output = sigmoid(hidden_layer_input)
# 计算输出层的输出
output_layer_input = np.dot(hidden_layer_output,
weights_hidden_output)
output = sigmoid(output_layer_input)
print(output)
```

六、神经网络涉及的内容 

学习神经网络的初步知识需要掌握以下几个方面的知识，这些之前课程有涉及一些： 

1.线性代数：神经网络中常用的矩阵运算，例如矩阵乘法，线性方程组的求解等都需要用到线性代数的知识。 

2.神经网络的基础概念：包括人工神经元、权重、激活函数、正则化、损失函数等。

3.梯度下降算法：神经网络的训练过程中，常常需要使用梯度下降算法来最小化损失函数。需要了解梯度下降算法的原理以及如何应用。 

4.神经网络的基本结构：需要了解神经网络的基本构成，包括输入层，隐藏层，输出层，权重，偏置等概念。

 5.前向传播算法：了解如何使用神经网络进行预测，包括使用权重和偏置计算输入到隐藏层的信号，以及使用激活函数对信号进行非线性变换。

6.反向传播算法：了解如何使用反向传播算法计算损失函数对权重和偏置的梯度，以便使用梯度下降算法进行训练。 

7.激活函数：了解常用的激活函数，包括 Sigmoid 函数，Tanh 函数，ReLU 函数，softmax函数等；

8.神经网络的架构：常用的神经网络架构，如卷积神经网络CNN、循环神经网络RNN、长短期记忆网络等LSTM。 

9.神经网络的应用：图像分类、语音识别、自然语言处理等。

10.神经网络的实现框架：TensorFlow、PyTorch、Paddle等。

### 7-高维空间的神经网络认识

####  **一、高维空间的神经网络概念**

高维空间的神经网络是一种特殊的深度学习模型，用于处理高维数据。它通过增加网络层数来处理高维数据，用于提取高维特征，解决复杂的机器学习问题，并且可以用于解决许多机器学习应用场景。高维空间的神经网络可以更好地捕捉数据的复杂性，从而更好地解决机器学习问题。

![042528fafbd2c734a6d7310c97c4cbf8](image\042528fafbd2c734a6d7310c97c4cbf8.png)

一个高维空间的神经网络就像是一个复杂的连续空间，里面充斥着各种各样的点，每一个点都有自己的位置和属性，它们可以相互之间的关联，比如一个点的属性可能会影响另一个点的属性，这样就形成了一个复杂的连续空间，被称为神经网络。在这个空间里，每个点都可以激活其他点，同时也可以被其他点所激活，形成一个复杂的互动关系，从而使整个空间变得更加复杂，并且能够记录下大量的信息。因此，这种高维空间的神经网络可以更好的理解和模拟复杂的现实世界，并且还能够帮助我们解决复杂的问题。

####  **二、高维空间神经网络的应用**

一个典型的高维空间神经网络案例应用是图像分类。图像分类任务中，输入数据是一张图像的像素，每个像素的值代表着图像的颜色，而这些像素的值在高维空间中代表了图像的特征。因此，使用神经网络对图像进行分类，就是利用高维空间中的特征对图像进行分类。

![309d96b0dea3192c6777943e16c48992](image\309d96b0dea3192c6777943e16c48992.png)

####  **三、高维空间神经网络的案例描述**

关于一个高维空间的非线性优化问题：例如，假设我们有一个n维的输入空间，其中每个维度代表一个特征。我们的任务是在这个n维空间中找到一个函数，它可以最好地拟合已知的数据点。
为了实现这一目标，我们可以使用一个神经网络，它由n个输入节点、m个隐藏层节点组成，以及一个输出节点。我们可以使用最小二乘法来优化神经网络的权重，使得神经网络能够最好地拟合已知的输入特征和输出结果。在这里，我们的目标是找到一组最优的权重参数，使得神经网络能够以最小的损失函数值来拟合已知的输入特征和输出结果。
有了这组最优的权重参数，我们就可以使用高维空间的非线性优化方法来求解这个问题。例如，我们可以使用genetic algorithms（遗传算法）、gradient descent（梯度下降法）等高维空间的最优化方法来求解这类问题。

![Snipaste_2025-03-13_11-04-20](image\Snipaste_2025-03-13_11-04-20.png)

####  **四、高维空间神经网络的代码：**

简单模型输入输出：

```
import numpy as np
import matplotlib.pyplot as plt
# 定义函数：激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
# 定义函数：梯度下降
def Gradient_Descent(x, y, theta, alpha, iterations):
    for i in range(iterations):
        grad = np.dot(x.T, (sigmoid(np.dot(x, theta)) - y))
        theta = theta - alpha * grad
    return theta
# 生成数据
X = np.random.randn(100, 2) # 生成100个样本，每个样本有2个特征
# 将标签设定为0-1分类
Y = np.array([0 if np.sum(x)<0 else 1 for x in X])
# 添加一列全为1的系数
X = np.c_[np.ones(X.shape[0]), X]
print(X)
# 初始化参数
theta = np.zeros(X.shape[1])
 
# 设定学习率和迭代次数
alpha = 0.01
iterations = 1000
# 调用梯度下降函数
theta = Gradient_Descent(X, Y, theta, alpha, iterations)
# 计算预测值
prediction = sigmoid(np.dot(X, theta))
 
#print(prediction)
 
# 画图查看结果
fig, ax = plt.subplots(1, 2, figsize=(15, 5))
ax[0].scatter(X[:, 1], X[:, 2], c=Y, cmap='viridis')
ax[0].set_xlabel('Feature 1')
ax[0].set_ylabel('Feature 2')
ax[1].scatter(X[:, 1], X[:, 2], c=prediction, cmap='plasma')
ax[1].set_xlabel('Feature 1')
ax[1].set_ylabel('Feature 2')
plt.show()
```

这段代码实现了一个简单的二分类问题的逻辑回归模型，使用梯度下降算法进行模型训练，并通过可视化展示了真实标签和预测概率的分布情况

 用pytorch框架构建一个三个输入变量和三个输出变量的神经网络模型：

```
import torch
import torch.nn as nn
 
n_inputs = 3
n_hidden = 3
n_outputs = 3
 
class NeuralNet(nn.Module):
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(n_inputs, n_hidden)
        self.fc2 = nn.Linear(n_hidden, n_outputs)
    def forward(self, x):
        x = self.fc1(x)
        x = torch.sigmoid(x)
        x = self.fc2(x)
        return x
 
model = NeuralNet()
 
# 打印模型结构
from torchsummary import summary
summary(model, (n_inputs,))
```

###  8-深度学习框架keras入门案例

#### 一.Keras框架的定义与优点：

Keras 是一个高度封装的用于构建和训练深度学习模型的开源库，具有以下几个优点：

1. 它是一个高度抽象的框架，可以节省人工智能学习者的时间，使其可以更快的上手，更快的开发模型；
2. Keras 是基于Python的，可以让学习者更容易掌握；
3. Keras 提供了一个简单的API，可以轻松构建和训练深度学习模型，而不用编写复杂的代码；
4. Keras 支持多种后端，可以让学习者更容易在不同硬件上部署模型；
5. Keras 也支持许多现有的深度学习框架，可以让学习者更容易迁移至其它框架；
6. 对于新手，Keras 提供了一个友好的、容易理解的框架，可以让人工智能初学者更容易上手。

Keras由纯Python编写而成并基Tensorflow、Theano以及CNTK后端。Keras 为支持快速实验而生，能够把你的idea迅速转换为结果，如果你是初学者，请选择Keras框架，带你初步了解深度神经网络框架。Keras可以让用户快速构建和实验深度学习模型的一种开源框架。它提供了一种模块化的架构，可以让用户将模型的层次组织在一起，从而构建复杂的神经网络。Keras还支持多种后端，这样就可以利用不同的计算资源来加速训练过程，如GPU、CPU等。Keras为深度学习提供了更加快捷和便捷的构建方法，从而让用户能够更快地开发和调试模型。

####   二.案例

一个二维特征，影响一个函数值，例如函数  ,  x,y是自变量，z与x,y存在函数f的映射关系，下面要做的事情是，随机生成一个若干个点，他们之间符和某一种函数关系，我们事先不知道，现在要利用神经网络框架，通过训练，得到预测函数f(x,y)  ,使得预测结果接近真实的数值。

代码如下：

#####  1.**导入模型**

```
import numpy as np
from tensorflow.keras.models import Sequential  # 导入keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.optimizers import SGD
import tushare as ts
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
 
 
# 生成相应的数据函数
def get_beans4(counts):
    xs = np.random.rand(counts, 2) * 2
    ys = np.zeros(counts)
    for i in range(counts):
        x = xs[i]
        if (np.power(x[0] - 1, 2) + np.power(x[1] - 0.3, 2)) < 0.5:
            ys[i] = 1
 
    return xs, ys
 
 
# 画出数据的散点图
def show_scatter(X, Y):
    if X.ndim > 1:
        show_3d_scatter(X, Y)
 
    else:
        plt.scatter(X, Y)
        plt.show()
 
 
# 画3d散点图
def show_3d_scatter(X, Y):
    x = X[:, 0]
 
 
    z = X[:, 1]
    fig = plt.figure()
    ax = Axes3D(fig)
    ax.scatter(x, z, Y)
    plt.show()
 
 
# 画3D图
def show_scatter_surface_with_model(X, Y, model):
    # model.predict(X)
    x = X[:, 0]
    z = X[:, 1]
    y = Y
 
    fig = plt.figure()
    ax = Axes3D(fig)
    ax.scatter(x, z, y)
 
    x = np.arange(np.min(x), np.max(x), 0.1)
    z = np.arange(np.min(z), np.max(z), 0.1)
    x, z = np.meshgrid(x, z)
 
    X = np.column_stack((x[0], z[0]))
 
    for j in range(z.shape[0]):
        if j == 0:
            continue
        X = np.vstack((X, np.column_stack((x[0], z[j]))))
 
    y = model.predict(X)
 
    # return
    # y = model.predcit(X)
    y = np.array([y])
    y = y.reshape(x.shape[0], z.shape[1])
    ax.plot_surface(x, z, y, cmap='rainbow')
    plt.show()
```

#####  **二、显示3D图像**

![9091ef675cbd60a601a724f187e9ade9](image\9091ef675cbd60a601a724f187e9ade9.png)

#####   **三、建立神经网络模型与训练**

```
m = 100 # 数据量
X, Y = get_beans4(m)
show_scatter(X, Y)
print(X)
print(X.shape)
 
建立网络模型：
model = Sequential()
model.add(Dense(units=10, activation='sigmoid', input_dim=2))
# units 神经元个数， activation激活函数类型， 输了特征维度
model.add(Dense(units=1, activation='sigmoid')) # 输出层
# 编译网络
model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.3), metrics=['accuracy'])
# mean_squared_error 均方误差 sgd 随机梯度下降算法 accuracy 准确度
 
# 训练回合数epochs， batch_size 批数量，一次训练利用多少样本
model.fit(X, Y, epochs=8000, batch_size=64)
```

```
训练中....

Epoch 1/8000
2/2 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.3900
Epoch 2/8000
2/2 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.7300
Epoch 3/8000
2/2 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.7300
Epoch 4/8000
2/2 [==============================] - 0s 998us/step - loss: 0.2050 - accuracy: 0.7300
Epoch 5/8000
2/2 [==============================] - 0s 998us/step - loss: 0.1995 - accuracy: 0.7300
Epoch 6/8000
2/2 [==============================] - 0s 960us/step - loss: 0.1964 - accuracy: 0.7300
Epoch 7/8000
2/2 [==============================] - 0s 998us/step - loss: 0.1946 - accuracy: 0.7300
Epoch 8/8000
2/2 [==============================] - 0s 2ms/step - loss: 0.1929 - accuracy: 0.7300
Epoch 9/8000
2/2 [==============================] - 0s 998us/step - loss: 0.1930 - accuracy: 0.7300
Epoch 10/8000
2/2 [==============================] - 0s 996us/step - loss: 0.1917 - accuracy: 0.7300
Epoch 11/8000
2/2 [==============================] - 0s 998us/step - loss: 0.1908 - accuracy: 0.7300
Epoch 12/8000
2/2 [==============================] - 0s 952us/step - loss: 0.1904 - accuracy: 0.7300
Epoch 13/8000
2/2 [==============================] - 0s 999us/step - loss: 0.1900 - accuracy: 0.7300
Epoch 14/8000
2/2 [==============================] - 0s 958us/step - loss: 0.1893 - accuracy: 0.7300
Epoch 15/8000
2/2 [==============================] - 0s 997us/step - loss: 0.1892 - accuracy: 0.7300
Epoch 16/8000
2/2 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.7300
Epoch 17/8000
2/2 [==============================] - 0s 997us/step - loss: 0.1880 - accuracy: 0.7300
Epoch 18/8000
2/2 [==============================] - 0s 999us/step - loss: 0.1879 - accuracy: 0.7300
Epoch 19/8000
2/2 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.7300
Epoch 20/8000
2/2 [==============================] - 0s 996us/step - loss: 0.1868 - accuracy: 0.7300
Epoch 21/8000
2/2 [==============================] - 0s 998us/step - loss: 0.1864 - accuracy: 0.7300
Epoch 22/8000
2/2 [==============================] - 0s 999us/step - loss: 0.1860 - accuracy: 0.7300
Epoch 23/8000
2/2 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.7300
Epoch 24/8000
2/2 [==============================] - 0s 999us/step - loss: 0.1852 - accuracy: 0.7300
Epoch 25/8000
2/2 [==============================] - 0s 997us/step - loss: 0.1849 - accuracy: 0.7300
```

#####  **四、预测结果**

```
# 预测函数
pres = model.predict(X)
show_scatter_surface_with_model(X, Y, model) # 三维的
```

#####  **五、预测结果曲面图**

![a6807d9a637cf36cd5cb91a72ac479ac](image\a6807d9a637cf36cd5cb91a72ac479ac.png)

### 9-深度学习深入了解

深度学习是一种机器学习技术，由一些多层次的非线性处理单元组成，每一层都可以学习数据中的复杂关系，并且可以组合到一起以构建更复杂的模型。它是由一系列的计算机模型组成的，这些模型是由非线性的多层神经网络构建的，其中每一层都可以学习数据中的复杂关系，并在更高的层次上抽象出更为宏观的模式。它主要应用于计算机视觉、[自然语言处理](https://so.csdn.net/so/search?q=自然语言处理&spm=1001.2101.3001.7020)、语音识别和推荐系统等领域。

#####  **一、深度学习的数学原理**

深度学习的数学原理主要是针对神经网络的，包括线性代数、概率论、微积分、信息论等等。
(1)线性代数：深度学习的基础，学习深度学习之前要掌握矩阵乘法、特征值分解、矩阵求导等知识，这些知识可以帮助我们更好地理解神经网络中的参数、激活函数以及权重的变化。

线性代数是数学中处理向量和矩阵的分支，用于解决多元函数和线性方程组，在深度学习中用于分析神经网络的结构和模型。

(2)概率论：深度学习中的概率论主要涉及概率分布、条件概率等，可以用于训练神经网络，并用于预测模型的性能。
(3)微积分：深度学习的核心是梯度下降，梯度下降是一种基于求导的优化算法，它可以帮助我们找到损失函数的最优值，从而让我们的模型更加准确。

微积分是数学中处理函数的变化率和极限问题的分支，在深度学习中用于处理神经网络中的各种变量，比如梯度、导数和微分。
(4)信息论：信息论主要涉及信息熵、最大似然估计等，可以用于测量神经网络的准确度，从而对模型进行优化。

#####  **二、深度学习的应用领域**

1. 图像识别：深度学习技术可以用于处理各种图像，从而实现图像分类、检测、识别等功能，如自动驾驶汽车、人脸识别、手写数字识别等。
2. 语音识别：语音识别是深度学习应用最受欢迎的领域之一，深度学习可以用于处理语音信号以及语音识别，从而构建语音对话系统、自动语音翻译系统等。
3. 自然语言处理：深度学习可以用于自然语言处理，如语义分析、机器翻译、命名实体识别、文本分类、关系抽取等，它可以帮助机器理解自然语言，从而实现自然语言交互。
4. 推荐系统：深度学习可以用于构建推荐系统，它可以根据用户的历史行为和兴趣，为用户推荐相关的商品或内容。
5. 智能客服：深度学习可以用于构建智能客服系统，它可以根据客户的输入自动识别意图，从而自动回答客户的问题。

![729f21eb3be2f001070344684cf0391f](image\729f21eb3be2f001070344684cf0391f.png)

##### 三**、深度学习的框架**

1、Tensorflow：Tensorflow是Google开源的机器学习框架，可以用来构建、训练和部署深度学习模型，也可以用来构建简单的模型来进行语音识别、图像分类等应用。
2、Keras：Keras是一个高级神经网络API，可以用来构建和训练深度学习模型，可以用来解决许多自然语言处理、语音识别、图像分类和计算机视觉等问题。
3、PyTorch：PyTorch是一个开源的深度学习库，可以用来构建和训练神经网络，可以用来解决许多机器学习任务，包括自然语言处理、语音识别、图像分类和计算机视觉等问题。
4、Scikit-Learn：Scikit-Learn是一个开源的Python机器学习库，可以用来构建和训练机器学习模型，可以用来解决许多机器学习任务，包括聚类、回归、分类等等。
5、Theano：Theano是一个开源的Python框架，可以用来构建、训练和部署深度学习模型，可以用来解决许多机器学习任务，包括自然语言处理、语音识别、图像分类和计算机视觉等问题。

6、PaddlePaddle：由百度研发，致力于成为业界最安全、最稳定、最易用的深度学习框架。PaddlePaddle结合了百度的大规模计算资源和深度学习经验，致力于为用户提供高性能、易用的深度学习服务。

![893bd02ccef413b51434e41358fc9130](image\893bd02ccef413b51434e41358fc9130.png)

##### 四**、深度学习的代码案例**

**1.keras 框架**

```
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
#创建一个模型
model = Sequential()
#添加一个全连接层
model.add(Dense(units=64, activation='relu', input_dim=100))
#添加一个第二个全连接层
model.add(Dense(units=10, activation='softmax'))
 
#优化器
opt = keras.optimizers.SGD(lr=0.01)
#编译模型
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])
# 生成虚拟数据
data = np.random.random((1000, 100))
labels = np.random.randint(10, size=(1000, 1))
#将标签转换为分类的 one-hot 编码
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)
 
#训练模型
model.fit(data, one_hot_labels, epochs=10, batch_size=32 )
```

**2.pytorch框架**

```
import torch
from torch import nn
 
# 创建一个模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 添加一个全连接层
        self.fc1 = nn.Linear(100, 64)
        # 添加一个第二个全连接层
        self.fc2 = nn.Linear(64, 10)
 
    def forward(self, x):
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = torch.softmax(x, dim=1)
        return x
 
model = Net()
#准备数据
data = torch.rand((1000,100))
labels = torch.randint(10,(1000,1))
#将标签转换为分类的 one-hot 编码
one_hot_labels = torch.zeros(1000, 10).scatter_(1, labels.long(), 1)
# 训练模型
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
 
#训练模型
for epoch in range(10):
    #前向传播
    y_pred = model(data)
    #计算损失
    loss = criterion(y_pred, one_hot_labels)
    #将梯度归零
    optimizer.zero_grad()
    #反向传播
    loss.backward()
    #更新参数
    optimizer.step()
    print('Epoch: %d Loss: %.5f' % (epoch, loss.item()))
```

### 10-卷积神经网络CNN初步认识

卷积神经网络（Convolutional Neural Network）是一种深度学习模型，也是最近几年受到最多关注的模型之一。它的核心是深度学习的卷积层，可以帮助用户抽取有效的特征，从而进行分类或者识别。这些特征可以是图像，文本，音频或者视频中的相关特征，可以构建许多实用的应用。 

![0deb97dead676ffbc8366af8a73529ae](image\0deb97dead676ffbc8366af8a73529ae.png)

##### **一、卷积神经网络的发展**

CNN最早的模型框架就是由LeNet给出的, 即卷积+池化+FC层的基本结构.

2012年另一个具有划时代意义的模型AlexNet横空出世. AlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势, 一举夺冠, 引起了相当大的轰动. 此前深度学习沉寂了很久, 但自AlexNet诞生后, 所有的ImageNet冠军都是用CNN来做的.

随后，CNN 又经历了几次发展，例如 VGG、GoogLeNet 等。这些模型在不同的计算机视觉任务中取得了很好的成绩，并在深度学习领域引起了巨大的关注。

近年来，CNN 又有了新的发展，例如 ResNet、DenseNet 等。这些模型在不同的计算机视觉任务中也取得了很好的成绩。

总的来说，CNN 经历了几十年的发展，在计算机视觉领域取得了巨大的成就。它的出现极大地提高了计算机对图像的理解能力，为人工智能的发展做出了重要贡献。

##### **二、卷积神经网络介绍**
  卷积神经网络由一系列可学习的卷积层和全连接层组成，每一层都有一组可学习的参数，卷积层的参数定义为卷积核，全连接层的参数叫做权重矩阵。卷积神经网络可以通过**反向传播算法**不断的调整参数值，使得输入的数据可以被准确的识别或者分类。
  卷积神经网络的优势主要在于它可以抽取更多的特征，抽取特征的过程是由卷积层完成的，卷积层是基于局部连接和权重共享的。另外，卷积神经网络可以更好的处理结构化的输入数据，比如图片、声音、文本等。它也具有更少的参数，更高的泛化能力，这使得卷积神经网络更容易收敛，可以更快的训练模型，更好的处理大规模的数据，以及解决更复杂的问题。

##### **三、卷积神经网络的原理**

卷积神经网络（Convolutional Neural Network，CNN） 具有许多层，包括**卷积层、池化层和全连接层**。

**卷积层的作用是提取图像的特征**。它通过使用卷积核（也称为滤波器）对图像进行卷积来实现这一点。卷积核是一个小的矩阵，它在图像上滑动，并与图像的每个部分做卷积运算。对于图像的每一个位置，卷积核都会与它所覆盖的图像像素做内积，然后将所有内积的结果相加，得到一个新的值。这个新值就是卷积核在该位置的输出。卷积核在图像上滑动的过程就是卷积运算。

**池化层的作用是降低图像的维度**，同时保留有用的特征。常用的池化操作有最大池化和平均池化。最大池化会保留池化窗口内的最大值，而平均池化则会保留池化窗口内的平均值。池化的效果是降低图像的维度，同时有效地抑制噪声和缩小模型的大小。

全连接层是普通的神经网络层，它将之前的输出进一步处理并得到最终的输出结果。全连接层中的每个单元都会与前一层中的所有单元相连。

在训练过程中，CNN 会自动**学习卷积核的权值，以便于提取图像中有用的特征**。这就是 CNN 的强大之处，因为它能够自动学习图像的特征，而不用人工去设计特征提取方法。

##### **四、卷积运算的数学表达**

![Snipaste_2025-03-13_13-42-42](image\Snipaste_2025-03-13_13-42-42.png)

##### **五、卷积神经网络pytorch案例**

```
import torchvision.transforms as transforms
from torch.nn import functional as F
 
# 定义模型
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
 
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
 
# 加载数据集
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)
testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)
 
net =CNN()
# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
 
if __name__ == '__main__':
    torch.multiprocessing.freeze_support()
 
    # 训练模型
    for epoch in range(5):  # 遍历数据集五次
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            # 获取输入和标签
            inputs, labels = data
            # 梯度清零
            optimizer.zero_grad()
            # 前向传播+反向传播+优化
            outputs= net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            # 打印统计信息
            running_loss += loss.item()
            if i % 2000 == 1999: # 每 2000 个 mini-batches 打印一次
                print('[%d, %5d] loss: %.3f' %
                (epoch + 1, i + 1, running_loss / 2000))
                running_loss = 0.0
 
                print('Finished Training')
    
    #模型测试
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
 
            print('Accuracy of the network on the 10000 test images: %d %%' % (
                    100 * correct / total))
```

### 11-图像识别实战（网络层联想记忆，代码解读）

图像识别实战是一个实际应用项目，下面介绍如何使用深度学习技术来识别和检测图像中的物体。主要涉及计算机视觉，实时图像处理和相关的深度学习算法。学习者将学习如何训练和使用深度学习模型来识别和检测图像中的物体，以及如何使用实时图像处理技术来处理图像。

项目还将涉及如何使用计算机视觉方法来识别和检测图像中的特征，以及利用卷积神经网络来进行识别图像。

##### **一、图像识别原理与步骤**

图像识别是指通过深度学习技术从图像中识别出特征和对象的过程。

图像识别我们主要采用卷积神经网络来实现，它可以用来识别和识别图像中的特征。它采用一种叫做卷积的技术来提取图像中的关键特征，并使用多层的神经网络来分类和识别图像。
对CNN进行复习：CNN的结构一般由输入层、卷积层、池化层、全连接层、输出层组成。

![a29acdf40725e9aefc007920786e21b3](image\a29acdf40725e9aefc007920786e21b3.png)

1. 输入层：将图像转化为数字信号，将每个像素转换为一个数字，作为神经网络的输入层。
2. 卷积层：卷积层用来提取图像的特征，它对图像的每个区域进行特征提取，并将提取的特征输出到另一卷积层。
3. 池化层：池化层可以提取图像的主要特征，它将大小相同的特征池化成更小的特征，并丢弃不具有代表性的特征。
4. 全连接层：全连接层可以将池化层提取出的特征拼接成一个完整的特征向量，用来进一步分析和提取图像特征。
5. 输出层：输出层将前面层提取出的特征转换为最终的识别结果。

##### **二、卷积神经网络网络层与记忆方法**

输入层：Input（In），把数据输入进去；
卷积层：Convolution（Conv），将图像数据翻译成特征数据；
池化层：Pooling（Pool），对特征数据进行子采样降维；
全连接层：Fully Connected（FC），将特征数据拉长到神经网络的输入；
输出层：Output（Out），将模型的输出展示出来。

**记忆方法：**

**Input：**想象你手上拿着一叠图片，要把它们输入到电脑里；
**Conv：**想象你用缝纫机把一张复杂的图案缝制在布料上，它把图案中的每个元素翻译成特征；
**Pool：**想象你一块块地把布料剪开，剪出来的图案比之前要小，它把特征数据降维了；
**FC：**想象你用胶水把图案拉长，它把原有的复杂图案拉长成神经网络的输入；
**Out：**想象你把拉长的图案绣在棉布上，它把模型的输出展示出来了。

##### **三、卷积神经网络网络的优点**

1. 具有非常强大的特征提取能力，能够从图像中提取有价值的特征；
2. 参数共享，卷积网络在同一层中共享参数，有效减少了参数量，大大减少了训练时间；
3. 能够学习到更多的高层抽象特征，使得卷积网络能够更好地处理复杂的问题；
4. 支持不同尺度的特征提取，可以从不同的尺度提取特征，并将不同尺度的特征结合起来；
5. 使用少量的参数能够很好的拟合大量的数据，使得训练模型的效果更好；
6. 支持在线学习，可以通过少量的训练数据快速拟合；
7. 可以用来实现深度学习，提升模型的性能。

##### **四、图像分类实战代码**

  以CIFAR-10数据集为例子做图像分类，CIFAR-10数据集是一个用于机器学习和计算机视觉识别研究的图像数据库。它由 Hinton 的学生 Alex Krizhevsky 和 Ilya Sutskever 整理的，包含五万张32x32彩色图像，分成10个类别：飞机，汽车，鸟类，猫，鹿，狗，青蛙，马，船和卡车。每一类都有5000张图像，总计50,000张图像。每张图像是一个numpy数组，尺寸为 32x32x3，每个像素的可能值从0到255。

![d773648e6b9bde807bfa0268b06e156a](D:\learn\笔记\人工智能\image\d773648e6b9bde807bfa0268b06e156a.png)

 CIFAR-10 具有以下不同点：

CIFAR-10 是 3 通道的彩色 RGB 图像，而 MNIST 是灰度图像。
CIFAR-10 的图片尺寸为 32×32， 而 MNIST 的图片尺寸为 28×28，比 MNIST 稍大。
相比于手写字符， CIFAR-10 含有的是现实世界中真实的物体，不仅噪声很大，而且物体的比例、 特征都不尽相同，这为识别带来很大困难。 直接的线性模型如 Softmax 在 CIFAR-10 上表现得很差
数据集下载

```
官方下载地址：（共有三个版本：python，matlab，binary version 适用于C语言）

http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

http://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz

http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz
```

```
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
# 加载数据
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)
train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)
test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)
# 定义网络架构
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
# 创建网络实例
net = Net()
# 定义优化器和损失函数
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
# 开始训练
for epoch in range(50):
    running_loss = 0.0
    for i, data in enumerate(train_data_loader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 2000 == 0:
            print('Epoch: %d, step: %d, loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))
            running_loss = 0.0
 
# 开始测试
correct = 0
total = 0
with torch.no_grad():
    for data in test_data_loader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
 
print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
```

代码解析： 

定义网络架构

class Net(nn.Module):

1.  继承自 pytorch 中的 nn.Module 类，构建网络框架，进而构建网络结构；

2. 在__init__()函数中，使用nn.Conv2d()函数定义卷积层；使用nn.MaxPool2d()函数定义池化层；使用nn.Linear()函数定义全连接层；
3. 在forward()函数中，使用F.relu()函数定义ReLu激活函数；使用x.view()函数将卷积层输出展平，作为全连接层的输入；最后使用self.fc3(x)输出最终结果。 模型训练过程： 定义交叉熵损失函数 criterion 和随机梯度下降优化器 optimizer。 用 for 循环迭代 50 个 epoch，每个 epoch 内迭代所有的训练数据集 train_data_loader。
在每个 mini-batch 中：
使用 optimizer.zero_grad() 将梯度缓存清零，清空上一步的残余更新参数值。
将输入数据 inputs 送入网络，得到输出 outputs。
计算损失值 loss，反向传播梯度，更新网络参数。
每 2000 个 mini-batch 打印一次平均损失 running_loss，其值等于当前损失 loss 与上 2000 个 mini-batch 的损失之和除以 2000。

### 12-循环神经网络RNN初步认识

循环神经网络 ([RNN](https://so.csdn.net/so/search?q=RNN&spm=1001.2101.3001.7020)) 是一种特殊的神经网络架构，能够处理序列数据，例如语音识别、文本生成等任务。RNN 的核心思想是在网络中增加一条“时间线”，允许信息在时间上传递。

##### **一、RNN的介绍**

RNN 的结构分为两部分：隐藏层和输出层。隐藏层包含一个或多个隐藏状态，用于记录之前的信息。当输入一个新的数据时，隐藏层会更新隐藏状态，并将其传递到输出层。输出层则根据当前的隐藏状态和输入数据生成输出。

![9273a2d3c230640d6cab3edc1cc15f25](image\9273a2d3c230640d6cab3edc1cc15f25.png)

RNN 的一个主要问题是长期依赖，即隐藏状态在时间上传递过程中可能会丢失之前的信息。为了解决这个问题，引入了长短时记忆网络 (LSTM) 和门控循环单元 (GRU)。这两种网络结构在隐藏层中增加了门控机制，能够更好地控制信息的传递。

![0be44c77d362d859d5d7f339e38d0991](image\0be44c77d362d859d5d7f339e38d0991.png)

RNN的另一个重要应用是序列到序列的学习。例如，翻译模型就是将一个句子从一种语言翻译成另一种语言的过程。这个过程可以用RNN模型来实现。 RNN模型在编码端对原始序列进行编码，在解码端进行解码，生成目标序列。

还有另外一种叫做双向RNN的结构，它在隐藏层中连接两个单向的RNN，一个是正向的，另一个是反向的，这样可以更好地捕捉序列中的上下文信息。

##### **二、RNN的原理**

循环神经网络 (RNN) 的原理可以用一个类比来形象地解释：假设你有一个记事本，你写下了一些文字，然后回到之前写过的文字上继续写，这样你就可以在之前写过的文字基础上进行更新。类似地，RNN 中的隐藏层就像是一个记事本，能够记录之前的信息并在之后的时间步骤中进行更新。

也可以想象成你正在学习一门新语言，你会将之前学过的单词记录下来，在学习新单词时使用这些单词来帮助理解。类似地，RNN 中的隐藏层就像是你的脑海中的“单词本”，能够记录之前学过的单词并在之后的学习过程中进行更新。

在这个例子中，你学习的单词就相当于 RNN 的输入，你脑海中的“单词本”就相当于 RNN 的隐藏层，当你学习新单词时，你的脑海会更新“单词本”并使用它来帮助理解新单词，类似地 RNN 中的隐藏层也会更新并使用之前的信息来处理当前的输入。

为了表示这个过程，我们可以使用一个公式来表示，![h_t = f(h_{t-1}, x_t)](https://latex.csdn.net/eq?h_t%20%3D%20f%28h_%7Bt-1%7D%2C%20x_t%29)，其中 ![h_t](https://latex.csdn.net/eq?h_t) 表示隐藏层在时间步![t](https://latex.csdn.net/eq?t)的状态，![x_t](https://latex.csdn.net/eq?x_t) 表示输入数据，![f](https://latex.csdn.net/eq?f) 表示更新隐藏状态的函数。

这个过程就是RNN的核心原理，它能够处理序列数据，并在保留之前信息的基础上对当前数据进行处理。

##### **三、关于RNN的代码案例**

如何使用循环神经网络 (RNN) 实现一个序列到序列的学习任务：

```
import numpy as np
from keras.layers import SimpleRNN, Input, Dense
from keras.models import Model
 
# 设置超参数
timesteps = 10
input_dim = 2
output_dim = 3
 
# 定义输入层
inputs = Input(shape=(timesteps, input_dim))
 
# 定义隐藏层
x = SimpleRNN(units=output_dim)(inputs)
 
# 定义输出层
predictions = Dense(units=output_dim, activation='softmax')(x)
 
# 构建模型
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
 
# 模拟输入数据
x_train = np.random.random((1000, timesteps, input_dim))
y_train = np.random.random((1000, output_dim))
 
# 训练模型
model.fit(x_train, y_train, epochs=10, batch_size=32)
```

以上我们使用了 Keras 框架，定义了一个简单的 RNN 模型。我们设置了输入数据的时间步数为 10，输入维度为 2，输出维度为 3。使用 SimpleRNN 层来定义隐藏层，并使用 Dense 层来定义输出层。
然后我们模拟了输入数据并使用 model.fit 来训练模型。这只是一个简单的示例，更复杂的 RNN 模型可能需要更复杂的数据和更多的超参数调整。

### 13-LSTM网络：预测上证指数走势

##### **一、LSTM网络简单介绍**

LSTM又称为：长短期记忆网络，它是一种特殊的 RNN。LSTM网络主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。对于相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

引入LSTM网络的原因：由于 RNN 网络主要问题是长期依赖，即隐藏状态在时间上传递过程中可能会丢失之前的信息。为了解决这个问题，引入了长短时记忆网络 (LSTM) 和门控循环单元 (GRU)。这两种网络结构在隐藏层中增加了门控机制，能够更好地控制信息的传递。

![Snipaste_2025-03-13_14-35-18](image\Snipaste_2025-03-13_14-35-18.png)



 LSTM中有三个门：
(1)遗忘门f：决定上一个时刻的记忆单元状态需要遗忘多少信息，保留多少信息到当前记忆单元状态。
(2)输入门i：控制当前时刻输入信息候选状态有多少信息需要保存到当前记忆单元状态。
(3)输出门o：控制当前时刻的记忆单元状态有多少信息需要输出给外部状态。

**形象的例子让我们更好的理解LSTM的原理：**

假设你是一个梦想远大的学生，你想通过学习一门课程获得更多的知识。在学习过程中，LSTM模型帮助你，它就像是一个老师，它的遗忘门就像是老师的提醒，它让你挑出不用的知识，以保持你对重要知识的清晰记忆。它的输入门就像是老师的指导，它会重新审视你学习过的知识，按照自己的逻辑把知识结合起来，进化出更多有用的知识。最后，它的输出门就像老师的监督，它会确保你学习到了有用的知识，不要浪费时间去学习无用的知识。

##### **二、LSTM网络运用-预测上证指数走势**

```
# 使用LSTM预测沪市指数
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from pandas import DataFrame
from pandas import concat
from itertools import chain
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = ['sans-serif']
plt.rcParams['font.sans-serif'] = ['SimHei']
 
# 转化为可以用于监督学习的数据
def get_train_set(data_set, timesteps_in, timesteps_out=1):
    train_data_set = np.array(data_set)
    reframed_train_data_set = np.array(series_to_supervised(train_data_set, timesteps_in, timesteps_out).values)
 
    train_x, train_y = reframed_train_data_set[:, :-timesteps_out], reframed_train_data_set[:, -timesteps_out:]
    # 将数据集重构为符合LSTM要求的数据格式,即 [样本数，时间步，特征]
 
    train_x = train_x.reshape((train_x.shape[0], timesteps_in, 1))
    return train_x, train_y
 
"""
将时间序列数据转换为适用于监督学习的数据
给定输入、输出序列的长度
data: 观察序列
n_in: 观测数据input(X)的步长，范围[1, len(data)], 默认为1
n_out: 观测数据output(y)的步长， 范围为[0, len(data)-1], 默认为1
dropnan: 是否删除NaN行
返回值：适用于监督学习的 DataFrame
"""
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    print(data.shape)
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]
    # 预测序列 (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]
    # 拼接到一起
    agg = concat(cols, axis=1)
    agg.columns = names
    # 去掉NaN行
    if dropnan:
        agg.dropna(inplace=True)
    return agg
 
# 使用LSTM进行预测
def lstm_model(source_data_set, train_x, label_y, input_epochs, input_batch_size, timesteps_out):
    model = Sequential()
    
    # 第一层, 隐藏层神经元节点个数为128, 返回整个序列
    model.add(LSTM(128, return_sequences=True, activation='tanh', input_shape=(train_x.shape[1], train_x.shape[2])))
    # 第二层，隐藏层神经元节点个数为128, 只返回序列最后一个输出
    model.add(LSTM(128, return_sequences=False))
    model.add(Dropout(0.5))
    # 第三层 因为是回归问题所以使用linear
    model.add(Dense(timesteps_out, activation='linear'))
    model.compile(loss='mean_squared_error', optimizer='adam')
 
    # LSTM训练 input_epochs次数
    res = model.fit(train_x, label_y, epochs=input_epochs, batch_size=input_batch_size, verbose=2, shuffle=False)
 
    # 模型预测
    train_predict = model.predict(train_x)
    #test_data_list = list(chain(*test_data))
    train_predict_list = list(chain(*train_predict))
 
    plt.plot(res.history['loss'], label='train')
    plt.show()
    #print(model.summary())
    plot_img(source_data_set, train_predict)
 
# 呈现原始数据，训练结果，验证结果，预测结果
def plot_img(source_data_set, train_predict):
    plt.figure(figsize=(24, 8))
    # 原始数据蓝色
    plt.plot(source_data_set[:, -1], c='b',label = '标签')
    # 训练数据绿色
    plt.plot([x for x in train_predict], c='g')
    plt.legend()
    plt.show()
 
# 设置观测数据input(X)的步长（时间步），epochs，batch_size
timesteps_in = 3
timesteps_out = 3
epochs = 1000
batch_size = 100
data = pd.read_csv('./shanghai_index_1990_12_19_to_2019_12_11.csv')
data_set = data[['Price']].values.astype('float64')
# 转化为可以用于监督学习的数据
train_x, label_y = get_train_set(data_set, timesteps_in=timesteps_in, timesteps_out=timesteps_out)
 
print(train_x, label_y )
print(train_x.shape)
print(train_x.shape[1], train_x.shape[2])
 
# 使用LSTM进行训练、预测
lstm_model(data_set, train_x, label_y, epochs, batch_size, timesteps_out=timesteps_out)
```

运行结果：

![d43049e46e2d2b3ca4a96a051b85b1aa](image\d43049e46e2d2b3ca4a96a051b85b1aa.png)

### 14-蒙特卡洛方法在人工智能中的应用及其Python实现

#### 一、蒙特卡洛方法简介

蒙特卡洛方法是通过随机数进行数据模拟和[数值计算](https://so.csdn.net/so/search?q=数值计算&spm=1001.2101.3001.7020)的一种方法。它的基本思想是通过随机抽样进行计算或模拟，然后对估计结果进行统计分析。蒙特卡洛方法最初是用于求解概率问题，例如对于一个掷骰子的游戏，要求掷出6的概率，可以通过大量的模拟实验来估计这个概率。

蒙特卡洛方法在计算机科学、物理学、金融学、生物学等领域都有广泛的应用。在物理学中，蒙特卡洛方法可以用来模拟分子的运动和相互作用，以及求解复杂的高能物理问题。在金融学中，蒙特卡洛方法可以用来估计期权的价格和风险，以及进行投资[组合优化](https://so.csdn.net/so/search?q=组合优化&spm=1001.2101.3001.7020)。

蒙特卡洛方法，又称统计模拟方法，是一种通过随机抽样的方式求解各种问题的数值计算方法。在 AI 领域，它被广泛应用于搜索策略、强化学习、博弈论等多个方面。
蒙特卡洛方法的主要特点有：

1. 基于随机采样：通过反复随机抽样实现对问题的求解，从而避免了穷举和解析方法所面临的计算复杂度问题。
2. 简单易行：算法实现相对简单，通常需要编写较少的代码，并且容易调试。
3. 并行性强：方法本身没有严格的先后顺序，适合于并行计算和分布式计算。
4. 收敛性：随着采样数据的逐渐增多，计算结果将逐步接近于真实值，具有较好的收敛性。

#### 二、蒙特卡洛方法原理

蒙特卡洛方法的基本思想是将问题转化为一个随机实验。通过构造适当的随机变量来模拟该实验，并通过对随机变量的反复抽样来求出问题的近似解。
蒙特卡洛方法的关键步骤：

1. 确定问题的随机模型和随机变量；
2. 进行随机抽样模拟；
3. 根据模拟结果求取问题的解。

#### 三、示例：计算圆周率

计算圆周率是一个经典的**蒙特卡洛方法**的应用。假设我们已知一个单位正方形内部包含一个半径为 1 的圆，我们可以通过随机采样的方法计算该正方形内部的点落在圆内的概率，从而估计圆周率。我们使用 `random.uniform()` 函数来生成随机数，计算随机点 `(x, y)` 落在圆内的概率。然后根据比例计算圆周率。

```
import random
 
def monte_carlo_pi(num_samples):
    num_points_in_circle = 0
 
    for _ in range(num_samples):
        x = random.uniform(-1, 1)
        y = random.uniform(-1, 1)
 
        distance = x*x + y*y
        if distance <= 1:
            num_points_in_circle += 1
 
    return 4 * num_points_in_circle / num_samples
 
def test_monte_carlo_pi():
    pi_estimate = monte_carlo_pi(100000)
    print(f"评估 圆周率约为: {pi_estimate}")
 
if __name__ == "__main__":
    test_monte_carlo_pi()
```

运行结果：

```
评估 圆周率约为: 3.14036
```

我们得到圆周率的近似解 3.14036，随着num_samples的增大，圆周率的近似解会更加接近真实值。

### 15-自然语言处理中的数据处理上采样、下采样、负采样是什么？

#### 一、负采样（Negative Sampling）

负采样是一种针对**skip-gram、CBOW**向量模型的优化技术，用于提高训练速度和效果。**skip-gram**是已知一个词去预测上下文。

**Skip-Gram模型**：以一个词作为输入，尝试预测上下文的词。

**CBOW模型**：以一组词(上下文词)作为输入，预测其中一个中心词的出现概率。

在Word2Vec模型中，负采样可以有效地解决softmax计算时的速度问题。负采样的基本思想是对于每个正样本，随机从词典中选择若干个负面样本，使得它们的概率尽可能地小。这样可以加速模型训练过程，同时还可以避免训练过程中出现梯度爆炸和消失的问题。

具体来说，对于每个正样本（即一个单词及其上下文环境），我们从整个词汇表中随机抽取若干个负样本，并将它们作为上下文预测词的负例。这样，我们只需要计算少量的正负样本的概率，就可以更新模型参数。这样既可以减少计算时间，同时也能够使得模型更加关注那些重要的词汇。

#### 二、上采样（Upsampling）

在自然语言处理中，例如在文本分类任务中，往往会出现某些类别的样本数量非常少，这时候我们就可以通过上采样来增加这些类别的样本数量。本文提供上采样的代码示例：

```
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from imblearn.over_sampling import SMOTE

# 生成简单数据集
texts = ['apple banana orange', 'orange apple', 'banana']
labels = [0, 0, 1]

# 文本向量化
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)
y = labels

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 使用SMOTE算法进行上采样
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# 训练模型
clf = MultinomialNB()
clf.fit(X_resampled, y_resampled)

# 在测试集上评估模型
score = clf.score(X_test, y_test)
print("Accuracy:", score)
```

#### 三、下采样（Subsampling）

下采样是一种用于处理高频词汇的技术，帮助我们降低训练过程中高频词汇所占的比例。由于许多高频词汇并没有提供太多的信息，反而会干扰训练过程，因此我们需要通过下采样来降低它们的权重。本文提供以下下采样的代码示例：

```
import collections
import random

def subsample(corpus, freq_threshold):
    word_freq = dict(collections.Counter(corpus))
    total_words = len(corpus)
    subsampled_corpus = []
    for word in corpus:
        freq = word_freq[word] / total_words
        prob_keep = (freq_threshold / freq)**0.5
        if random.random() < prob_keep:
            subsampled_corpus.append(word)
    return subsampled_corpus
    
corpus = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']
subsampled_corpus = subsample(corpus, freq_threshold=0.001)
print(subsampled_corpus)
```

以上我们首先定义了一个subsample函数，其中corpus表示原始语料库，freq_threshold表示一个阈值，用于决定哪些单词需要保留。然后，我们计算每个单词出现的频率，并根据频率计算其被保留的概率。最后，我们随机决定是否保留每个单词，并返回下采样后的语料库。

### 16-神经网络与GPU加速训练的原理与应用

#### 一、神经网络与GPU的关系

神经网络是一种模拟人脑神经元连接的计算系统，具有非常强大的表达和学习能力。与传统的计算机程序不同，神经网络是一个大规模并行的计算系统，因此天然适合于使用GPU进行并行计算。
GPU，全称是Graphical Processing Unit，中文为图形处理器。最初设计用于图形渲染任务。随着计算能力的提升，现在的GPU也被广泛应用于通用计算任务，如深度学习。相较于通用的CPU(中央处理器)，GPU具有更多的内核，更大的内存带宽，从而能够更好地完成这些并行计算任务。
深度学习框架如TensorFlow和PyTorch等已经实现了利用GPU加速[神经网络训练](https://so.csdn.net/so/search?q=神经网络训练&spm=1001.2101.3001.7020)的功能。在接下来的部分，我们将详细介绍一个简单的神经网络实现，并展示如何使用PyTorch库和GPU加速训练。

#### 二、 GPU的原理

GPU的基本原理是将大规模的并行计算任务拆分成更小的任务，并将其分发给GPU上的多个内核同时进行处理。这种基于数据并行的计算方式非常适合神经网络的训练。在神经网络的训练过程中，包括前向传播、反向传播和权重更新等步骤可以很容易地拆分成并行任务，因此利用GPU进行加速非常有效。

GPU是一种专门用于图形渲染、计算和加速的处理器。与CPU相比，GPU在大规模并行计算方面具有优势，因此被广泛应用于深度学习、科学计算、密码学等领域。

**GPU的底层原理：**

**架构结构：**GPU通过一系列**流处理器**来进行并行计算。每个流处理器（**Stream Processor**，SP）类似于CPU中的核心，可以执行独立的指令序列。不同的GPU架构会有不同的流处理器数量和组织方式。

**内存结构：**GPU通常使用**全局内存和共享内存来存储数据**。全局内存是所有流处理器都可以访问的内存，在访问时需要花费较长时间。而共享内存则位于每个流处理器的本地存储器中，访问速度更快，但容量较小。

**计算模型：**GPU采用**SIMD（Single Instruction Multiple Data**，单指令多数据）计算模型。即将一个指令同时应用于多个数据元素，从而实现高效的并行计算。例如，在矩阵乘法中，GPU可以同时对多个矩阵元素进行计算，极大地提高了计算速度。

**数据传输：**GPU与主机之间的数据传输通常通过PCIe总线进行。由于PCIe带宽较小，因此在大规模计算中可能会成为瓶颈。一些新型GPU支持NVLink技术，可以实现更高速的GPU-GPU和GPU-CPU数据传输。

**编程模型：**GPU编程通常使用CUDA、OpenCL等框架进行开发。这些框架提供了丰富的API和工具，方便开发者进行并行计算任务的编写和调试。

![b3270275502cec62971bcf55ae04d3dd](image\b3270275502cec62971bcf55ae04d3dd.png)

#### 三、GPU与CPU的对比

CPU（中央处理器）和 GPU（图形处理器）是计算机中两种不同类型的处理器，它们在设计理念、架构、性能特点等方面存在显著差异，下面从多个维度为你详细对比： 

##### 1. 架构设计 

- **CPU**：CPU 的设计注重通用性和灵活性，它拥有复杂的控制单元和大量的缓存，能够高效处理各种不同类型的任务。CPU 通常包含较少但功能强大的核心，每个核心都可以独立处理复杂的指令序列，适用于串行计算。 
- **GPU**：GPU 的设计侧重于并行计算，它拥有大量相对简单的处理核心。这些核心可以同时处理多个数据，以实现对大规模数据的并行处理。GPU 的架构使得它在处理高度并行的任务时具有显著的优势，但在处理复杂的串行任务时效率较低。 

##### 2. 计算能力 

**CPU**：CPU 的单核性能较高，能够快速处理复杂的逻辑运算和控制任务。然而，由于核心数量相对较少，CPU 在处理大规模并行计算任务时的能力有限。 

-**GPU**：GPU 拥有数千个甚至数万个处理核心，可以同时对大量数据进行并行计算。因此，GPU 在处理大规模矩阵运算、深度学习训练等并行计算任务时具有强大的计算能力，其计算速度可以比 CPU 快数十倍甚至数百倍。 

##### 3. 缓存和内存带宽 

**CPU**：CPU 配备了较大的缓存，用于存储经常访问的数据和指令，以减少内存访问延迟。此外，CPU 的内存带宽相对较低，但足以满足其处理串行任务的需求。 - **GPU**：GPU 的缓存相对较小，但它具有极高的内存带宽，能够快速地从显存中读取和写入数据。这使得 GPU 在处理大规模数据时能够高效地进行数据传输，从而充分发挥其并行计算能力。 

##### 4. 应用场景 - 

**CPU**：由于其通用性和灵活性，CPU 适用于各种类型的计算任务，包括日常办公、操作系统管理、数据库查询、串行算法等。在这些场景中，任务的复杂度较高，需要 CPU 强大的逻辑处理能力和单核性能。 -

**GPU**：GPU 主要应用于需要大规模并行计算的领域，如图形渲染、游戏开发、深度学习、科学计算、密码学等。在这些场景中，任务通常可以被分解为大量的独立子任务，适合由 GPU 的多个核心同时处理。 

##### 5. 功耗和成本 - 

**CPU**：CPU 的功耗相对较低，尤其是在处理低负载任务时。此外，CPU 的价格相对较高，特别是高端的服务器级 CPU。 - 

**GPU**：GPU 的功耗较高，尤其是在进行大规模并行计算时。然而，由于 GPU 的大规模生产和广泛应用，其价格相对较为亲民，特别是对于消费级 GPU。

#### 四、代码示例

为了演示如何使用PyTorch和GPU进行神经网络训练，我们以一个简单的多层感知机（MLP）为例。这是一个简单的线性二分类问题，我们使用随机生成的数据集进行训练。利用代码生成了一个包含1000个样本的数据集，每个样本具有20个特征。类别标签为0或1。

```
import numpy as np
 
np.random.seed(0)
NUM_SAMPLES = 1000
NUM_FEATURES = 20
 
X = np.random.randn(NUM_SAMPLES, NUM_FEATURES)
y = np.random.randint(0, 2, (NUM_SAMPLES,))
 
print("X shape:", X.shape)
print("y shape:", y.shape)
 
 
### 构建简单的神经网络
 
#接下来，我们使用PyTorch框架来构建一个简单的多层感知机。这个感知机包括一个输入层、一个隐藏层和一个输出层。
 
import torch
import torch.nn as nn
 
class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
 
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
 
INPUT_SIZE = 20
HIDDEN_SIZE = 10
OUTPUT_SIZE = 1
 
model = SimpleMLP(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)
 
print(model)
 
 
### 训练神经网络
 
#现在，我们将训练这个简单神经网络。为了使用GPU进行训练，请确保已经安装了适当的PyTorch GPU版本。
 
# 判断是否有GPU可用，如果有，则将模型和数据移动到GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
 
 
### 训练循环
 
#下面的代码将执行训练循环，并在每个循环后输出训练损失。
 
# 超参数设置
learning_rate = 0.001
num_epochs = 500
batch_size = 40
num_batches = NUM_SAMPLES // batch_size
 
# 创建优化器和损失函数
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.BCEWithLogitsLoss()
 
# 转换数据为PyTorch张量
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)
 
# 训练循环
for epoch in range(num_epochs):
    for i in range(num_batches):
        start = i * batch_size
        end = start + batch_size
        inputs = X[start:end].to(device)  # 将数据移动到GPU
        targets = y[start:end].to(device)  # 将数据移动到GPU
 
        outputs = model(inputs)
        loss = criterion(outputs, targets)
 
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
 
    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}")
```

运行结果：

```
，，，
Epoch [489/500], Loss: 0.476614773273468
Epoch [490/500], Loss: 0.47668877243995667
Epoch [491/500], Loss: 0.47654348611831665
Epoch [492/500], Loss: 0.47667455673217773
Epoch [493/500], Loss: 0.4764176309108734
Epoch [494/500], Loss: 0.476365864276886
Epoch [495/500], Loss: 0.47667425870895386
Epoch [496/500], Loss: 0.47667378187179565
Epoch [497/500], Loss: 0.4764541685581207
Epoch [498/500], Loss: 0.47650662064552307
Epoch [499/500], Loss: 0.47656387090682983
Epoch [500/500], Loss: 0.4765079915523529
```

### 17-隐马尔科夫模型在序列问题的应用

####  一、隐马尔可夫模型简介

隐马尔可夫模型是一种具有隐藏状态的马尔可夫过程。在这个模型中，我们观察到的序列是由一个隐藏的马尔可夫链生成的。具体来说，隐马尔可夫模型由以下三部分组成：

**状态序列：**表示系统的内部状态，通常不可直接观察。

**观测序列：**根据状态序列生成的可观察到的序列。

**参数：**包括状态转移概率矩阵、观测概率矩阵和初始状态概率向量。

![dac8125613188d8ba0cd12c84053ab98](image\dac8125613188d8ba0cd12c84053ab98.png)

#### 二、隐马尔可夫模型的三个基本问题

隐马尔可夫模型需要解决以下三个基本问题：

**概率计算问题：**给定模型参数和观测序列，计算该观测序列出现的概率。

**学习问题：**给定观测序列，估计模型的参数。

**预测问题：**给定观测序列和模型参数，找到最有可能的隐藏状态序列。

#### 三、生活中的应用实例

假设有一个简化的天气系统，其中天气有两种状态：晴天（Sunny）和雨天（Rainy）。我们不能直接观察到天气，但是可以通过人们的穿着（例如戴太阳镜或雨伞）来间接地观察天气。我们将使用隐马尔可夫模型来描述这个系统，并通过给定的观测序列来预测天气状态。

#### 四、代码实现与解析

本文通过使用Python实现的简单隐马尔可夫模型。代码实现了`forward`算法来计算观测序列的概率，`viterbi`算法来预测隐藏状态序列。

```
import numpy as np
 
class HiddenMarkovModel:
    def __init__(self, transition_matrix, observation_matrix, initial_prob):
        self.transition_matrix = transition_matrix
        self.observation_matrix = observation_matrix
        self.initial_prob = initial_prob
 
    def forward(self, observations):
        alpha = np.zeros((len(observations), len(self.initial_prob)))
 
        alpha[0] = self.initial_prob * self.observation_matrix[:, observations[0]]
 
        for t in range(1, len(observations)):
            alpha[t] = np.dot(alpha[t - 1], self.transition_matrix) * self.observation_matrix[:, observations[t]]
 
        return alpha, np.sum(alpha[-1])
 
    def viterbi(self, observations):
        path = np.zeros(len(observations), dtype=int)
        delta = np.zeros((len(observations), len(self.initial_prob)))
        psi = np.zeros((len(observations), len(self.initial_prob)))
 
        delta[0] = self.initial_prob * self.observation_matrix[:, observations[0]]
 
        for t in range(1, len(observations)):
            for j in range(len(self.initial_prob)):
                delta[t, j] = np.max(delta[t - 1] * self.transition_matrix[:, j]) * self.observation_matrix[j, observations[t]]
                psi[t, j] = np.argmax(delta[t - 1] * self.transition_matrix[:, j])
 
        path[-11] = np.argmax(delta[-1])
 
        for t in range(len(observations) - 2, -1, -1):
            path[t] = psi[t + 1, path[t + 1]]
 
        return path
 
if __name__ == "__main__":
    transition_matrix = np.array([[0.7, 0.3], [0.4, 0.6]])  # 状态转移矩阵
    observation_matrix = np.array([[0.9, 0.1], [0.2, 0.8]])  # 观测概率矩阵
    initial_prob = np.array([0.6, 0.4])  # 初始状态概率向量
 
    hmm = HiddenMarkovModel(transition_matrix, observation_matrix, initial_prob)
 
    observations = [0, 0, 1, 1, 0]  # 0代表戴太阳镜，1代表撑雨伞
 
    _, prob = hmm.forward(observations)
    print(f"观测序列概率：{prob:.4f}")
 
    hidden_states = hmm.viterbi(observations)
    print(f"最有可能的隐藏状态序列：{''.join(['S' if state == 0 else 'R' for state in hidden_states])}")
```

以上我定义了一个`HiddenMarkovModel`类，其中`forward`方法实现了前向算法计算观测序列的概率，`viterbi`方法实现了Viterbi算法来预测最有可能的隐藏状态序列。在`__main__`部分，我们生成了一个简化的天气系统的隐马尔可夫模型，并通过观测序列`[0, 0, 1, 1, 0]`（戴太阳镜、戴太阳镜、撑雨伞、撑雨伞、戴太阳镜）计算了观测序列的概率和最可能的天气状态序列。

#### **五、总结**

本文详细介绍了隐马尔可夫模型的原理，解决三个基本问题的方法，并通过一个简化的天气系统的例子提供了完整的代码实现。隐马尔可夫模型在许多实际应用中都有非常高的价值，如语音识别、自然语言处理等。希望本文能帮助大家更好地理解隐马尔可夫模型，并将其应用到实际问题中。

### 18-条件随机场CRF模型的应用

#### 一、条件随机场(CRF)原理

条件随机场（CRF）是一种用于建模输入序列与输出序列之间的依赖关系的统计模型。CRF广泛应用于各种自然语言处理任务，如词性标注、命名实体识别和语义角色标注等。CRF的主要优点是能够明确地建模观测数据与标签之间的依赖关系，同时考虑整个序列的上下文信息。

CRF基于图模型，其中输入序列表示为节点，依赖关系表示为边。CRF的主要目标是学习一个条件概率分布，用于预测输出序列中的标签。具体而言，给定输入序列𝑥和输出序列𝑦，CRF试图通过最大化条件概率𝑃(𝑦|𝑥)来学习权重参数。

![b04d1bc060ec344f7156273afd6bb0be](image\b04d1bc060ec344f7156273afd6bb0be.png)

#### 二、CRF的优缺点

优点：

1.模型表现力强：CRF能够对标记之间的依赖关系建模，因此能够处理更加复杂的序列标注任务。

2.预测准确率高：CRF对于模型训练和预测都采用了统计学习的方法，所以预测准确率相对比较高。

3.可解释性强：CRF的特征函数定义比较直观，因此可以帮助我们理解模型的预测过程。

缺点

1.训练速度比较慢：CRF需要对整个训练集进行参数估计，时间复杂度较高，对于大规模数据集训练过程比较缓慢。

2.特征选择比较困难：CRF的性能比较依赖于特征函数的选择和设计，因此需要手动设计特征函数。

3.对于没有显式标记的数据来说准确率会比较低。

#### 三、生活中的应用代码样例

假设我们有一个简单的任务，根据天气和温度预测人们是否会外出。我们将使用CRF模型对这个问题进行建模。

数据样例

我们的数据样例存储在`data/sample_data.csv`文件中，内容如下：

```
weather,temperature,go_out
sunny,high,No
sunny,low,Yes
rain,high,No
rain,low,No
cloudy,high,Yes
cloudy,high,Yes
```

数据预处理

首先，我们需要对数据进行预处理。在`data_preprocessing.py`文件中实现如下

```
# -*- coding: utf-8 -*-
import pandas as pd
 
def read_data(file_path):
    dataset = pd.read_csv(file_path,encoding='utf-8')
    dataset.columns = ['weather', 'temperature', 'go_out']
    return dataset
 
def preprocess_data(dataset):
    X = dataset[['weather', 'temperature']].values.tolist()
    y = dataset['go_out'].values.tolist()
 
    return X, y
```

CRF模型

接下来，我们在`crf_model.py`中实现CRF模型：

```
# -*- coding: utf-8 -*-
from sklearn_crfsuite import CRF
 
def train_crf(X_train, y_train):
    crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)
    y_train = [[label,label] for label in y_train]
    #print(X_train, y_train)
    crf.fit(X_train, y_train)
    return crf
 
def predict(crf, X_test):
    return crf.predict(X_test)
```

主程序 最后，在main.py文件中编写主程序：

```
# -*- coding: utf-8 -*-
from data_preprocessing import read_data, preprocess_data
from crf_model import train_crf, predict
 
def main():
    # 读取数据
    dataset = read_data('data/sample_data.csv')
 
    # 预处理数据
    X, y = preprocess_data(dataset)
 
    # 训练CRF模型
    crf = train_crf(X, y)
    # 预测
    X_test = [['sunny', 'low'], ['rain', 'low']]
    y_pred = predict(crf, X_test)
    y_pred = [s[0] for s in y_pred]
    # 输出预测结果
    print("预测结果：", y_pred)
 
 
if __name__ == '__main__':
    main()
```

运行模型
确保已安装sklearn-crfsuite库，然后运行main.py文件，查看预测结果：

预测结果： ['Yes', 'No']

#### 四、总结

本文介绍了条件随机场(CRF)模型的原理并通过一个简单的示例展示了如何在实际应用中使用CRF模型。我们从数据预处理开始，接着训练CRF模型并进行预测。最后，我们成功地完成了一个基于条件随机场的人工智能应用。在实际应用中，基本原理和方法仍然适用。

##  算法

在数据预处理和分析完成之后，我们可以使用各种机器学习算法进行预测。这些算法可以分为有监督学习和无监督学习。

有监督学习算法需要使用标记数据集进行训练，以生成预测模型。常用的有监督学习算法包括线性回归、决策树、随机森林、向量机分类模型(SVC算法)等。

无监督学习算法则不需要标记数据集，而是通过发现数据集中的潜在规律进行预测。常用的无监督学习算法包括聚类、降维等。

### 1-决策树算法详解

![c24aa974-3708-46b1-80d7-11c5c4e1d55b_1741917472874167090_origin~tplv-a9rns2rl98-image-qvalue](image\c24aa974-3708-46b1-80d7-11c5c4e1d55b_1741917472874167090_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

#### 一、引言

决策树算法是机器学习领域中一种极为重要且应用广泛的有监督学习算法。其核心优势在于能够以直观的树形结构对数据进行分类和预测，这种可视化特性使得模型易于理解和解释，无论是在理论研究还是实际业务场景中都备受青睐。

#### 二、决策树的基本结构

决策树由节点和边构成，主要包含以下三种类型的节点：

1. **根节点**：处于决策树的顶端，是整个决策过程的起始点，包含了所有的训练数据。

1. **内部节点**：每个内部节点代表一个属性上的测试条件。例如，在预测水果类别时，内部节点可能是 “颜色是否为红色” 这样的条件判断。

1. **叶子节点**：位于决策树的末端，每个叶子节点都对应一个具体的类别标签（在分类问题中）或数值（在回归问题中），表示经过一系列条件判断后得出的最终决策结果。

节点之间通过边相互连接，边代表属性的取值路径。从根节点开始，数据沿着满足测试条件的边向下流动，直至到达叶子节点，从而得到相应的分类或预测结果。

#### 三、决策树的构建过程

1. **特征选择**

这是构建决策树的关键步骤，目的是从众多特征中挑选出对分类或预测最具影响力的特征。常见的特征选择指标有信息增益、信息增益比和基尼指数等。

- **信息增益**：基于信息论中的熵的概念。熵衡量的是数据的不确定性，信息增益表示在一个特征上进行划分后，数据不确定性减少的程度。信息增益越大，说明该特征对分类的贡献越大。以ID3算法为例，它采用信息增益作为特征选择的标准。
- **信息增益比**：信息增益在选择特征时存在偏向于取值较多特征的问题，信息增益比通过引入分裂信息对信息增益进行修正，从而避免这种偏差，C4.5算法使用信息增益比来选择特征。
- **基尼指数**：反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。基尼指数越小，数据集的纯度越高。CART算法在构建决策树时采用基尼指数来选择特征。

1. **决策树生成**

- 从根节点开始，基于选定的特征对数据集进行划分。例如，若选择“颜色”作为根节点的划分特征，根据不同的颜色取值，将数据集划分为多个子集。
- 对每个划分得到的子集递归地重复上述特征选择和划分过程，不断生成新的节点和分支，直至满足某个停止条件。停止条件通常包括：子集中的样本都属于同一类别，无需再进行划分；所有特征都已被使用，无法进一步划分；子集中的样本数量小于某个阈值等。

1. **剪枝**

决策树在生成过程中容易出现过拟合现象，即模型对训练数据过于适应，而对新数据的泛化能力较差。剪枝是解决过拟合问题的重要手段。

- **预剪枝**：在决策树生成过程中，提前对节点进行评估，若当前节点的划分不能带来模型性能的提升（如验证集上的准确率没有提高），则停止对该节点的划分，直接将其标记为叶子节点。
- **后剪枝**：先构建完整的决策树，然后从叶子节点开始，自下而上地对非叶子节点进行评估。若将某个非叶子节点转换为叶子节点后，模型在验证集上的性能得到提升（如泛化误差减小），则进行剪枝操作，将该节点及其子树替换为叶子节点。

#### 四、决策树的分类与预测过程

1. **分类过程**：对于一个新的待分类样本，从决策树的根节点开始，依次根据节点上的测试条件对样本的特征进行判断，按照符合条件的边向下移动，直至到达某个叶子节点，该叶子节点对应的类别即为样本的预测类别。

1. **预测过程（回归问题）**：在回归问题中，决策树的叶子节点存储的是一个数值。同样从根节点开始，依据样本特征沿着决策树向下遍历，到达叶子节点后，该叶子节点存储的数值就是对样本的预测值。

#### 五、决策树算法的优缺点

1. **优点**

- **可解释性强**：决策树的树形结构清晰展示了从特征到决策结果的推理过程，业务人员或非专业人士也能够轻松理解模型的决策逻辑。
- **处理非线性数据能力强**：决策树可以通过多次划分特征空间，有效处理数据中的非线性关系，无需对数据进行复杂的预处理或特征工程。
- **对数据要求低**：对数据的分布没有严格要求，无论是连续型数据还是离散型数据都能很好地处理，并且对缺失值也有一定的容忍度。

1. **缺点**

- **容易过拟合**：由于决策树在构建过程中极力拟合训练数据，可能会捕捉到数据中的噪声和细微的局部特征，导致过拟合，影响模型在新数据上的表现。
- **对数据变化敏感**：训练数据的微小变动可能会导致决策树的结构发生较大变化，从而影响模型的稳定性和泛化能力。
- **缺乏全局最优性**：决策树的构建过程是基于贪心策略，每次选择局部最优的特征进行划分，无法保证得到的决策树是全局最优的。

#### 六、决策树算法的应用场景

1. **医疗诊断**：医生可以根据患者的症状、检查结果等特征，利用决策树模型快速判断患者可能患有的疾病类别，辅助医疗诊断决策。例如，根据患者的体温、咳嗽症状、白细胞计数等特征来判断是普通感冒、流感还是其他疾病。

1. **金融风险评估**：金融机构可以通过客户的年龄、收入、信用记录、负债情况等特征构建决策树模型，评估客户的信用风险，决定是否给予贷款以及贷款额度等。

1. **市场营销**：企业根据客户的年龄、性别、消费行为、购买历史等特征，使用决策树模型对客户进行分类，针对不同类别的客户制定个性化的营销策略，提高营销效果和客户满意度。

1. **数据分析与探索**：在数据分析的早期阶段，决策树可以帮助分析师快速了解数据中各个特征对目标变量的影响关系，发现数据中的潜在模式和规律，为进一步的数据分析和建模提供指导。

### 2-支持向量机算法详解

#### 一、引言

支持向量机（Support Vector Machine，SVM）是一种有监督的机器学习算法，在分类、回归等问题上表现卓越。其核心思想是在特征空间中找到一个最优的分类超平面，使不同类别的数据点尽可能地远离该超平面，从而实现对数据的高效分类与预测，在学术界和工业界都得到广泛应用。

#### 二、基本原理

1. **线性可分情况**

- 对于线性可分的数据集，SVM 的目标是寻找一个超平面，将不同类别的数据点完全分开，并且使该超平面与最近的数据点之间的距离最大化，这个距离被称为间隔（Margin）。

- 假设我们有一个二分类问题，数据点\(x_i\in R^n\)，类别标签\(y_i\in\{ - 1,1\}\)。超平面可以表示为\(w^Tx + b = 0\)，其中\(w\)是超平面的法向量，决定了超平面的方向，\(b\)是偏置项，决定了超平面与原点的距离。

- 距离超平面最近的几个数据点被称为支持向量（Support Vectors），间隔的大小为\(\frac{2}{\|w\|}\)。通过最大化间隔，即最小化\(\frac{1}{2}\|w\|^2\)，同时满足约束条件\(y_i(w^Tx_i + b)\geq1\)（对于所有的\(i\)），可以找到最优的超平面。这是一个凸二次规划问题，利用拉格朗日乘子法可以将其转化为对偶问题进行求解。

1. **线性不可分情况**

- 当数据在原始特征空间中线性不可分时，SVM 引入松弛变量\(\xi_i\geq0\)，允许一些数据点在一定程度上违反分类约束。此时目标函数变为\(\min_{w,b,\xi}\frac{1}{2}\|w\|^2 + C\sum_{i = 1}^{m}\xi_i\)，约束条件变为\(y_i(w^Tx_i + b)\geq1-\xi_i\)（对于所有的\(i\)），其中\(C\gt0\)是惩罚参数，用于平衡间隔最大化和对误分类样本的惩罚程度。\(C\)值越大，对误分类的惩罚越重；\(C\)值越小，对误分类的惩罚越轻，更注重间隔的最大化。同样通过拉格朗日乘子法转化为对偶问题求解。

#### 三、核函数

1. **作用**

- 对于非线性问题，SVM 通过核函数将低维的原始特征空间映射到高维特征空间，使得在高维空间中数据变得线性可分，从而可以使用线性 SVM 的方法进行处理。

1. **常见核函数**

- **线性核函数**：\(K(x_i,x_j)=x_i^Tx_j\)，适用于数据本身在原始特征空间中接近线性可分的情况，计算简单，效率高。

- **多项式核函数**：\(K(x_i,x_j)=( \gamma x_i^Tx_j + r)^d\)，其中\(\gamma\gt0\)，\(r\)和\(d\)为参数。多项式核函数可以学习到数据的多项式关系，随着\(d\)的增大，模型复杂度增加，能够处理更复杂的非线性关系。

- **高斯核函数（径向基函数，RBF）**：\(K(x_i,x_j)=\exp(-\gamma\|x_i - x_j\|^2)\)，其中\(\gamma\gt0\)。高斯核函数具有很强的灵活性，它可以将数据映射到无限维的特征空间，对各种复杂的非线性数据都有很好的处理能力，是应用最广泛的核函数之一。

- **Sigmoid 核函数**：\(K(x_i,x_j)=\tanh(\gamma x_i^Tx_j + r)\)，它将数据映射到 Sigmoid 函数的值域空间，在某些特定的问题中表现良好。

#### 四、模型求解

1. **对偶问题求解**

- 通过拉格朗日乘子法将原问题转化为对偶问题后，对偶问题是一个关于拉格朗日乘子\(\alpha_i\)的二次规划问题。求解对偶问题得到\(\alpha_i\)的最优解，然后可以计算出\(w=\sum_{i = 1}^{m}\alpha_iy_ix_i\)和\(b\)（通常通过支持向量对应的约束条件求解）。

1. **二次规划求解方法**

- 常用的求解二次规划问题的方法有 SMO（Sequential Minimal Optimization）算法等。SMO 算法将大的二次规划问题分解为一系列小的二次规划子问题进行求解，大大提高了计算效率。它每次只优化两个拉格朗日乘子，通过不断迭代直到满足收敛条件。

#### 五、支持向量机算法的优缺点

1. **优点**

- **泛化能力强**：通过最大化间隔，SVM 能够在训练数据上找到一个较为鲁棒的分类超平面，对未知数据具有较好的泛化性能，尤其在小样本、非线性问题上表现出色。

- **适合高维数据**：借助核函数，SVM 可以有效地处理高维特征空间的数据，而无需像其他一些算法那样担心维度灾难问题。

- **可解释性相对较好**：决策边界由支持向量决定，支持向量的数量通常相对较少，这使得我们可以通过分析支持向量来理解模型的决策过程，相比一些复杂的深度学习模型具有更好的可解释性。

1. **缺点**

- **计算复杂度较高**：在训练过程中，尤其是对于大规模数据集，求解二次规划问题的计算量较大，时间复杂度较高。虽然有 SMO 等优化算法，但在处理大数据集时仍然面临挑战。

- **参数选择困难**：SVM 的性能对核函数的选择以及惩罚参数\(C\)等参数非常敏感，不同的参数设置可能导致模型性能差异很大，而参数的调优往往需要一定的经验和多次实验。

- **对缺失值敏感**：如果数据集中存在较多的缺失值，需要进行额外的处理，否则会影响模型的性能。

#### 六、应用场景

1. **图像识别**：在图像分类任务中，SVM 可以通过提取图像的特征（如 HOG 特征、SIFT 特征等），利用核函数将图像数据映射到合适的特征空间进行分类，例如识别手写数字、识别不同类别的物体等。

1. **生物信息学**：用于基因表达数据分析、蛋白质结构预测等。例如，通过分析基因表达数据来区分正常细胞和癌细胞，或者根据蛋白质的序列特征预测其结构类别。

1. **文本分类**：对大量的文本数据进行分类，如新闻分类、邮件分类、情感分析等。将文本表示为向量形式后，SVM 可以有效地对文本进行分类，利用核函数处理文本数据中的非线性关系。

1. **故障诊断**：在工业领域，通过对设备运行过程中的各种特征数据进行分析，使用 SVM 构建故障诊断模型，判断设备是否处于正常运行状态，以及识别不同类型的故障。

### 3-随机森林算法详解

![6f865dab-95d7-4b65-90df-ddb405e03a1e_1741917831167047333_origin~tplv-a9rns2rl98-image-qvalue](image\6f865dab-95d7-4b65-90df-ddb405e03a1e_1741917831167047333_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

#### 一、基本概念

随机森林（Random Forest）是一种集成学习算法，由多个决策树组成。它基于 Bagging（Bootstrap Aggregating）思想，通过构建多个相互独立的决策树，并将这些决策树的预测结果进行综合，从而得出最终的预测。随机森林能够有效地减少单一决策树的过拟合问题，提升模型的泛化能力和稳定性。

#### 二、构建过程

1. **样本随机抽样**

从原始训练数据集\(D\)中，通过有放回抽样的方式，构建\(m\)个与原始数据集大小相同的自助样本集\(D_1, D_2, \cdots, D_m\)。每个自助样本集都可能包含原始数据集中的重复样本，同时也会有部分样本未被抽到，这些未被抽到的样本称为袋外数据（Out-of-Bag，OOB）。

1. **特征随机选择**

对于每个自助样本集，在构建决策树时，不是考虑所有的特征，而是从\(d\)个特征中随机选择\(k\)个特征（\(k \lt d\)），然后在这\(k\)个特征中选择最优的特征进行节点分裂。通常\(k\)的取值为\(\sqrt{d}\)。这种特征随机选择的方式，进一步增加了决策树之间的差异性，降低了决策树之间的相关性。

1. **决策树构建**

基于每个自助样本集，利用选定的特征构建决策树。决策树的构建过程与普通决策树类似，例如使用 ID3、C4.5 或 CART 算法来选择特征进行节点分裂，直至满足一定的停止条件，如节点中的样本数小于某个阈值、树的深度达到上限等。

#### 三、模型训练

1. **训练决策树**

对每个自助样本集构建的决策树进行独立训练，每个决策树在训练过程中，根据选定的特征对样本进行划分，学习数据中的模式和规律。

1. **综合决策树结果**

在分类任务中，随机森林的最终预测结果通常采用投票法，即每个决策树对测试样本进行预测，将每个决策树预测的类别进行投票，得票数最多的类别作为随机森林的最终预测类别。在回归任务中，通常采用平均法，将每个决策树预测的数值结果进行平均，得到随机森林的最终预测值。

1. **袋外数据评估**

由于每个自助样本集都有袋外数据，这些袋外数据可以用于对随机森林模型进行评估。在训练过程中，不需要额外划分验证集，直接利用袋外数据计算模型的误差等指标，评估模型的性能。袋外误差（OOB Error）是随机森林特有的一种性能评估指标，它能够较为准确地反映模型的泛化能力。

#### 四、随机森林算法的优缺点

1. **优点**

- **抗过拟合能力强**：通过构建多个决策树并综合其结果，减少了单一决策树容易出现的过拟合问题。决策树之间的差异性使得模型能够学习到数据中更广泛的特征和模式，提高了模型的泛化能力。

- **处理高维数据能力好**：不需要对数据进行特征选择或降维处理，能够自动处理高维数据中的复杂关系。在特征随机选择的过程中，模型能够发现数据中对预测最有贡献的特征组合。

- **鲁棒性强**：对数据中的噪声和异常值具有较好的容忍度。因为多个决策树的综合结果会在一定程度上抵消噪声和异常值对预测的影响。

- **可并行化训练**：由于各个决策树的构建相互独立，因此可以在多台计算机上并行进行训练，大大缩短了训练时间，提高了训练效率。

1. **缺点**

- **模型复杂度高**：由多个决策树组成，模型的存储和计算需求相对较大。在解释模型时，由于涉及多个决策树，相比单一决策树，其可解释性会有所降低。

- **对小数据集效果可能不佳**：对于样本量较小的数据，由于自助抽样的随机性，可能导致构建的决策树之间差异不大，无法充分发挥随机森林的优势。

- **调参相对复杂**：虽然随机森林对参数不太敏感，但仍然存在一些参数需要调整，如决策树的数量、特征选择的数量等。不同的参数设置可能会对模型性能产生一定影响，调参过程需要一定的经验和多次实验。

#### 五、应用场景

1. **金融风险评估**：用于评估客户的信用风险，通过分析客户的年龄、收入、信用记录、负债情况等多个特征，预测客户违约的可能性，帮助金融机构做出贷款决策。

1. **医疗诊断**：辅助医生进行疾病诊断，根据患者的症状、检查结果、病史等多方面数据，判断患者患某种疾病的概率，例如判断患者是否患有癌症、心脏病等。

1. **市场营销**：对客户进行细分和目标定位，通过分析客户的消费行为、购买历史、人口统计学特征等数据，将客户分为不同的群体，针对不同群体制定个性化的营销策略，提高营销效果。

1. **图像识别**：在图像分类任务中，提取图像的各种特征后，利用随机森林对图像进行分类，如识别不同类型的动物、植物图像，或者对交通标志进行识别等。

1. **数据挖掘与分析**：从大量的数据中挖掘潜在的模式和规律，发现数据中变量之间的关系，例如在电商数据中，分析用户的购买行为与商品属性之间的关系，为商家提供决策支持。

### 4-K 近邻分类算法详解

![6f865dab-95d7-4b65-90df-ddb405e03a1e_1741917831167047333_origin~tplv-a9rns2rl98-image-qvalue](D:\learn\笔记\人工智能\人工智能笔记\image\9fe18cc5-14ce-45f4-a17c-703ac931c33b_1741918061830734663_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

#### 一、引言

K 近邻（K-Nearest Neighbors，KNN）算法是一种基本的分类与回归算法，属于有监督学习算法。其原理简单直观，在机器学习领域有着广泛的应用，尤其在数据分类任务中表现出色。

#### 二、算法原理

1. **核心思想**：对于一个新的待分类样本，KNN 算法会在已有的训练数据集中，找到与该样本距离最近的 K 个邻居样本。然后，根据这 K 个邻居样本的类别分布情况，通过投票或加权投票等方式，来确定新样本的类别。例如，在一个二分类问题中，如果这 K 个邻居样本中有多数属于类别 A，那么新样本就被分类为类别 A。

1. **决策边界**：随着 K 值的不同，KNN 算法形成的决策边界也有所不同。较小的 K 值会使决策边界更加复杂，对局部数据的变化更为敏感，容易过拟合；较大的 K 值则会使决策边界更加平滑，对全局数据的趋势把握更好，但可能会导致欠拟合。

#### 三、距离度量

1. **欧氏距离**：在特征空间中，欧氏距离是最常用的距离度量方式。对于两个 n 维向量 X=(x1,x2,...,xn) 和 Y=(y1,y2,...,yn)，它们之间的欧氏距离计算公式为：\(d(X,Y)=\sqrt{\sum_{i = 1}^{n}(x_i - y_i)^2}\)。例如，在二维平面上，点 (1,2) 和点 (4,6) 之间的欧氏距离为\(\sqrt{(4 - 1)^2+(6 - 2)^2}=\sqrt{9 + 16}=\sqrt{25}=5\)。

1. **曼哈顿距离**：曼哈顿距离又称城市街区距离，它是各维度坐标数值差的绝对值之和。对于上述两个 n 维向量 X 和 Y，曼哈顿距离计算公式为：\(d(X,Y)=\sum_{i = 1}^{n}|x_i - y_i|\)。在二维平面上，点 (1,2) 和点 (4,6) 之间的曼哈顿距离为\(|4 - 1|+|6 - 2|=3 + 4 = 7\)。

1. **闵可夫斯基距离**：闵可夫斯基距离是欧氏距离和曼哈顿距离的一般化形式，其计算公式为：\(d(X,Y)=(\sum_{i = 1}^{n}|x_i - y_i|^p)^{\frac{1}{p}}\)。当 p=2 时，就是欧氏距离；当 p=1 时，就是曼哈顿距离。

#### 四、K 值选择

1. **交叉验证**：K 值的选择对 KNN 算法的性能至关重要。通常采用交叉验证的方法来确定最优的 K 值。例如，将训练数据集分为 k 折（一般 k 取 5 或 10），每次使用其中一折作为验证集，其余 k-1 折作为训练集。在不同的 K 值下进行模型训练和验证，计算模型在验证集上的准确率等指标。通过比较不同 K 值下模型的性能，选择使模型性能最优的 K 值。

1. **经验法则**：在没有先验知识的情况下，可以先从一个较小的 K 值（如 K=3）开始尝试，然后逐渐增大 K 值，观察模型性能的变化趋势。一般来说，当数据集规模较大时，K 值可以适当选大一些；当数据集规模较小时，K 值宜选择较小的值。

#### 五、算法实现步骤

1. **数据准备**：收集并整理训练数据集，包括特征数据和对应的类别标签。同时，准备好待分类的新样本数据。

1. **计算距离**：对待分类的新样本，计算它与训练数据集中每个样本的距离，可选用上述提到的距离度量方法。

1. **选取 K 个邻居**：根据计算得到的距离，按照从小到大的顺序进行排序，选取距离最近的 K 个邻居样本。

1. **分类决策**：统计这 K 个邻居样本中不同类别的数量，采用投票法，将新样本分类为出现次数最多的类别；也可以采用加权投票法，距离越近的邻居样本权重越大，根据权重计算各类别的得分，将新样本分类为得分最高的类别。

#### 六、KNN 算法的优缺点

1. **优点**

- **简单易实现**：算法原理直观，实现过程相对简单，不需要复杂的模型训练过程。

- **对非线性数据适应性强**：KNN 算法不依赖于数据的分布形式，能够处理各种复杂的非线性数据关系，只要数据中存在局部相似性，就能够进行有效的分类。

- **无需假设数据分布**：不像一些其他算法（如线性回归假设数据呈线性分布），KNN 算法对数据的分布没有任何先验假设，适用于各种类型的数据。

1. **缺点**

- **计算量大**：在预测阶段，需要计算待分类样本与所有训练样本的距离，当训练数据集规模较大时，计算量会非常大，导致预测效率低下。

- **存储需求高**：需要存储全部的训练数据集，对于大规模数据，这会占用大量的内存空间。

- **对 K 值敏感**：K 值的选择直接影响模型的性能，若 K 值选择不当，容易导致过拟合或欠拟合问题。

#### 七、应用场景

1. **图像识别**：在手写数字识别中，将待识别的手写数字图像的特征与训练集中已标注数字图像的特征进行比较，通过 KNN 算法找到最相似的 K 个图像，根据这 K 个图像的类别确定待识别数字的类别。

1. **推荐系统**：根据用户的历史行为数据（如购买记录、浏览记录等）构建用户特征向量，当为新用户推荐商品时，利用 KNN 算法找到与新用户特征最相似的 K 个用户，将这 K 个用户购买或浏览过的商品推荐给新用户。

1. **疾病诊断辅助**：医生根据患者的症状、检查结果等特征数据，通过 KNN 算法在已有的病例数据集中找到与之相似的 K 个病例，参考这 K 个病例的诊断结果，辅助对新患者进行疾病诊断。

1. **文本分类**：将文本转化为向量形式（如词向量）后，对于一篇待分类的新文本，利用 KNN 算法找到与它最相似的 K 篇已分类文本，根据这 K 篇文本的类别确定新文本的类别，例如对新闻文章进行分类、对邮件进行垃圾邮件与正常邮件的分类等。

### 5-聚类算法详解

![31808793-2e90-4061-9676-18f45d0aed92_1741918246686983158_origin~tplv-a9rns2rl98-image-qvalue](image\31808793-2e90-4061-9676-18f45d0aed92_1741918246686983158_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

#### 一、引言

聚类算法是一类无监督学习算法，旨在将数据集中的样本划分为多个组或簇，使得同一簇内的样本具有较高的相似性，而不同簇之间的样本具有较大的差异性。其主要作用是发现数据的内在结构和分布规律，为数据分析、模式识别、数据挖掘等领域提供重要支持。

#### 二、常见聚类算法类型

1. **划分聚类算法**

- **K-Means 算法**：是最经典的划分聚类算法之一。它预先设定要划分的簇的数量 K，随机选取 K 个点作为初始聚类中心。然后，计算每个样本到各个聚类中心的距离，将样本分配到距离最近的聚类中心所在的簇。接着，重新计算每个簇的中心（通常是簇内所有样本的均值）。不断重复上述过程，直到聚类中心不再变化或达到预设的迭代次数。例如，在对一群客户的消费数据进行聚类时，可通过 K-Means 算法将客户按消费行为特点分为 K 个不同的群体。

- **K-Medoids 算法**：与 K-Means 类似，但 K-Medoids 算法选择簇内实际存在的样本点作为簇中心（称为 medoid），而不是像 K-Means 那样计算均值。这种方式对离群点的敏感度更低，因为均值易受离群点影响，而 medoid 是实际样本点。例如在地理坐标数据聚类中，K-Medoids 能更好地应对可能存在的异常坐标点。

1. **层次聚类算法**

- **凝聚式层次聚类**：从每个样本作为一个单独的簇开始，逐步合并相似的簇。通过计算簇与簇之间的距离（如最小距离、最大距离、平均距离等），每次将距离最近的两个簇合并，直到所有样本都在一个簇中或满足特定停止条件。例如在对生物物种进行分类时，可从每个物种作为一个簇，根据物种间的相似性逐步合并，构建出物种分类的层次结构。

- **分裂式层次聚类**：与凝聚式相反，它从包含所有样本的一个大簇开始，逐步分裂成更小的簇。根据一定的分裂准则（如最大化簇间差异），将一个大簇分裂成两个子簇，不断重复这个过程，直到每个簇只包含一个样本或满足停止条件。在图像分割任务中，可利用分裂式层次聚类将一幅图像逐步分割成具有不同特征的区域。

1. **密度聚类算法**

- **DBSCAN 算法**：基于数据点的密度进行聚类。它将数据空间划分为核心点、边界点和噪声点。核心点是在一定半径邻域内包含足够数量样本的点；边界点是在核心点邻域内，但自身邻域内样本数量不足的点；噪声点是不属于任何核心点邻域的点。DBSCAN 从一个核心点出发，将密度相连的点聚成一个簇，能发现任意形状的簇，并且能有效识别噪声点。例如在地理信息系统中，可利用 DBSCAN 对城市中的建筑物分布进行聚类，能很好地处理建筑物分布不规则的情况。

- **OPTICS 算法**：是 DBSCAN 的扩展，它通过为每个点计算一个可达距离和核心距离，构建出一个有序的点集。在聚类时，可以根据不同的密度阈值从这个有序点集中提取出不同的簇，不需要像 DBSCAN 那样预先指定聚类参数，能更灵活地处理不同密度分布的数据。例如在分析社交网络用户关系数据时，OPTICS 能根据用户之间联系的紧密程度，更准确地发现不同密度的用户群体。

1. **网格聚类算法**

- **STING 算法**：将数据空间划分为多个网格单元，预先计算每个网格单元的统计信息（如均值、方差等）。通过这些统计信息来进行聚类，计算速度快，适用于大规模数据。例如在对海量的气象数据进行聚类分析时，可将地理区域划分为网格单元，利用每个网格单元内气象数据的统计特征进行聚类，快速发现不同气象特征的区域。

- **WaveCluster 算法**：结合了信号处理中的小波变换思想。它先将数据投影到网格上，然后对网格单元进行小波变换，根据小波系数来确定簇的边界。能有效处理高维数据和噪声，在高维数据聚类分析中有较好表现，如在基因表达数据聚类中，可通过 WaveCluster 算法挖掘出具有相似基因表达模式的基因簇。

#### 三、聚类算法的评估指标

1. **外部指标**：需要借助已知的真实类别标签来评估聚类结果。

- **兰德指数（Rand Index，RI）**：计算聚类结果与真实类别标签之间的一致性程度。RI 值越接近 1，表示聚类结果与真实情况越吻合；RI 值为 0，表示聚类结果与随机划分没有区别。例如在对图像数据集进行聚类后，通过与图像的真实类别标签对比计算 RI 值，评估聚类效果。

- **调整兰德指数（Adjusted Rand Index，ARI）**：对 RI 进行了调整，消除了随机因素的影响。ARI 值范围在 [-1, 1] 之间，值越高表示聚类结果与真实类别越相似。在评估复杂数据集的聚类效果时，ARI 比 RI 更能准确反映聚类质量。

1. **内部指标**：仅依据聚类结果本身来评估。

- **轮廓系数（Silhouette Coefficient）**：综合考虑了样本与同簇内其他样本的紧密程度（凝聚度）以及与其他簇的分离程度。轮廓系数取值范围在 [-1, 1] 之间，值越接近 1，表示样本聚类效果越好，即样本既紧密聚集在所属簇内，又与其他簇有明显区分。例如在对客户数据进行聚类后，通过计算轮廓系数来评估聚类的质量，判断聚类结果是否合理。

- **Calinski-Harabasz 指数**：通过计算簇内方差和簇间方差的比值来评估聚类效果。该指数值越大，说明聚类效果越好，即簇内样本紧密，簇间分离明显。在比较不同聚类算法对同一数据集的聚类效果时，Calinski-Harabasz 指数是一个常用的评估指标。

#### 四、聚类算法的应用场景

1. **市场细分**：企业可根据客户的年龄、性别、消费行为、购买偏好等多维度数据，利用聚类算法将客户分为不同的细分市场。针对不同细分市场的特点，制定个性化的营销策略，提高市场推广效果和客户满意度。例如，将客户聚类为高消费、低消费、频繁购买、偶尔购买等不同群体，为每个群体提供定制化的产品推荐和促销活动。

1. **图像识别与处理**：在图像分割任务中，聚类算法可将图像中具有相似颜色、纹理等特征的像素点聚成一个区域，实现对图像的分割。例如，将一幅自然风景图像分割为天空、草地、树木等不同的区域，有助于图像分析和目标识别。在图像检索中，也可通过聚类算法将相似的图像聚成一组，提高检索效率。

1. **生物信息学**：对基因表达数据进行聚类，可发现具有相似表达模式的基因簇，有助于研究基因的功能和生物过程。在蛋白质结构分类中，聚类算法能将具有相似结构的蛋白质聚在一起，为蛋白质功能预测和药物研发提供支持。例如，通过聚类分析发现与某种疾病相关的基因簇，进一步研究这些基因在疾病发生发展中的作用机制。

1. **异常检测**：在数据集中，异常点通常与大多数正常点的特征分布不同。聚类算法可将正常数据点聚成簇，那些远离这些簇的点就可能被视为异常点。例如在网络流量监测中，通过聚类算法发现与正常流量模式不同的异常流量，及时检测网络攻击或故障。在信用卡交易数据中，也可利用聚类算法识别可能的欺诈交易。

1. **文本挖掘**：将文本数据（如新闻文章、学术论文、社交媒体帖子等）转化为向量形式后，聚类算法可将主题相似的文本聚成一组。这有助于文本分类、信息检索和话题发现。例如，将大量新闻文章聚类为政治、经济、文化、体育等不同主题的类别，方便用户快速浏览和获取感兴趣的信息。在舆情分析中，通过聚类算法将相似观点的文本聚类，能更好地了解公众对某一事件的看法和态度分布。

### 6-SVC 算法(向量机分类)详解

SVC 即 Support Vector Classification，是支持向量机（SVM）在分类任务中的具体实现。在你正在阅读的关于支持向量机算法的介绍中，已经涵盖了 SVC 算法的核心原理，这里将从 SVC 算法的特点、使用场景、代码示例等方面进一步展开。

#### 一、算法核心特点

1. **基于 SVM 原理**：SVC 完全遵循支持向量机寻找最优分类超平面的理念。在面对线性可分数据时，它致力于找到一个超平面，不仅能将不同类别的数据点分开，还能使该超平面与最近的数据点之间的间隔最大化。对于线性不可分数据，则引入松弛变量和惩罚参数\(C\)，在最大化间隔与允许一定程度误分类之间进行权衡。例如，在一个简单的二维数据分类场景中，两类数据点呈现出部分重叠的情况，SVC 会根据\(C\)值的设定，在尽量扩大间隔的同时，对落入错误一侧的数据点进行适当 “容忍”。

1. **灵活运用核函数**：和 SVM 一样，SVC 借助核函数来处理非线性分类问题。通过将低维原始特征空间映射到高维空间，原本线性不可分的数据在高维空间中可能变得线性可分。不同的核函数如线性核函数、多项式核函数、高斯核函数、Sigmoid 核函数等，为处理各种复杂的数据分布提供了多样的选择。例如在图像分类任务中，数据往往具有高度复杂的特征关系，使用高斯核函数的 SVC 能够有效将图像特征映射到合适的高维空间，实现精准分类。

#### 二、使用场景

1. **小样本分类**：由于 SVC 在小样本情况下也能通过最大化间隔找到较为鲁棒的分类超平面，具备良好的泛化能力，所以在小样本分类场景中表现出色。比如在珍稀物种的识别研究中，由于可获取的样本数量有限，SVC 可以基于这些少量样本构建有效的分类模型，准确识别物种类别。

1. **高维数据分类**：在处理高维数据时，SVC 利用核函数能够将数据映射到高维空间而无需担心维度灾难问题，使其在高维数据分类领域得到广泛应用。像在基因数据分析中，基因数据维度极高，SVC 能够对大量的基因特征进行分析，区分不同的基因表达模式类别。

1. **复杂边界分类**：当数据的分类边界呈现复杂的非线性形状时，SVC 通过合适的核函数可以很好地拟合这种复杂边界。例如在手写字符识别中，不同手写风格的字符数据边界复杂，SVC 能够通过选择恰当的核函数，精确划分不同字符类别。

#### 三、代码示例（以 Python 的 scikit - learn 库为例）

```
from sklearn import svm
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 生成模拟分类数据集
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建SVC分类器对象，使用默认参数（线性核函数）
clf = svm.SVC()

# 训练模型
clf.fit(X_train, y_train)

# 预测测试集
y_pred = clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print("模型准确率:", accuracy)
```

在上述代码中，首先使用make_classification函数生成了一个模拟的分类数据集。接着将数据集划分为训练集和测试集，然后创建了一个默认使用线性核函数的 SVC 分类器对象。通过调用fit方法对训练集进行训练，再使用训练好的模型对测试集进行预测，最后计算并输出模型在测试集上的准确率。如果需要使用其他核函数，只需在创建SVC对象时指定kernel参数，例如clf = svm.SVC(kernel='rbf')即可使用高斯核函数。

#### 四、与其他分类算法对比

1. **与决策树对比**：决策树算法的决策边界是基于特征的阈值划分，呈现出矩形区域，对于复杂的非线性边界拟合能力有限，且容易出现过拟合。而 SVC 借助核函数能够构建更复杂、平滑的决策边界，在处理非线性问题上更具优势。不过决策树算法计算速度快，对数据的解释性非常直观，而 SVC 计算复杂度较高，可解释性相对较弱。

1. **与 K 近邻对比**：K 近邻算法属于基于实例的学习算法，在预测时需要计算待预测样本与所有训练样本的距离，计算量较大，且对 K 值的选择非常敏感。SVC 在训练后得到一个固定的决策边界，预测时计算量小，并且在小样本、高维数据场景下表现优于 K 近邻算法。但 K 近邻算法不需要对数据进行复杂的训练过程，对于数据分布变化的适应性较强。

### 7-线性回归算法详解

![ff431048-75ec-4db1-be5b-04488efc8f0c_1741920224179261338_origin~tplv-a9rns2rl98-image-qvalue](image\ff431048-75ec-4db1-be5b-04488efc8f0c_1741920224179261338_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

#### 一、引言

线性回归算法是一种经典的回归分析方法，属于有监督学习算法。其目标是通过建立一个线性模型，描述自变量和因变量之间的关系，从而实现对因变量的预测。在众多领域，如经济学、物理学、数据分析等，线性回归都有着广泛的应用，为解决实际问题提供了有力的工具。

#### 二、算法原理

1. **一元线性回归**

- 对于一元线性回归，假设因变量\(y\)与自变量\(x\)之间存在线性关系，其数学模型可以表示为\(y = \beta_0+\beta_1x+\epsilon\)，其中\(\beta_0\)是截距，\(\beta_1\)是斜率，\(\epsilon\)是误差项，通常假设\(\epsilon\)服从均值为\(0\)的正态分布。

- 模型的任务就是通过给定的训练数据\((x_i,y_i)\)（\(i = 1,2,\cdots,n\)）来估计参数\(\beta_0\)和\(\beta_1\)的值。常用的估计方法是最小二乘法，其核心思想是找到一组\(\beta_0\)和\(\beta_1\)，使得预测值\(\hat{y}_i=\beta_0+\beta_1x_i\)与实际值\(y_i\)之间的误差平方和最小，即\(S(\beta_0,\beta_1)=\sum_{i = 1}^{n}(y_i - (\beta_0+\beta_1x_i))^2\)最小。通过对\(S(\beta_0,\beta_1)\)分别关于\(\beta_0\)和\(\beta_1\)求偏导数，并令偏导数为\(0\)，可以得到\(\beta_0\)和\(\beta_1\)的估计值。

1. **多元线性回归**

- 当存在多个自变量\(x_1,x_2,\cdots,x_p\)时，多元线性回归模型可以表示为\(y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\epsilon\)。同样使用最小二乘法来估计参数\(\beta_0,\beta_1,\cdots,\beta_p\)，目标是最小化误差平方和\(S(\beta_0,\beta_1,\cdots,\beta_p)=\sum_{i = 1}^{n}(y_i - (\beta_0+\sum_{j = 1}^{p}\beta_jx_{ij}))^2\)，其中\(x_{ij}\)表示第\(i\)个样本的第\(j\)个自变量的值。在实际计算中，通常会使用矩阵运算来简化求解过程，将模型表示为矩阵形式\(Y = X\beta+\epsilon\)，其中\(Y\)是因变量的向量，\(X\)是包含自变量的设计矩阵，\(\beta\)是参数向量。通过矩阵运算可以得到参数\(\beta\)的最小二乘估计\(\hat{\beta}=(X^TX)^{-1}X^TY\)。

#### 三、模型训练

1. **数据准备**：收集和整理包含自变量和因变量的训练数据，确保数据的准确性和完整性。通常需要对数据进行预处理，如数据清洗（去除异常值、处理缺失值）、标准化（将数据转换为均值为\(0\)，标准差为\(1\)的形式）等，以提高模型的训练效果和稳定性。

1. **选择优化算法**：除了最小二乘法，还可以使用梯度下降法等优化算法来求解线性回归模型的参数。梯度下降法通过不断迭代更新参数，沿着损失函数（如误差平方和）的梯度反方向移动，逐步减小损失函数的值，直到达到收敛条件。随机梯度下降法（SGD）是梯度下降法的一种变体，它每次只使用一个样本（或一小批样本）来计算梯度并更新参数，计算效率更高，适用于大规模数据集。

1. **训练模型**：根据选择的优化算法，在训练数据上进行模型训练，不断调整参数值，使模型能够更好地拟合训练数据，即最小化损失函数。在训练过程中，可以通过监控损失函数的值、观察模型在验证集上的性能等方式来判断模型是否收敛以及是否出现过拟合或欠拟合现象。

#### 四、模型评估

1. **均方误差（MSE）**：计算预测值与真实值之间误差的平方的平均值，即\(MSE=\frac{1}{n}\sum_{i = 1}^{n}(y_i - \hat{y}_i)^2\)，MSE 值越小，说明模型的预测效果越好。

1. **均方根误差（RMSE）**：是 MSE 的平方根，\(RMSE=\sqrt{MSE}\)，它与因变量的单位相同，更直观地反映了预测值与真实值之间的平均误差大小。

1. **平均绝对误差（MAE）**：计算预测值与真实值之间误差的绝对值的平均值，即\(MAE=\frac{1}{n}\sum_{i = 1}^{n}|y_i - \hat{y}_i|\)，MAE 对异常值的敏感度相对较低。

1. **决定系数（**\(R^2\)**）**：衡量模型对数据的拟合优度，\(R^2 = 1-\frac{\sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i = 1}^{n}(y_i-\bar{y})^2}\)，其中\(\bar{y}\)是因变量\(y\)的均值。\(R^2\)的值越接近\(1\)，说明模型对数据的拟合效果越好，模型能够解释因变量的大部分变异。

#### 五、线性回归算法的优缺点

1. **优点**

- **模型简单易懂**：线性回归模型的形式简单直观，参数具有明确的物理意义，容易理解和解释，无论是专业的数据分析人员还是非专业人士都能较好地掌握。

- **计算效率高**：对于大规模数据集，使用最小二乘法或梯度下降法等优化算法求解模型参数的计算速度较快，尤其是在数据特征维度不高的情况下。

- **可扩展性强**：可以很容易地扩展到多元线性回归，处理多个自变量的情况，并且可以通过添加多项式特征等方式来处理非线性关系（此时称为多项式回归，本质上仍然可以转化为线性回归问题）。

1. **缺点**

- **对数据分布有假设**：假设误差项服从正态分布，并且自变量和因变量之间存在线性关系。如果数据不满足这些假设，模型的性能可能会受到较大影响，例如当数据存在非线性关系时，线性回归模型可能无法准确拟合数据。

- **容易过拟合**：在多元线性回归中，如果自变量数量过多，且存在一些与因变量相关性不强的自变量，可能会导致模型过拟合，即模型在训练数据上表现良好，但在新的数据上泛化能力较差。可以通过正则化（如岭回归、Lasso 回归等）来缓解过拟合问题。

- **对异常值敏感**：由于最小二乘法是基于误差平方和最小化，异常值会对误差平方和产生较大影响，从而影响参数估计的准确性，导致模型的稳定性下降。

#### 六、应用场景

1. **经济学领域**：用于预测经济指标，如国内生产总值（GDP）、通货膨胀率、股票价格等。例如，通过分析历史数据中利率、汇率、消费者信心指数等自变量与 GDP 之间的关系，建立线性回归模型，对未来的 GDP 进行预测，为政府制定经济政策、企业进行投资决策提供参考。

1. **物理学研究**：许多物理现象可以用线性关系来描述，线性回归可用于分析实验数据，确定物理量之间的定量关系。比如在牛顿第二定律实验中，通过测量不同外力作用下物体的加速度，利用线性回归确定力与加速度之间的比例关系（即质量）。

1. **销售预测**：企业根据历史销售数据以及相关的市场因素（如广告投入、季节因素、竞争对手情况等），建立线性回归模型来预测未来的产品销售量，以便合理安排生产计划、制定营销策略和进行库存管理。

1. **医疗领域**：例如通过分析患者的年龄、性别、血压、血糖等生理指标（自变量）与某种疾病的发病风险（因变量）之间的关系，建立线性回归模型，用于疾病的风险评估和预测，辅助医生进行疾病诊断和预防。

### 8-降维算法详解

![3e265f06-c404-45de-98c8-422e6d1d6e9c_1741920386643832848_origin~tplv-a9rns2rl98-image-qvalue](image\3e265f06-c404-45de-98c8-422e6d1d6e9c_1741920386643832848_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

在数据分析与机器学习领域，数据维度的不断增加既带来了丰富的信息，也引发了诸多挑战，如维度灾难问题，致使计算复杂度飙升、模型过拟合风险加剧。降维算法应运而生，其核心目标是在尽可能留存关键信息的前提下，削减数据的维度，为后续分析与建模 “减负”。

#### 一、主成分分析（PCA）

1. **原理**：PCA 基于数据的协方差矩阵，旨在找出一组相互正交的主成分。这些主成分是原始特征的线性组合，且按照方差大小排序。方差越大，表明该主成分携带的数据信息越丰富。通过保留前 k 个方差最大的主成分，就能以较低维度近似表示原始数据。以二维数据为例，若数据点大致呈线性分布，PCA 会找到这条直线的方向作为第一主成分，垂直于该直线的方向为第二主成分，且第一主成分的方差大于第二主成分。在实际计算中，需先对数据进行标准化处理，计算协方差矩阵，接着求其特征值与特征向量，依据特征值大小排序，选取前 k 个特征向量构建变换矩阵，实现数据降维。

1. **特点**：优点在于能够无监督地对数据进行线性变换，有效去除数据中的冗余信息，且对数据的可解释性强，主成分的系数反映了原始特征与主成分之间的关系。然而，PCA 是一种全局线性降维方法，对非线性数据的降维效果欠佳，并且对数据中的噪声较为敏感，噪声可能会影响协方差矩阵的计算，进而干扰主成分的提取。

1. **应用场景**：在图像压缩领域，可将高维图像数据通过 PCA 降维，减少存储空间，同时尽可能保留图像的关键特征，在解压时仍能保证一定的图像质量。在化学分析中，面对众多化学物质的成分数据，PCA 可提取主要成分，简化数据，辅助分析物质的特性与反应规律。

#### 二、线性判别分析（LDA）

1. **原理**：LDA 属于有监督的降维算法，其核心思想是最大化类间距离与最小化类内距离。它依据数据的类别标签，计算类内散度矩阵与类间散度矩阵，通过求解广义特征值问题，获取使类间散度与类内散度比值最大的投影方向，这些方向即为降维后的特征。例如在二分类问题中，LDA 试图找到一个投影方向，使得两类数据在该方向上的投影尽可能分开，同时同一类数据的投影尽可能聚集。

1. **特点**：由于利用了类别信息，LDA 在分类任务中降维效果显著，能够有效提升分类模型的性能。它对数据的线性可分性假设较强，当数据线性可分性较差时，降维效果会大打折扣。而且 LDA 的降维结果维度上限为类别数减 1，在处理类别数较少的数据时，降维空间有限。

1. **应用场景**：在人脸识别系统中，LDA 可对高维的人脸图像特征进行降维，提取出最具区分性的特征，用于识别不同人的身份，提高识别准确率与速度。在生物医学领域，对疾病相关的基因表达数据进行分析时，LDA 能够筛选出与疾病类别密切相关的基因特征，辅助疾病的诊断与分类研究。

#### 三、局部线性嵌入（LLE）

1. **原理**：LLE 是一种非线性降维算法，它假设数据在局部区域内具有线性结构。对于每个数据点，LLE 通过寻找其在局部邻域内的近邻点，计算该点与近邻点之间的线性重构系数，使得该点能够通过近邻点的线性组合尽可能精确地重构。在降维过程中，保持这些线性重构系数不变，将数据点映射到低维空间，同时保留数据的局部几何结构。例如在一个三维流形数据上，LLE 能捕捉到数据在局部的弯曲特性，并将其映射到二维空间中，保留这种局部结构。

1. **特点**：LLE 擅长处理非线性数据，能够有效揭示数据的内在流形结构，在降维后的数据中保留更多的局部细节信息。它的计算过程依赖于邻域点的选择，邻域参数的设置对结果影响较大，若设置不当，可能导致降维效果不佳。此外，LLE 对离群点较为敏感，离群点可能破坏局部线性结构的计算。

1. **应用场景**：在蛋白质结构分析中，蛋白质的三维结构数据复杂且呈现非线性特征，LLE 可将高维的蛋白质结构数据降维，帮助研究人员理解蛋白质结构的内在规律与相似性，为蛋白质功能预测提供支持。在地理信息系统中，对地形数据等非线性空间数据进行降维，LLE 能保留地形的局部起伏等特征，便于数据存储与可视化展示。

#### 四、t - 分布随机邻域嵌入（t - SNE）

1. **原理**：t - SNE 主要用于将高维数据映射到低维空间，以便进行可视化分析。它基于数据点之间的相似性，通过构建高维空间和低维空间的概率分布来实现降维。在高维空间中，用高斯分布衡量数据点之间的相似度；在低维空间中，采用 t 分布来近似高维空间的分布，通过最小化两个分布之间的 KL 散度，将高维数据点映射到低维空间，使得在高维空间中相似的数据点在低维空间中也尽量靠近。

1. **特点**：t - SNE 在数据可视化方面表现卓越，能够将复杂的高维数据以直观的方式展示在二维或三维空间中，清晰呈现数据的聚类结构和分布模式。但它的计算复杂度较高，运行时间较长，且结果对参数设置（如困惑度等）较为敏感，不同的参数可能导致差异较大的降维结果。

1. **应用场景**：在基因表达数据分析中，研究人员可利用 t - SNE 将大量基因的表达数据降维至二维或三维空间，直观地观察不同基因表达模式的聚类情况，发现潜在的基因功能关联。在客户行为分析中，将客户的多维度消费行为数据通过 t - SNE 降维可视化，有助于企业洞察客户群体的细分情况，制定针对性营销策略。

### 9-LSTM 算法详解

LSTM 即长短期记忆网络（Long Short-Term Memory），它是一种特殊的循环神经网络（RNN）架构，专门为解决传统 RNN 在处理长序列数据时面临的梯度消失和梯度爆炸问题而设计，在序列数据处理领域应用广泛。我将从其原理、结构、优势、应用场景等方面展开介绍

![ad1ebca0-44d5-46ef-b91e-97967cd6d44b_1741920900577632776_origin~tplv-a9rns2rl98-image-qvalue](image\ad1ebca0-44d5-46ef-b91e-97967cd6d44b_1741920900577632776_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

在机器学习和深度学习领域，处理序列数据一直是一项具有挑战性的任务。传统的神经网络在处理短序列时表现尚可，但面对长序列数据，如时间序列数据、自然语言文本等，往往难以捕捉到数据中的长期依赖关系。长短期记忆网络（LSTM）应运而生，它通过独特的结构设计，有效解决了传统循环神经网络（RNN）在处理长序列时出现的梯度消失和梯度爆炸问题，从而能够更好地学习和利用长距离的依赖信息。

#### 一、LSTM 的原理

1. **循环神经网络基础**：循环神经网络是一类能够处理序列数据的神经网络，其隐藏层不仅接收当前输入，还接收上一时刻隐藏层的输出，这种结构使得 RNN 具有对序列历史信息的记忆能力。然而，随着序列长度的增加，RNN 在反向传播过程中，梯度会出现指数级的衰减或增长，导致模型难以学习到长距离的依赖关系，这就是梯度消失和梯度爆炸问题。

1. **LSTM 单元结构**：LSTM 通过引入门控机制来改进 RNN 的隐藏层结构。每个 LSTM 单元包含三个门：输入门、遗忘门和输出门，以及一个记忆细胞。

- **遗忘门**：决定从记忆细胞中保留或丢弃哪些信息。它接收上一时刻隐藏层输出\(h_{t-1}\)和当前输入\(x_t\)，通过一个 Sigmoid 函数输出一个在\(0\)到\(1\)之间的值\(f_t\)，其中\(0\)表示完全丢弃，\(1\)表示完全保留。公式为\(f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\)，这里\(W_f\)是权重矩阵，\(b_f\)是偏置项，\(\sigma\)是 Sigmoid 函数，\([h_{t-1}, x_t]\)表示将上一时刻隐藏层输出和当前输入拼接在一起。

- **输入门**：控制当前输入信息进入记忆细胞。它由两部分组成，一部分通过 Sigmoid 函数决定哪些新信息要被添加到记忆细胞中，输出为\(i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\)；另一部分通过一个 tanh 函数生成候选值\(\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)\)。最终，新的记忆细胞信息\(C_t\)通过遗忘门和输入门的输出进行更新，即\(C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t\)。

- **输出门**：决定记忆细胞中哪些信息将被输出到当前隐藏层。它接收上一时刻隐藏层输出\(h_{t-1}\)、当前输入\(x_t\)以及更新后的记忆细胞\(C_t\)，首先通过 Sigmoid 函数得到输出门的值\(o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\)，然后通过 tanh 函数对记忆细胞进行变换\(\tanh(C_t)\)，最后当前隐藏层输出\(h_t = o_t \cdot \tanh(C_t)\)。

#### 二、LSTM 的优势

1. **长距离依赖学习能力**：通过门控机制，LSTM 能够有选择地保留或遗忘记忆细胞中的信息，使得模型可以学习和利用长距离的依赖关系，有效克服了传统 RNN 的梯度消失和梯度爆炸问题。例如在自然语言处理中，对于一个长句子，LSTM 可以记住句子开头的关键信息，并在处理句子结尾时利用这些信息进行准确的语义理解。

1. **对噪声和异常值的鲁棒性**：由于 LSTM 在更新记忆细胞时是基于门控机制的加权求和，不是简单地累加，所以对于数据中的噪声和异常值有一定的抵抗能力。即使在输入序列中存在噪声或异常值，LSTM 也能通过门控机制调整对这些信息的处理，保证模型的稳定性。

1. **灵活性**：LSTM 可以根据不同的任务和数据特点进行灵活调整。例如，可以通过堆叠多个 LSTM 单元来构建更深层次的模型，以学习更复杂的模式；也可以与其他神经网络结构（如卷积神经网络）结合使用，进一步提升模型性能。

#### 三、LSTM 的应用场景

1. **自然语言处理**

- **文本分类**：将文本按其主题、情感等进行分类。例如，对新闻文章进行分类，判断是政治、经济、文化等哪种类型；对用户评论进行情感分析，判断是正面、负面还是中性情感。LSTM 能够捕捉文本中的语义信息和上下文关系，从而准确地进行分类。

- **机器翻译**：将一种语言翻译成另一种语言。LSTM 可以理解源语言句子的结构和语义，并生成目标语言的翻译。在翻译过程中，它能够处理句子中的长距离依赖关系，如语法结构和语义关联，提高翻译的准确性。

- **语音识别**：将语音信号转换为文本。LSTM 可以处理语音信号中的时间序列信息，识别出语音中的单词和句子。通过学习大量的语音数据，LSTM 能够适应不同人的语音特点和语速变化，提高语音识别的准确率。

1. **时间序列预测**

- **股票价格预测**：根据历史股票价格数据预测未来的股票价格走势。股票价格受多种因素影响，且具有较强的时间序列特征，LSTM 能够学习到这些因素之间的复杂关系以及价格变化的趋势，从而进行有效的预测。

- **天气预报**：利用历史气象数据（如温度、湿度、气压等）预测未来的天气情况。气象数据是典型的时间序列数据，LSTM 可以捕捉到气象要素在时间上的变化规律和相互关系，为天气预报提供更准确的模型支持。

1. **生物信息学**

- **基因序列分析**：分析 DNA 或 RNA 序列，预测基因的功能、结构以及与疾病的关系。基因序列是一种序列数据，LSTM 可以学习到序列中的模式和特征，帮助研究人员理解基因的生物学意义。

- **蛋白质结构预测**：根据蛋白质的氨基酸序列预测其三维结构。蛋白质的结构与其功能密切相关，LSTM 可以通过学习氨基酸序列中的信息，预测蛋白质的折叠方式和空间结构，为药物研发和疾病治疗提供重要信息。

### 10-XGBoost 算法详解

XGBoost 算法作为一种高效的梯度提升框架，在数据挖掘和机器学习领域应用广泛。我将从算法原理、核心组件、优势、应用场景等方面为你详细介绍。

在数据挖掘与机器学习的算法版图中，XGBoost（eXtreme Gradient Boosting）占据着重要地位。它是一种基于梯度提升决策树（GBDT）的高效实现，通过迭代训练一系列弱分类器（通常是决策树），并将它们的结果进行加权累加，从而构建出一个强大的预测模型，能够有效处理分类、回归等多种问题。

#### 一、XGBoost 的原理

1. **梯度提升框架基础**：梯度提升框架的核心思想是通过不断拟合前一轮模型的残差来构建新的弱分类器，逐步提升模型的预测能力。具体而言，对于一个回归问题，初始时模型预测值为所有样本标签的均值。在第 t 轮训练中，计算当前模型预测值与真实值之间的残差，将残差作为新的目标值，训练一个新的弱分类器（如决策树）。然后，将这个新的弱分类器的预测结果乘以一个学习率（通常是一个较小的值，如 0.1），并加到之前的模型预测值上，得到新的模型预测值。通过多次迭代，不断减小预测值与真实值之间的误差。

1. **XGBoost 的目标函数**：XGBoost 在梯度提升框架的基础上，对目标函数进行了精心设计。其目标函数由两部分组成，即损失函数和正则化项。损失函数衡量模型预测值与真实值之间的差异，常见的损失函数有均方误差（用于回归问题）、对数损失函数（用于分类问题）等。正则化项用于防止模型过拟合，它对模型的复杂度进行惩罚，通常包含决策树的叶子节点数量、叶子节点输出值的 L2 范数等。通过最小化目标函数，XGBoost 在提升模型拟合能力的同时，有效控制了模型的复杂度，提高了模型的泛化性能。

1. **二阶导数信息利用**：与传统的梯度提升算法不同，XGBoost 在计算梯度时利用了二阶导数信息。在求目标函数的最小值时，传统方法通常只考虑一阶导数（梯度），而 XGBoost 通过二阶泰勒展开式来近似目标函数，不仅利用了一阶导数，还考虑了二阶导数。二阶导数信息能够更准确地估计函数的曲率，使得 XGBoost 在寻找最优解时能够更快地收敛，并且在某些情况下能够得到更精确的模型参数。

#### 二、XGBoost 的核心组件

1. **决策树构建**：XGBoost 中的弱分类器通常是决策树，它采用了一种基于贪心策略的算法来构建决策树。在构建过程中，XGBoost 会遍历每个特征的每个可能的分割点，计算在该点进行分割后目标函数的增益（即分割前后目标函数值的变化）。选择增益最大的分割点作为当前节点的分割点，递归地构建决策树的子节点，直到满足一定的停止条件（如叶子节点的样本数量小于某个阈值、树的深度达到上限等）。

1. **并行计算**：XGBoost 支持并行计算，这是其高效性的重要体现。在构建决策树时，XGBoost 可以并行地计算不同特征和分割点的增益，大大缩短了模型训练时间。具体实现方式是，在训练前对数据进行排序并存储在块结构中，每个块对应一个特征的所有样本数据。在计算增益时，不同的块可以在不同的线程或处理器上并行处理，从而提高计算效率。

1. **缺失值处理**：XGBoost 能够自动处理数据中的缺失值。在构建决策树时，对于存在缺失值的特征，XGBoost 会尝试将缺失值样本分别划分到左子树和右子树，计算两种情况下的增益，选择增益较大的方向作为缺失值样本的划分方向。同时，XGBoost 还会学习一个默认的划分方向，当遇到新的缺失值样本时，可以按照默认方向进行划分，无需额外的预处理步骤。

#### 三、XGBoost 的优势

1. **高效性**：通过并行计算和对目标函数的优化，XGBoost 在大规模数据集上能够快速训练模型，相比传统的梯度提升算法，训练速度有显著提升。在处理包含数百万条记录和数百个特征的数据集时，XGBoost 能够在较短时间内完成模型训练，满足实际应用中的时间要求。

1. **准确性高**：XGBoost 利用二阶导数信息，能够更精确地拟合数据，并且通过正则化项有效防止过拟合，使得模型在训练集和测试集上都具有较高的预测准确性。在许多数据挖掘竞赛和实际项目中，XGBoost 常常能够取得优于其他算法的成绩。

1. **可扩展性**：XGBoost 支持多种类型的数据（如数值型、类别型），并且能够处理大规模数据集和高维数据。它还提供了丰富的参数设置，用户可以根据不同的任务和数据特点进行灵活调整，以优化模型性能。无论是在单机环境还是分布式环境下，XGBoost 都能很好地运行，具有很强的可扩展性。

1. **鲁棒性**：对数据中的噪声和异常值具有一定的鲁棒性。由于 XGBoost 是通过迭代多个弱分类器来构建最终模型，个别噪声或异常值对整体模型的影响相对较小。同时，正则化项也有助于提高模型的稳定性，减少噪声和异常值对模型的干扰。

#### 四、XGBoost 的应用场景

1. **金融风险评估**：在金融领域，XGBoost 可用于评估客户的信用风险、预测贷款违约概率等。通过分析客户的年龄、收入、信用记录、负债情况等多个特征，XGBoost 能够构建准确的风险评估模型，帮助金融机构做出合理的贷款决策，降低风险损失。

1. **电商销售预测**：电商企业可以利用 XGBoost 根据历史销售数据、用户行为数据、市场趋势等信息，预测未来的产品销售量。这有助于企业合理安排库存、制定营销策略、优化供应链管理，提高企业的运营效率和盈利能力。

1. **医疗诊断辅助**：在医疗领域，XGBoost 可辅助医生进行疾病诊断。例如，通过分析患者的症状、检查结果、病史等多方面数据，XGBoost 能够预测患者患某种疾病的概率，为医生提供诊断参考，提高诊断的准确性和效率。

1. **广告点击率预测**：互联网广告平台利用 XGBoost 根据用户的特征（如年龄、性别、兴趣爱好）、广告内容、投放环境等信息，预测广告的点击率。这有助于广告平台优化广告投放策略，提高广告投放效果，为广告主带来更好的投资回报率。

### 11-K-means 算法详解

![c46c7f60-6db0-4848-96d9-90c7d45c3738_1741923469915170957_origin~tplv-a9rns2rl98-image-qvalue](image\c46c7f60-6db0-4848-96d9-90c7d45c3738_1741923469915170957_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

K-means 算法作为一种经典的聚类算法，旨在将给定的数据点集合划分成 K 个簇，使同一簇内的数据点具有较高的相似度，而不同簇之间的数据点相似度较低。它在数据挖掘、机器学习、图像处理等众多领域都有着广泛应用，是探索数据内在结构的重要工具。

#### 一、算法原理

K-means 算法基于距离度量来衡量数据点之间的相似性，通常使用欧氏距离。其核心思想是通过迭代的方式，不断更新簇的中心位置，使得每个数据点都被分配到距离其最近的簇中心所在的簇中。具体而言，算法预先设定簇的数量 K，随机选择 K 个数据点作为初始的簇中心。然后，计算每个数据点到各个簇中心的距离，将数据点划分到距离最近的簇中。划分完成后，重新计算每个簇内数据点的均值，将其作为新的簇中心。不断重复上述过程，直到簇中心不再发生变化或者达到预设的迭代次数，此时认为算法收敛，聚类结果稳定。

#### 二、执行步骤

1. **初始化簇中心**：从数据集中随机选择 K 个数据点作为初始的簇中心。例如，在一个包含 100 个数据点的二维数据集里，若设定 K=3，就从这 100 个点中随机挑选 3 个点作为初始的 3 个簇的中心。

1. **分配数据点到簇**：对于数据集中的每个数据点，计算它与各个簇中心的距离，将其分配到距离最近的簇中。以欧氏距离为例，若有一个数据点 A，计算它到簇中心 B、C、D 的欧氏距离，若到 B 的距离最短，则将 A 分配到以 B 为中心的簇中。

1. **更新簇中心**：在完成所有数据点的分配后，对于每个簇，计算簇内所有数据点的均值，将这个均值作为新的簇中心。假设某个簇中有 5 个数据点，它们在二维空间中的坐标分别为 (x1,y1)、(x2,y2)、(x3,y3)、(x4,y4)、(x5,y5)，则新的簇中心坐标为 ((x1+x2+x3+x4+x5)/5, (y1+y2+y3+y4+y5)/5)。

1. **迭代与收敛**：重复步骤 2 和步骤 3，不断调整数据点的分配和簇中心的位置。当簇中心不再发生变化，即前后两次迭代中簇中心的位置完全相同，或者达到预设的迭代次数（如 100 次）时，算法停止迭代，此时得到的聚类结果即为最终结果。

#### 三、优缺点

1. **优点**

- **算法简单直观**：K-means 算法的原理和执行步骤都相对简单，易于理解和实现，对于初学者来说容易上手。在实际应用中，能够快速搭建模型进行数据聚类分析。

- **计算效率较高**：在数据量不是特别巨大的情况下，算法的计算速度较快。因为每次迭代主要进行距离计算和均值计算，计算复杂度相对较低。例如，在处理包含几千个数据点的数据集时，能够在较短时间内完成聚类。

- **对大规模数据有一定适用性**：通过合理的优化（如使用并行计算），K-means 算法可以处理大规模数据集，在工业界和学术界都有广泛应用。

1. **缺点**

- **对 K 值敏感**：K 值的选择对聚类结果影响很大，但在实际应用中，很难预先确定一个合适的 K 值。不同的 K 值可能导致截然不同的聚类结果，且缺乏通用的理论方法来确定最优的 K 值，通常需要通过多次实验和评估来选择。

- **对初始簇中心敏感**：由于初始簇中心是随机选择的，不同的初始选择可能会导致最终聚类结果的差异较大。如果初始簇中心选择不当，可能会使算法收敛到局部最优解，而不是全局最优解。

- **对异常值敏感**：因为簇中心是通过均值计算得到的，异常值会对均值产生较大影响，从而干扰聚类结果。例如，在一个簇中若存在一个离群较远的异常值，可能会使该簇的中心位置偏离正常范围，导致聚类不准确。

#### 四、应用场景

1. **客户细分**：企业可根据客户的消费行为、购买偏好、人口统计学特征等多维度数据，使用 K-means 算法将客户细分为不同的群体。例如，将客户聚类为高消费群体、低消费群体、频繁购买群体、偶尔购买群体等，针对不同群体制定个性化的营销策略，提高客户满意度和企业收益。

1. **图像分割**：在图像处理中，K-means 算法可将图像中的像素点根据颜色、亮度等特征进行聚类，实现图像分割。比如，将一幅自然风景图像分割为天空、草地、树木、河流等不同的区域，有助于图像识别、目标检测等后续处理。

1. **文档分类**：对于大量的文本数据（如新闻文章、学术论文、邮件等），将文本转化为向量形式后，利用 K-means 算法根据文本向量的相似性将文档聚类为不同的主题类别。例如，将新闻文章聚类为政治、经济、文化、体育等类别，方便信息检索和管理。

1. **生物信息学**：在基因表达数据分析中，K-means 算法可将具有相似基因表达模式的基因聚类在一起，有助于研究基因的功能和生物过程。通过聚类分析，可以发现与特定疾病相关的基因簇，为疾病诊断和治疗提供线索。

### 12-CNN 算法详解

卷积神经网络（Convolutional Neural Network，CNN）是一种专门为处理具有网格结构数据（如图像、音频）而设计的深度学习算法，在计算机视觉、语音识别等众多领域取得了卓越成就，极大推动了人工智能的发展。

#### 一、算法原理

1. **卷积运算**：CNN 的核心操作是卷积运算。它通过卷积核（也叫滤波器）在输入数据上滑动，对局部区域进行加权求和，生成特征映射。例如在图像领域，卷积核是一个小的矩阵，假设我们有一个 3x3 的卷积核，在一幅图像上逐像素滑动，将卷积核覆盖区域内的像素值与卷积核对应位置的权重相乘后累加，得到特征映射上对应位置的值。这种局部连接的方式，使得 CNN 能够自动学习到数据中的局部特征，并且大大减少了参数数量。

1. **特征提取**：随着卷积层的堆叠，CNN 能够从低级特征逐步提取到高级、抽象的特征。浅层卷积层学习到的可能是边缘、角点等简单特征，而深层卷积层则能够识别更复杂的物体部件、形状等特征。例如在人脸识别中，浅层网络可能检测到眼睛、鼻子的边缘，而深层网络则能综合这些特征判断出是否为某个人的脸。

#### 二、网络结构

1. **卷积层**：是 CNN 的基础层，通过卷积核进行卷积运算，提取输入数据的特征。卷积层中可以设置多个不同的卷积核，每个卷积核学习到不同的特征模式。例如在图像分类任务中，一个卷积层可能包含 32 个或 64 个卷积核，每个卷积核负责提取特定类型的特征。

1. **池化层**：通常紧跟在卷积层之后，常见的池化方式有最大池化和平均池化。最大池化是在一个局部区域内取最大值，平均池化则是取平均值。以 2x2 的最大池化为例，将输入数据划分为多个 2x2 的小块，每个小块中取最大值作为输出，这样可以降低数据的维度，减少计算量，同时保留主要特征，增强模型对平移、缩放等变换的鲁棒性。

1. **全连接层**：在经过多个卷积层和池化层后，将提取到的特征进行整合，通过全连接的方式将所有特征连接到输出层。全连接层的神经元与上一层的所有神经元都有连接，用于最终的分类或回归任务。例如在一个图像分类网络中，全连接层根据前面卷积和池化层提取的特征，输出属于各个类别的概率。

#### 三、训练过程

1. **数据准备**：收集和整理大量的训练数据，对于图像任务，需要准备包含不同类别图像的数据集，并进行标注。同时，对数据进行预处理，如归一化、裁剪、增强等操作，提高数据的质量和多样性，增强模型的泛化能力。例如对图像数据进行随机旋转、缩放、裁剪等操作，生成更多不同视角的图像数据。

1. **参数初始化**：初始化卷积核权重、全连接层权重等参数，通常采用随机初始化的方式。合理的初始化有助于模型更快收敛和更好地学习。

1. **前向传播**：将训练数据输入到 CNN 网络中，数据依次经过卷积层、池化层和全连接层，每一层根据其定义的操作对数据进行变换，最终得到预测结果。例如在图像分类中，预测结果是属于各个类别的概率分布。

1. **反向传播**：根据预测结果与真实标签之间的差异（如交叉熵损失函数），通过反向传播算法计算梯度，调整网络中的权重参数，使得损失函数值逐渐减小。反向传播利用链式法则，从输出层开始，将误差反向传播到每一层，计算每一层参数的梯度，以更新参数。

1. **迭代训练**：不断重复前向传播和反向传播过程，通过多轮训练，逐步优化模型的参数，提高模型在训练数据上的准确性，同时在验证集上监控模型的性能，防止过拟合。当模型在验证集上的性能不再提升时，通常认为模型达到了较好的训练效果。

#### 四、优势

1. **强大的特征提取能力**：能够自动从大量数据中学习到丰富、有效的特征，无需手动设计特征工程，在图像和音频处理等领域表现出色。例如在图像识别任务中，CNN 能够学习到物体的形状、纹理、颜色等复杂特征，准确识别不同类别的物体。

1. **参数共享与局部连接**：卷积核在数据上滑动进行卷积运算，共享相同的权重参数，大大减少了参数数量，降低了计算复杂度，同时也提高了模型的泛化能力。局部连接使得网络专注于学习局部特征，更符合数据的局部相关性特点。

1. **对平移、缩放等变换的鲁棒性**：池化层的存在使得 CNN 对输入数据的平移、缩放、旋转等变换具有一定的不变性。例如在图像中，即使物体发生了一定的平移或缩放，CNN 仍能准确识别出物体类别。

#### 五、应用场景

1. **图像分类**：广泛应用于识别不同类别的图像，如在安防监控中识别入侵人员、车辆；在医疗影像分析中识别病变组织；在自动驾驶中识别交通标志、行人等。例如，在一个安防监控系统中，CNN 可以实时对监控视频中的图像进行分类，判断是否有异常情况发生。

1. **目标检测**：不仅能够识别图像中物体的类别，还能确定物体在图像中的位置。在智能交通中，用于检测道路上的车辆、行人、交通标志等；在工业生产中，检测产品的缺陷、零部件的位置等。例如在工业质检中，CNN 可以快速检测出产品表面是否存在划痕、裂纹等缺陷，并定位缺陷位置。

1. **图像生成**：通过生成对抗网络（GAN）等与 CNN 结合的方式，生成逼真的图像。如生成虚拟的人脸、风景图像等，在艺术创作、虚拟现实、游戏开发等领域有广泛应用。例如在游戏开发中，利用 CNN 生成虚拟场景中的建筑、自然景观等元素，提高游戏画面的丰富度和逼真度。

1. **语音识别**：对语音信号进行处理和识别，将语音转换为文本。在智能语音助手、语音输入系统、电话客服自动应答等场景中发挥重要作用。例如智能语音助手通过 CNN 对用户的语音指令进行识别和理解，执行相应的操作。

### 13-RNN 算法详解

![50be726e-153d-4c6d-8558-34f55aaba197_1741928906791461703_origin~tplv-a9rns2rl98-image-qvalue](image\50be726e-153d-4c6d-8558-34f55aaba197_1741928906791461703_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

循环神经网络（Recurrent Neural Network，RNN）是一种专门为处理序列数据而设计的神经网络架构，在自然语言处理、时间序列分析等领域发挥着关键作用，能够有效捕捉数据中的时间依赖关系。

#### 一、算法原理

1. **序列数据处理机制**：与传统神经网络不同，RNN 具有循环连接的隐藏层。在处理序列数据（如文本中的单词序列、时间序列数据）时，RNN 的隐藏层不仅接收当前输入数据，还接收上一时刻隐藏层的输出。这种结构使得 RNN 能够保留序列中的历史信息，从而对当前输入进行更全面的理解。例如，在处理一个句子时，RNN 可以记住前面已经出现的单词，以此来理解当前单词在句子中的含义和作用。

1. **循环计算过程**：假设我们有一个输入序列\(x_1,x_2,\cdots,x_T\)，在时刻\(t\)，RNN 的隐藏层状态\(h_t\)通过当前输入\(x_t\)和上一时刻隐藏层状态\(h_{t - 1}\)共同计算得出。其计算公式为\(h_t=\sigma(W_{hh}h_{t - 1}+W_{xh}x_t + b_h)\)，其中\(\sigma\)是激活函数（如 tanh 或 ReLU），\(W_{hh}\)是隐藏层到隐藏层的权重矩阵，\(W_{xh}\)是输入层到隐藏层的权重矩阵，\(b_h\)是偏置项。这种循环计算的方式使得 RNN 能够逐步处理序列中的每个元素，并利用之前的信息来更新当前状态。

#### 二、网络结构

1. **基本 RNN 单元**：RNN 的基本单元由输入层、隐藏层和输出层组成。输入层接收当前时刻的输入数据，隐藏层通过循环连接保留历史信息，输出层根据当前隐藏层状态输出预测结果。在文本分类任务中，输入层可能接收一个单词的向量表示，隐藏层通过循环计算整合之前单词的信息，输出层输出该文本属于不同类别的概率。

1. **展开的时间序列结构**：当将 RNN 在时间维度上展开时，可以清晰地看到其对序列数据的处理过程。从初始时刻开始，输入数据依次进入 RNN，隐藏层状态不断更新，每个时刻的输出都依赖于当前隐藏层状态以及之前的所有输入信息。例如在股票价格预测中，以每天的股票价格作为输入序列，RNN 通过展开的结构，能够根据过去多天的价格信息来预测未来的价格走势。

#### 三、训练过程

1. **数据准备**：收集和整理序列数据，并将其转化为适合 RNN 输入的格式。在自然语言处理中，需要将文本数据进行分词、编码等预处理操作，将单词转换为向量表示。同时，划分训练集、验证集和测试集，用于模型的训练、评估和测试。

1. **参数初始化**：初始化 RNN 中的权重矩阵（如\(W_{hh}\)、\(W_{xh}\)）和偏置项（\(b_h\)），通常采用随机初始化的方法，但需要注意初始化值的范围，以确保模型能够有效收敛。

1. **前向传播**：将训练数据输入到 RNN 中，按照时间顺序依次处理序列中的每个元素。在每个时刻，根据当前输入和上一时刻隐藏层状态计算当前隐藏层状态，并输出预测结果。例如在语音识别中，前向传播过程就是将语音信号的时间序列数据依次输入 RNN，逐步生成对每个时间点语音内容的预测。

1. **反向传播**：在 RNN 中，反向传播采用随时间反向传播（Backpropagation Through Time，BPTT）算法。该算法通过将误差从输出层反向传播到每个时间步的隐藏层，计算每个权重参数的梯度。由于 RNN 存在循环连接，BPTT 算法需要在时间维度上展开计算梯度，以更新权重参数，使得损失函数值逐渐减小。

1. **迭代训练**：不断重复前向传播和反向传播过程，通过多轮训练，逐步调整权重参数，提高模型在训练数据上的预测准确性。在训练过程中，需要监控模型在验证集上的性能，防止过拟合。当模型在验证集上的性能不再提升时，停止训练，选择性能最佳的模型进行测试。

#### 四、优势

1. **处理序列数据能力强**：RNN 能够有效处理具有时间顺序或序列结构的数据，捕捉数据中的长期依赖关系。在自然语言处理中，能够理解句子中单词之间的顺序和语义关联；在时间序列分析中，能够根据历史数据预测未来趋势。

1. **模型灵活性高**：可以根据不同的任务和数据特点进行灵活调整。例如，可以通过堆叠多个 RNN 层来构建更深层次的模型，以学习更复杂的序列模式；也可以与其他神经网络结构（如卷积神经网络）结合使用，进一步提升模型性能。

1. **对数据的适应性好**：不需要对数据进行复杂的预处理或特征工程，能够自动学习数据中的特征和模式。对于不同长度的序列数据，RNN 可以通过循环结构自适应地处理，而不需要对数据进行固定长度的截断或填充。

#### 五、应用场景

1. **自然语言处理**

- **文本分类**：根据文本的内容将其分类到不同的类别中，如新闻分类、邮件分类、情感分析等。RNN 能够理解文本的语义和上下文信息，准确判断文本的类别。

- **机器翻译**：将一种语言的文本翻译成另一种语言。RNN 可以学习源语言句子的结构和语义，并生成目标语言的翻译。在翻译过程中，能够处理句子中的长距离依赖关系，提高翻译的准确性。

- **语音识别**：将语音信号转换为文本。RNN 可以处理语音信号中的时间序列信息，识别出语音中的单词和句子。通过学习大量的语音数据，RNN 能够适应不同人的语音特点和语速变化，提高语音识别的准确率。

1. **时间序列预测**

- **股票价格预测**：根据历史股票价格数据预测未来的股票价格走势。股票价格受多种因素影响，且具有较强的时间序列特征，RNN 能够学习到这些因素之间的复杂关系以及价格变化的趋势，从而进行有效的预测。

- **天气预报**：利用历史气象数据（如温度、湿度、气压等）预测未来的天气情况。气象数据是典型的时间序列数据，RNN 可以捕捉到气象要素在时间上的变化规律和相互关系，为天气预报提供更准确的模型支持。

1. **音乐生成**：根据给定的音乐风格或主题，生成新的音乐旋律。RNN 可以学习音乐的节奏、和声、旋律等特征，通过对大量音乐数据的学习，生成符合特定风格的音乐作品。在音乐创作中，RNN 可以作为辅助工具，为音乐家提供灵感和创意。

### 14-协同过滤算法详解

![0a08b0ea-f480-4608-befe-5ce3cfc649c9_1741930101563340161_origin~tplv-a9rns2rl98-image-qvalue](image\0a08b0ea-f480-4608-befe-5ce3cfc649c9_1741930101563340161_origin~tplv-a9rns2rl98-image-qvalue.jpeg)

在信息爆炸的时代，推荐系统成为帮助用户从海量数据中发现感兴趣内容的重要工具。协同过滤算法作为推荐系统的核心算法之一，通过分析用户的行为数据，发现用户之间的相似性以及物品之间的关联，从而为用户提供个性化的推荐。

#### 一、算法原理

协同过滤算法基于 “人以群分” 的理念，其核心假设是：具有相似行为模式的用户对物品的喜好也相似。具体而言，该算法通过构建用户 - 物品矩阵，其中行表示用户，列表示物品，矩阵元素可以是用户对物品的评分、购买记录、浏览行为等。例如，在一个电商平台的用户 - 商品矩阵中，元素可能是用户是否购买过某商品（0 表示未购买，1 表示购买），或者用户对商品的评分（如 1 - 5 分）。

1. **用户相似度计算**：计算不同用户之间的相似度，常用的相似度度量方法有皮尔逊相关系数、余弦相似度等。以皮尔逊相关系数为例，它衡量两个用户对共同评价过的物品的评分之间的线性相关性。假设用户 A 和用户 B 对物品 i、j、k 都有评分，皮尔逊相关系数的计算公式为：

\( r_{AB}=\frac{\sum_{u\in U_{AB}}(R_{Au}-\overline{R}_A)(R_{Bu}-\overline{R}_B)}{\sqrt{\sum_{u\in U_{AB}}(R_{Au}-\overline{R}_A)^2\sum_{u\in U_{AB}}(R_{Bu}-\overline{R}_B)^2}} \)

其中\(U_{AB}\)是用户 A 和用户 B 共同评价过的物品集合，\(R_{Au}\)表示用户 A 对物品 u 的评分，\(\overline{R}_A\)是用户 A 对所有评价过物品的平均评分。

1. **物品相似度计算**：除了用户相似度，还可以计算物品之间的相似度。例如，计算不同电影之间的相似度，若两部电影被相似的用户群体喜爱，那么它们的相似度就较高。常用的物品相似度计算方法同样包括余弦相似度等。假设物品 i 和物品 j，计算它们被共同用户评价的情况，通过向量表示用户对物品的评价，利用余弦相似度公式计算物品 i 和物品 j 的相似度。

1. **推荐生成**：基于用户相似度或物品相似度，为目标用户生成推荐。如果基于用户相似度，找到与目标用户最相似的若干个用户（即邻居用户），然后根据邻居用户对物品的偏好，将邻居用户喜欢但目标用户未接触过的物品推荐给目标用户。若基于物品相似度，根据目标用户已经喜欢的物品，找到与之相似的其他物品进行推荐。

#### 二、协同过滤算法的分类

1. **基于用户的协同过滤**：主要关注用户之间的相似性。首先计算用户之间的相似度，构建用户相似度矩阵。然后，对于目标用户，根据相似度矩阵找到其邻居用户。最后，根据邻居用户对物品的评分或行为，预测目标用户对未接触物品的喜好程度，并进行推荐。例如在音乐推荐系统中，找到与目标用户音乐品味相似的其他用户，将这些相似用户喜欢的新歌推荐给目标用户。

1. **基于物品的协同过滤**：侧重于物品之间的相似性。先计算物品之间的相似度，得到物品相似度矩阵。对于目标用户，根据其已有的物品偏好（如购买、收藏的物品），通过物品相似度矩阵找到与之相似的物品，将这些相似物品推荐给目标用户。比如在电商平台上，当用户购买了一款手机后，基于物品的协同过滤算法会推荐与该手机相似的其他手机或手机配件。

1. **混合协同过滤**：结合基于用户和基于物品的协同过滤方法，综合考虑用户和物品两方面的信息，以提高推荐的准确性和稳定性。例如，可以在不同阶段分别使用基于用户和基于物品的协同过滤，然后对两种方法得到的推荐结果进行融合；或者在计算相似度时，同时考虑用户和物品的特征信息。

#### 三、协同过滤算法的优缺点

1. **优点**

- **不需要领域知识**：与基于内容的推荐算法不同，协同过滤算法不需要对物品的内容（如文本、图像的特征）进行深入分析，只依赖用户的行为数据，因此适用于各种类型的物品推荐，如电影、音乐、商品等。

- **能够发现新的兴趣点**：通过分析大量用户的行为，协同过滤算法可以发现用户潜在的兴趣点，为用户推荐一些他们可能从未接触过但与他们兴趣相似的物品，从而拓宽用户的视野。例如，在图书推荐中，为喜欢科幻小说的用户推荐一些小众但高质量的科幻作品。

- **个性化程度高**：根据每个用户的独特行为模式进行推荐，能够提供高度个性化的推荐结果，满足不同用户的多样化需求。

1. **缺点**

- **冷启动问题**：当新用户加入系统或有新物品上架时，由于缺乏足够的用户行为数据，难以准确计算用户或物品的相似度，从而影响推荐效果。例如，新上线的电影很难被推荐给用户，因为没有足够的用户评分数据来计算其与其他电影的相似度。

- **数据稀疏性**：在实际应用中，用户 - 物品矩阵往往非常稀疏，即大部分元素为 0（表示用户未对该物品有过行为）。这会导致相似度计算不准确，影响推荐的质量。例如，在一个拥有大量商品的电商平台上，每个用户只购买过少数商品，使得用户 - 商品矩阵稀疏，难以准确找到相似用户或相似商品。

- **可扩展性差**：随着用户和物品数量的增加，计算相似度和生成推荐的计算量会急剧增加，对系统的计算资源和时间性能提出了很高的要求。例如，在一个拥有数亿用户和海量商品的大型电商平台上，实时计算协同过滤推荐结果可能面临很大的挑战。

#### 四、协同过滤算法的应用场景

1. **电商推荐**：各大电商平台广泛使用协同过滤算法为用户推荐商品。根据用户的购买历史、浏览记录、收藏行为等，为用户推荐相关的商品，提高用户的购买转化率和平台的销售额。例如，当用户浏览了一款运动鞋后，电商平台会推荐其他品牌的运动鞋、运动服装以及运动配件等。

1. **视频和音乐推荐**：视频平台（如 YouTube、Netflix）和音乐平台（如 Spotify、网易云音乐）利用协同过滤算法为用户推荐视频或音乐。通过分析用户的观看历史、播放列表、点赞评论等行为，推荐符合用户口味的新视频或音乐作品。比如，根据用户喜欢的摇滚音乐，推荐同类型的摇滚乐队的新歌或经典曲目。

1. **新闻推荐**：新闻客户端通过协同过滤算法为用户推荐个性化的新闻内容。根据用户的阅读历史、收藏分享等行为，找到与目标用户兴趣相似的其他用户，将这些相似用户关注的新闻推荐给目标用户。例如，为关注科技新闻的用户推荐最新的科技动态、科研成果等相关新闻。

1. **旅游推荐**：旅游网站或 APP 利用协同过滤算法，根据用户的旅游目的地选择、酒店预订记录、景点评价等行为，为用户推荐合适的旅游目的地、酒店、旅游线路等。例如，为喜欢海滨度假的用户推荐热门的海滨旅游城市以及相关的度假酒店和游玩项目。

### 15-基于内容的推荐算法详解

在推荐系统的算法体系中，基于内容的推荐算法占据重要地位。随着互联网信息呈指数级增长，用户面临信息过载的困境，推荐系统旨在帮助用户高效地发现感兴趣的内容，基于内容的推荐算法便是达成这一目标的有力手段。

#### 一、算法原理

基于内容的推荐算法主要依据物品自身的特征信息以及用户对物品的偏好历史，为用户推荐与其过往兴趣相符的物品。该算法假设用户对具有相似特征的物品会有相似的喜好。以电影推荐为例，电影的特征包括导演、演员、类型、剧情简介等。如果一位用户过去喜欢观看由某位导演执导，且具有科幻类型和宏大视觉特效特征的电影，那么算法会寻找具有相似导演、类型及特效风格的其他电影推荐给该用户。

#### 二、关键步骤

1. **物品特征提取**：对于不同类型的物品，需要采用合适的方法提取其特征。在文本类物品（如新闻、书籍）中，常利用词袋模型、TF - IDF（词频 - 逆文档频率）等技术将文本转化为向量形式，向量中的每个维度代表一个特征词及其权重。对于图像类物品，可提取颜色直方图、纹理特征、形状特征等。例如在一幅风景图像中，通过颜色直方图可以描述图像中不同颜色的分布情况，作为图像的特征之一。对于音乐类物品，可提取节奏、旋律、和声等特征。

1. **用户画像构建**：根据用户对物品的历史行为（如浏览、购买、评分等），结合物品的特征信息，构建用户画像。例如，用户 A 对多部科幻电影给出高分评价，那么在用户 A 的画像中，科幻电影相关的特征（如科幻类型标签、科幻电影常见关键词等）的权重会相应提高。通过不断更新用户的行为数据，持续优化用户画像，使其更精准地反映用户的兴趣偏好。

1. **推荐生成**：计算目标用户画像与物品特征向量之间的相似度，常见的相似度计算方法有余弦相似度、欧氏距离等。以余弦相似度为例，它衡量两个向量在方向上的相似程度，取值范围在 [-1, 1] 之间，值越接近 1，表示两个向量越相似。将与目标用户画像相似度高的物品推荐给用户。例如，在图书推荐中，计算用户画像与每本图书的特征向量的余弦相似度，选取相似度排名靠前且用户未阅读过的图书进行推荐。

#### 三、优势

1. **推荐的准确性较高**：由于直接基于物品的特征和用户的兴趣偏好进行推荐，只要特征提取准确，就能精准地为用户推荐符合其兴趣的物品。在专业领域的内容推荐中，如学术文献推荐，基于内容的推荐算法能够根据文献的关键词、研究方向等特征，为科研人员推荐高度相关的文献，助力其研究工作。

1. **新物品推荐友好**：对于新上架的物品，只要能够提取其特征，就可以将其纳入推荐体系。不像协同过滤算法受限于用户对新物品的行为数据缺失。例如，新发布的一款电子产品，基于内容的推荐算法可以根据其产品参数、功能特点等特征，将其推荐给对该类产品感兴趣的用户。

1. **可解释性强**：推荐结果具有清晰的解释性，能够明确地向用户说明推荐某物品的原因，是因为该物品与用户过往喜欢的物品在某些特征上相似。例如，在音乐推荐中，可以向用户解释推荐某首歌曲是因为它与用户之前收藏的歌曲具有相同的音乐风格、歌手等特征，增强用户对推荐系统的信任。

#### 四、劣势

1. **特征提取难度大**：对于复杂的物品（如图像、视频），准确提取其特征并非易事，需要专业的领域知识和复杂的技术。例如，对于视频内容，不仅要提取画面特征，还需考虑音频、剧情等多方面特征，且不同类型视频的特征提取重点和方法各异。此外，对于一些抽象的概念（如艺术风格、情感倾向），特征提取的准确性难以保证。

1. **用户兴趣变化适应性差**：用户的兴趣可能随时间变化，而基于内容的推荐算法主要依据用户过去的行为构建画像，对用户兴趣的动态变化反应相对迟缓。例如，一位用户原本喜欢健身相关的内容，但近期由于工作变动，兴趣转向了职场技能提升，基于内容的推荐算法可能仍会持续推荐健身相关内容，无法及时适应用户兴趣的转变。

1. **物品长尾问题**：对于一些小众、独特的物品，由于缺乏足够的相似物品，基于内容的推荐算法可能难以找到合适的推荐对象，导致这些物品难以被推荐给合适的用户，影响了物品的曝光和传播。例如，一些小众艺术电影、独立音乐作品，在基于内容的推荐中可能因为特征过于独特，难以与其他作品匹配，从而无法有效触达目标用户。

#### 五、应用场景

1. **新闻资讯推荐**：新闻客户端利用基于内容的推荐算法，根据新闻的主题、关键词、来源等特征，结合用户的阅读历史和偏好，为用户推荐相关的新闻文章。例如，对于关注科技领域的用户，推荐最新的科技动态、科研成果等相关新闻，满足用户对特定领域信息的需求。
1. **图书推荐**：在图书销售平台上，通过提取图书的作者、题材、内容简介等特征，以及分析用户的阅读历史和评价，为用户推荐符合其阅读口味的图书。比如，为喜欢推理小说的用户推荐同类型的经典作品或新晋作者的佳作，提升用户的阅读体验和图书销售转化率。
1. **视频内容推荐**：视频平台（如爱奇艺、腾讯视频）运用基于内容的推荐算法，对视频的类别（如电视剧、电影、综艺）、演员、剧情等特征进行分析，结合用户的观看历史和收藏行为，为用户推荐可能感兴趣的视频。例如，为喜欢古装剧的用户推荐同类型的热门剧集或新上线的古装作品，增加用户在平台上的停留时间和观看满意度。
1. **商品推荐（专业性商品）**：在一些专业性较强的商品领域，如数码产品、医疗器械等，基于内容的推荐算法根据商品的技术参数、功能特点等特征，为用户推荐合适的商品。例如，为摄影爱好者推荐符合其需求的相机及配件，帮助用户快速找到满足自身专业需求的商品。

### 16-NLP 舆情分析算法详解

#### 一、引言

在信息爆炸的当下，互联网文本数据海量剧增。舆情分析凭借自然语言处理（NLP）技术，成为挖掘这些数据价值的关键途径。NLP 舆情分析算法的目标是从海量文本中精准提炼出有价值的舆情信息，为企业、政府等各类组织的决策提供有力支撑。接下来详细阐述几种常见的 NLP 舆情分析算法。

#### 二、情感分析算法

1. 基于词典的情感分析算法
   - **原理**：构建一个涵盖丰富情感倾向词汇的情感词典，为每个词汇明确赋予积极、消极或中性的情感极性以及强度值。在对文本进行情感分析时，将文本中的词汇与情感词典中的词汇逐一比对，依据比对结果计算文本的情感得分，进而判断文本的情感倾向。
   - **示例**：以知网的 HowNet 情感词典为例，“喜悦”“畅快” 等词汇被标记为积极情感，“哀愁”“烦闷” 等词汇被标记为消极情感。对于文本 “今天的聚会特别有意思，我感到无比畅快”，通过与情感词典匹配，可识别出 “有意思”“畅快” 等积极词汇，经计算积极词汇的数量和强度，可判断该文本具有积极情感倾向。
2. 基于机器学习的情感分析算法
   - **原理**：首先大规模收集已准确标注情感极性的文本数据作为训练集。然后选取合适的机器学习算法，如朴素贝叶斯、支持向量机、神经网络等，对训练集进行训练，构建出能够精准识别文本情感极性的分类模型。在预测阶段，将待分析的文本输入已训练好的模型，模型依据训练过程中学习到的文本特征和模式，对文本的情感极性进行分类。
   - **示例**：以朴素贝叶斯算法为例，该算法假定文本中的每个词汇相互独立。通过统计词汇在积极文本和消极文本中的出现概率，计算出文本属于积极或消极类别的概率。例如，对于文本 “这部电视剧剧情拖沓，表演也很生硬”，模型在训练过程中学习到 “拖沓”“生硬” 等词汇在消极文本中出现的概率较高，从而判断该文本具有消极情感倾向。

#### 三、主题模型算法

1. 潜在狄利克雷分配（LDA）算法
   - **原理**：LDA 是一种无监督的主题模型算法，其核心假设是文档由多个主题混合构成，每个主题由一系列词汇组成。通过对大量文档进行深入分析，LDA 算法能够自动挖掘出文档集合中的潜在主题结构，同时确定每个文档与主题之间的概率分布以及每个主题与词汇之间的概率分布。
   - **示例**：在分析社交媒体上的舆情数据时，LDA 算法可识别出诸如 “时事热点”“文化艺术”“生活消费” 等不同的话题主题。例如，对于一条社交媒体帖子 “最新的科技成果发布会引起广泛关注”，LDA 算法会计算该帖子与 “时事热点” 主题的相关程度，发现其与该主题关联紧密。
2. 非负矩阵分解（NMF）算法
   - **原理**：NMF 同样用于主题提取，它将文档 - 词汇矩阵分解为两个非负矩阵，一个矩阵体现文档与主题的关系，另一个反映主题与词汇的关系。与 LDA 不同，NMF 更侧重于通过矩阵分解的方式，找出一组能够线性组合表示原始文档的基向量（主题）。
   - **示例**：在舆情分析中，NMF 可助力发现舆情数据中的主要主题。例如，对一系列关于城市规划的舆情文本进行分析，NMF 算法能够找出如 “区域布局”“交通规划”“生态建设” 等主要主题，并呈现这些主题在不同时间段或不同群体中的分布变化情况。

#### 四、文本分类算法

1. 支持向量机（SVM）算法
   - **原理**：SVM 是一种经典的机器学习算法，在文本分类任务中，通过寻找一个最优的超平面，将不同类别的文本数据区分开来。它将文本表示为向量空间中的向量，借助核函数将低维向量映射至高维空间，以解决文本数据在低维空间中线性不可分的问题。
   - **示例**：在舆情分析场景中，SVM 可用于将舆情文本分类为不同类别。比如，将新闻文本划分为政治、经济、文化等类别，或者将用户评论归为正面、负面和中性评论。对于一条新闻文本 “政府出台新的税收政策刺激经济增长”，SVM 模型根据文本中的关键词、语义等特征，将其分类为经济类别。
2. 决策树算法
   - **原理**：决策树是基于树结构的分类算法，它通过对文本特征进行测试和划分，构建一棵决策树来实现文本分类。决策树的每个内部节点对应一个特征属性上的测试，分支为测试输出，叶节点则为类别标签。
   - **示例**：在舆情分析中，决策树可依据文本的特定特征，如关键词的出现与否、文本的长度等对舆情文本分类。例如，对于用户评论 “这款产品质量超棒，值得购买”，决策树可能根据其中 “超棒”“值得购买” 等关键词以及文本整体的积极表述，将其分类为正面评论，并且决策树的结构能够直观展示分类的决策过程。

#### 五、命名实体识别算法

1. 基于规则的命名实体识别算法
   - **原理**：该算法依据预先精心定义的规则和模式来识别文本中的命名实体。这些规则可基于语法、语义以及特定领域知识制定。例如，通过定义 “姓氏 + 名字” 的模式来识别人员姓名，利用 “地名修饰词 + 地名” 的模式来识别地理名称。
   - **示例**：在特定领域的舆情分析中，如金融领域，基于规则的方法可快速准确识别出公司名称、金融产品名称等实体。假设文本为 “建设银行推出了新的理财产品”，通过预定义的规则可识别出 “建设银行” 这一公司名称。但该方法编写规则需大量领域知识与人工投入，且对新出现的实体及复杂语言现象适应性欠佳。
2. 基于深度学习的命名实体识别算法
   - **原理**：近年来，基于深度学习的方法在命名实体识别任务中取得了显著成果。例如，利用循环神经网络（RNN）及其变体长短时记忆网络（LSTM）、门控循环单元（GRU）等对文本进行建模，能够有效捕捉文本中的上下文信息与语义特征，显著提升命名实体识别的准确率。此外，结合条件随机场（CRF）可进一步优化命名实体的边界识别，获取更精准的识别结果。
   - **示例**：在舆情分析中，基于深度学习的命名实体识别算法能自动学习丰富的语义特征，对各种类型的命名实体都有良好识别效果。对于文本 “马云宣布阿里巴巴将加大在人工智能领域的投入”，该算法可准确识别出 “马云” 这一人物实体以及 “阿里巴巴”“人工智能” 等组织和领域实体，并且能够适应不同领域和风格的文本数据。

### 17-朴素贝叶斯算法

#### 一、原理

- **基于贝叶斯定理**：*P*(*C*∣*F*)=*P*(*F*)*P*(*F*∣*C*)*P*(*C*)。这里的*P*(*C*∣*F*)是在给定特征*F*的情况下类别*C*的后验概率，*P*(*F*∣*C*)是在类别*C*下特征*F*出现的概率，*P*(*C*)是类别*C*的先验概率，*P*(*F*)是特征*F*的概率。
- **特征条件独立假设**：假定每个特征对于给定类别的影响是相互独立的，也就是*P*(*F*1,*F*2,⋯,*F**n*∣*C*)=*P*(*F*1∣*C*)*P*(*F*2∣*C*)⋯*P*(*F**n*∣*C*) ，其中*F*1,*F*2,⋯,*F**n*为不同的特征。此假设能简化计算，通过算出每个特征在各个类别下的概率，进而计算给定特征集合属于某个类别的概率，最后选取概率最高的类别作为预测结果。

#### 二、算法步骤

1. **数据准备**：收集并整理带有类别标签的训练数据，对文本数据开展预处理，例如分词、去除停用词等操作，之后将其表示成特征向量。
2. **计算先验概率**：统计每个类别在训练数据里出现的频率，把它当作该类别的先验概率*P*(*C*)。
3. **计算条件概率**：针对每个类别*C*和每个特征*F**i*，统计*F**i*在类别*C*的文本中出现的频率，从而计算得到条件概率*P*(*F**i*∣*C*)。
4. **预测**：对于待分类的文本，将其转化为特征向量，依据贝叶斯定理和特征条件独立假设，计算该文本属于每个类别的概率*P*(*C*∣*F*1,*F*2,⋯,*F**n*)，选择概率最大的类别作为预测结果。

#### 三、示例

假设有以下一些关于水果的文本数据，已标注为 “苹果” 和 “橙子” 两类：

- “红色的，圆形的，甜的” - 苹果
- “橙色的，圆形的，多汁的” - 橙子
- “红色的，甜的，脆的” - 苹果

现在有一个新的文本 “红色的，圆形的，多汁的” 要进行分类。

1. 初始概率计算（未平滑）

   - 先计算先验概率：苹果，橙子。
   - 计算条件概率：红色苹果，红色橙子；圆形苹果，圆形橙子；多汁苹果，多汁橙子。
   - 计算新文本属于苹果的概率：苹果红色圆形多汁红色苹果圆形苹果多汁苹果苹果红色圆形多汁红色圆形多汁。
   - 计算新文本属于橙子的概率：橙子红色圆形多汁红色橙子圆形橙子多汁橙子橙子红色圆形多汁红色圆形多汁。

2. 使用拉普拉斯平滑重新计算

   在实际计算时，通常会对概率进行平滑处理，以避免出现概率为

   0

   的情况。假设使用拉普拉斯平滑，平滑参数为

   1

   。

   - 红色苹果，红色橙子；圆形苹果，圆形橙子；多汁苹果，多汁橙子。
   - 苹果红色圆形多汁红色苹果圆形苹果多汁苹果苹果红色圆形多汁红色圆形多汁红色圆形多汁。
   - 橙子红色圆形多汁红色橙子圆形橙子多汁橙子橙子红色圆形多汁红色圆形多汁红色圆形多汁。

比较两者概率大小，323>814，所以预测该文本属于苹果类别。

#### 四、优缺点

1. 优点
   - 算法简单，计算量小，在处理大规模文本数据时效率较高。
   - 对缺失数据不太敏感。
   - 在文本分类任务中，特别是当特征之间满足条件独立假设时，往往能取得较好的效果。
2. 缺点
   - 特征条件独立假设在实际中往往难以完全满足，这可能会影响分类的准确性。
   - 对数据的依赖性较强，如果训练数据不具有代表性，可能导致分类结果不准确。















# 机器学习实战

## 1-四种算法对比对客户信用卡还款情况进行预测

信用卡又叫贷记卡，是由商业银行或信用卡公司对信用合格的消费者发行的信用证明。现在的年轻人，特别是80后，90后甚至00后到喜欢超前消费，每个人名下多多少少都有至少一张信用卡，有些人由于过度超前消费，导致下个月无法还款导致的逾期，这样会对个人征信产生影响，今天我们就来分析分析具有哪些特性的人会有信用卡逾期的可能。

### 一、前期工作

##### 1. 导入库包

```
import pandas as pd
import numpy as np
from sklearn.model_selection import learning_curve, train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from matplotlib import pyplot as plt
import seaborn as sns
```

####   2.导入数据 

```
# 数据加载
data = pd.read_csv('Credit_Card.csv')

print(data.shape) # 查看数据集大小
print(data.describe()) # 数据集概览
```

  (30000, 25)                

![Snipaste_2025-03-13_17-20-04](image\Snipaste_2025-03-13_17-20-04.png)

数据样例：

![下载](image\下载.png)

### 二、数据分析和可视化

#### 1.查看年龄分布情况

```
# 查看年龄分布情况
age = data['AGE']
payment = data[data["payment.next.month"]==1]['AGE']
bins =[20,30,40,50,60,70,80]
seg = pd.cut(age,bins,right=False)
print(seg)
counts =pd.value_counts(seg,sort=False)
b = plt.bar(counts.index.astype(str),counts)
plt.bar_label(b,counts)
plt.show()
```

![下载 (1)](image\下载 (1).png)

信用卡使用最多的年龄是在30-40岁之间，有11238人，其实是20-30岁的人，有9618人，80后90后是信用卡使用的大军。
信用卡有逾期的客户年龄分布：

```
#逾期的用户年龄分布
payment_seg = pd.cut(payment,bins,right=False)
counts1 =pd.value_counts(payment_seg,sort=False)
b2 = plt.bar(counts1.index.astype(str),counts1,color='r')
plt.bar_label(b2,counts1)
plt.show()
```

![下载 (2)](image\下载 (2).png)

逾期率对比：
20-30岁：22.84%，
30-40岁：20.25%，
40-50岁：22.97，
50-60岁：24.86%，
70-80岁：28%
可以看出70-80岁逾期率最高，可能是他们年龄的原因忘记还款，或者子女未帮忙还款所致；

#### 2.查看下一个月逾期率的情况

```
next_month = data['payment.next.month'].value_counts()
print(next_month)
df = pd.DataFrame({'payment.next.month': next_month.index,'values': next_month.values})
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.figure(figsize = (6,6))

plt.title('逾期率客户\n (还款：0,逾期：1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'payment.next.month', y="values", data=df)
plt.show()
```

![下载 (3)](image\下载 (3).png)

### 三、数据特征处理

```
# 特征选择，去掉ID字段、最后一个结果字段即可
data.drop(['ID'], inplace=True, axis =1) #ID这个字段没有用
target = data['payment.next.month'].values
columns = data.columns.tolist()
columns.remove('payment.next.month')
features = data[columns].values

# 70%作为训练集，30%作为测试集
train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1)
```

### 四、机器学习算法分类器

下面我们采用四种机器学习算法进行分类预测，分别是支持向量机、决策树、随机森林、 K近邻算法，小伙伴是不是对这四类算法一下子有了熟悉的感觉。

```
# 构造各种分类器
classifiers = [
    SVC(random_state = 1, kernel = 'rbf'),     # 支持向量机分类
    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),  # 决策树分类
    RandomForestClassifier(random_state = 1, criterion = 'gini'),  # 随机森林分类
    KNeighborsClassifier(metric = 'minkowski'),    # K近邻分类
]
# 分类器名称
classifier_names = [
            'svc',
            'decisiontreeclassifier',
            'randomforestclassifier',
            'kneighborsclassifier',
]
# 分类器参数
classifier_param_grid = [
            {'svc__C':[1], 'svc__gamma':[0.01]},
            {'decisiontreeclassifier__max_depth':[6,9,11]},
            {'randomforestclassifier__n_estimators':[3,5,6]} ,
            {'kneighborsclassifier__n_neighbors':[4,6,8]},
]
```

### 五、参数调优

```
# 对具体的分类器进行GridSearchCV参数调优
def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = 'accuracy'):
    response = {}
    gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score)

    # 寻找最优的参数 和最优的准确率分数
    search = gridsearch.fit(train_x, train_y)
    print("GridSearch最优参数：", search.best_params_)
    print("GridSearch最优分数： %0.4lf" %search.best_score_)
    predict_y = gridsearch.predict(test_x)
    print("准确率 %0.4lf" %accuracy_score(test_y, predict_y))
    response['predict_y'] = predict_y
    response['accuracy_score'] = accuracy_score(test_y,predict_y)
    return response
```

### 六、模型对比分析

```
for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):
    pipeline = Pipeline([
            ('scaler', StandardScaler()),
            (model_name, model)
    ])
    result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = 'accuracy')
Name: payment.next.month, dtype: int64
GridSearch最优参数： {'svc__C': 1, 'svc__gamma': 0.01}
GridSearch最优分数： 0.8186
准确率 0.8172
GridSearch最优参数： {'decisiontreeclassifier__max_depth': 6}
GridSearch最优分数： 0.8208
准确率 0.8113
GridSearch最优参数： {'randomforestclassifier__n_estimators': 6}
GridSearch最优分数： 0.8004
准确率 0.7994
GridSearch最优参数： {'kneighborsclassifier__n_neighbors': 8}
GridSearch最优分数： 0.8040
准确率 0.8036
```

我们可以看到运行结果：
支持向量机算法分类：准确率 0.8172
决策树算法分类：准确率 0.8113
随机森林分类：准确率 0.7994
K近邻分类：准确率 0.8036
这四种算法中，准确率都差不多，其中准确率最高的是支持向量机算法。

## 2-聚类算法分析亚洲足球梯队

2022卡塔尔世界杯决赛圈名单如下，各大州的分布：

欧洲(13)：比利时、克罗地亚、丹麦、英国、法国、德国、荷兰、塞尔维亚、西班牙、瑞士、威尔士、波兰、葡萄牙。

亚洲(6)：卡塔尔、伊朗、日本、韩国、沙特阿拉伯、澳大利亚。

南美洲(4)：巴西、阿根廷、乌拉圭、厄瓜多尔。

非洲(5)：塞内加尔、突尼斯、摩洛哥、喀麦隆、加纳。

北美和加勒比海地区(4)：美国、加拿大、墨西哥、哥斯达黎加。

遗憾还是没有看到国足的身影，那只能看看国足在亚洲处于哪个梯队。

下面是详细的代码介绍

一、前期工作

#### 1. 导入库包

```
from sklearn.cluster import KMeans
from sklearn import preprocessing
import pandas as pd
import numpy as np
```

#### 2.导入数据

```
# 数据加载
data = pd.read_csv('team_cluster_data.csv', encoding='gbk')
train_x = data[["2019国际排名","2018世界杯排名","2015亚洲杯排名"]]
```

![下载(4)](image\下载(4).png)

#### 3.聚类算法

```
kmeans = KMeans(n_clusters=3)
# 规范化到 [0,1] 空间
min_max_scaler=preprocessing.MinMaxScaler()
train_x=min_max_scaler.fit_transform(train_x)

predict_y = kmeans.fit_predict(train_x)
# 合并聚类结果，插入到原数据中
result = pd.concat((data,pd.DataFrame(predict_y)),axis=1)
result.rename({0:u'聚类结果'},axis=1,inplace=True)
print(result)
```

#### 4.运行结果

```
 国家  2019国际排名  2018世界杯排名  2015亚洲杯排名  聚类结果
0       中国        73         40          7     0
1       日本        60         15          5     2
2       韩国        61         19          2     2
3       伊朗        34         18          6     2
4       沙特        67         26         10     2
5      伊拉克        91         40          4     0
6      卡塔尔       101         40         13     1
7      阿联酋        81         40          6     0
8   乌兹别克斯坦        88         40          8     0
9       泰国       122         40         17     1
10      越南       102         50         17     1
11      阿曼        87         50         12     1
12      朝鲜       110         50         14     1
13      印尼       164         50         17     3
14      澳洲        40         30          1     2
15     叙利亚        76         40         17     1
16      约旦       118         50          9     0
17     科威特       160         50         15     3

Process finished with exit code 0
```

聚类结果数值发现：

（数值2）第一梯队：日本、韩国、 伊朗、沙特、澳洲

（数值0）第二梯队：中国、伊拉克、阿联酋、乌兹别克斯坦、约旦

（数值1）第三梯队：卡塔尔、泰国、越南、阿曼、朝鲜

（数值3）第四梯队：印尼、科威特

国足属于第二梯队，当然这个数值分类还是随着国际比赛是时刻变化的。

## 3-利用决策树算法根据天气数据集做出决策

### **一、决策树的介绍**

对于决策树算法，想一棵树一样有节点与分支，每个节点代表一个特征属性，对应着数据集中的一个特征。每个节点都有一个决策规则，用于判断当前数据样本的特征属性值是否满足要求，根据规则的判断结果，将数据样本分配到该节点的某个子节点。

决策树的构建是通过一种递归的分割方式实现的，每一次分割都是为了提高模型的预测准确性。决策树的生成过程包括三个步骤：

选择最佳特征，划分数据集和递归建树。选择最佳特征的过程是通过计算数据集中各个特征的信息增益或信息增益比等指标，找到最适合用来进行分割的特征。

在根据最佳特征将数据集划分成子集，每个子集对应着决策树的一个分支，然后递归地对子集进行上述操作，直到达到预定的停止条件为止。

再通过决策树可视化工具，可以将决策树图像化，直观地展示决策树的构建过程和结果。

![下载 (4)](image\下载 (4).png)

### **二、决策树的应用**

决策树被广泛用于分类和回归的各种实际问题。在生活中，例如在天气的变化方面，使用决策树可以帮助我们对明天是否出行做出决策。

下面我用一个简单的天气数据集作为例子来演示决策树的应用。我们已经创建了一个包含以下特征的CSV文件:

![Snipaste_2025-03-13_17-55-09](image\Snipaste_2025-03-13_17-55-09.png)

其中数据说明：

- 外观（Sunny、Overcast和Rainy）
- 温度（Hot、Mild和Cool）
- 湿度（High和Normal）
- 风（Weak和Strong）

最后列是目标标签，即是否出行。 

### **三、决策树的代码实例**

接下来，我们将使用Python代码使用决策树算法来判断明天是否出行。

```
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.tree import DecisionTreeClassifier,plot_tree
import matplotlib.pyplot as plt
#读取csv数据
data = pd.read_csv('weather.csv')

#将字符串编码为数字
label_encoder = LabelEncoder()
data['Outlook'] = label_encoder.fit_transform(data['Outlook'])
data['Temperature'] = label_encoder.fit_transform(data['Temperature'])
data['Humidity'] = label_encoder.fit_transform(data['Humidity'])
data['Windy'] = label_encoder.fit_transform(data['Windy'])

#将数字特征进行独热编码
one_hot_encoder = OneHotEncoder(categories='auto')
encoded_features = one_hot_encoder.fit_transform(data[['Outlook', 'Temperature']]).toarray()

#将Play列映射为二进制类别变量
data['Play'] = data['Play'].map({'Yes': 1, 'No': 0})

#将编码后的特征和标签分割
X = pd.concat([pd.DataFrame(encoded_features), data[['Windy', 'Humidity']]], axis=1)
y = data['Play']

#建立决策树模型并进行拟合
model = DecisionTreeClassifier()
model.fit(X, y)

#预测新数据
#【Sunny,Hot,High,Weak】编码为[0, 0, 1, 0, 1, 0, 1, 0]
new_data = pd.DataFrame([[0, 0, 1, 0, 1, 0, 1, 0]], columns=X.columns)
prediction = model.predict(new_data)

if prediction == 1:
    print('Play: No')
else:
    print('Play: Yes')
```

输入预测数据：【Sunny,Hot,High,Weak】编码为[0, 0, 1, 0, 1, 0, 1, 0]

运行结果：Play: Yes

下面用代码生成决策树图：

```
# 画出决策树
plt.figure(figsize=(8, 8))
plot_tree(model, filled=True, feature_names=X.columns, class_names=['Not Play', 'Play'])
plt.show()
```

### ![下载 (5)](image\下载 (5).png)四、决策树还有其他应用

**1. 特征选择：**通过决策树学习的过程可以得到各个特征在分类中的重要性，从而进行特征选择。

**2. 模式识别：**决策树可以用于语音识别、手写数字识别等模式识别问题。

**3. 数据挖掘：**决策树可以用于挖掘数据中的关联规则、异常值等。

**4. 购物推荐：**决策树可以根据用户的历史购买情况，推荐其可能感兴趣的商品。

**5. 医疗诊断：**决策树可以用于分析患者的症状和疾病之间的关系，辅助医生进行诊断。

**6. 金融风险评估：**决策树可以用于评估贷款申请人的信用风险等。 综上所述，决策树在数据分析、人工智能和机器学习等领域有广泛的应用。

## 4-教育领域:学生成绩的可视化分析与成绩预测

机器学习可以通过对学生的父母教育情况和学校表现等数据进行分析和挖掘，从而揭示潜在的学习模式和趋势。这种可视化分析可以帮助教师更好地了解学生的学习状况，并针对性地调整教学策略。机器学习还可以利用学生的历史数据、课程表、出勤记录等信息，建立模型来预测学生未来的成绩。这种预测可以帮助教师及时发现学生可能存在的问题并采取相应的措施加以干预，从而提高学生的学习效果和成绩。

#### **一、导入库和数据**

```
import numpy as np
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
plt.rcParams['font.sans-serif'] = ['SimHei']

df_pre = pd.read_csv('exams.csv')
df_pre[['math score', 'reading score', 'writing score']].agg(['var', 'std'])

correlation_matrix = df_pre.corr()
```

数据样例：

![下载](image\下载.png)

#### **二、**创建一个热图的相关矩阵

```
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')

plt.show()
```

![下载 (7)](image\下载 (7).png)

####  三、学生父母的教育水平和成绩之间的关系

```
education_score = df_pre.groupby('parental level of education')[['math score', 'reading score', 'writing score']].mean().reset_index()
education_score['average score'] = (education_score['math score']+education_score['reading score']+education_score['writing score'])/3
education_score = education_score.sort_values('average score', ascending=False)

plt.figure(figsize=(13,4))
plt.plot(education_score['parental level of education'], education_score['math score'], marker='o', label='Math Score')
plt.plot(education_score['parental level of education'], education_score['reading score'], marker='o', label='Reading Score')
plt.plot(education_score['parental level of education'], education_score['writing score'], marker='o', label='Writing Score')
plt.plot(education_score['parental level of education'], education_score['average score'], marker='s', label='Average Score')

plt.title('学生父母的教育水平和成绩之间的关系')
plt.xlabel('教育水平')
plt.ylabel('成绩')

plt.legend()
plt.show()
```

![下载 (8)](image\下载 (8).png)

#### 四、种族和成绩之间的关系

```
race_score = df_pre.groupby('race/ethnicity')[['math score', 'reading score', 'writing score']].mean().reset_index()
race_score['average score'] = (race_score['math score']+race_score['reading score']+race_score['writing score'])/3
race_score = race_score.sort_values('average score', ascending=False)

plt.figure(figsize=(13,4))
plt.plot(race_score['race/ethnicity'], race_score['math score'], marker='o', label='Math Score')
plt.plot(race_score['race/ethnicity'], race_score['reading score'], marker='o', label='Reading Score')
plt.plot(race_score['race/ethnicity'], race_score['writing score'], marker='o', label='Writing Score')
plt.plot(race_score['race/ethnicity'], race_score['average score'], marker='s', label='Average Score')

plt.title('种族和成绩之间的关系')
plt.xlabel('种族')
plt.ylabel('成绩')

plt.legend()
plt.show()
```

![下载 (9)](image\下载 (9).png)

#### 五、测试准备课程和成绩之间的关系

```
prep_score = df_pre.groupby('test preparation course')[['math score', 'reading score', 'writing score']].mean().reset_index()
prep_score['average score'] = (prep_score['math score']+prep_score['reading score']+prep_score['writing score'])/3
prep_score = prep_score.sort_values('average score', ascending=False)

plt.figure(figsize=(13,4))
plt.plot(prep_score['test preparation course'], prep_score['math score'], marker='o', label='Math Score')
plt.plot(prep_score['test preparation course'], prep_score['reading score'], marker='o', label='Reading Score')
plt.plot(prep_score['test preparation course'], prep_score['writing score'], marker='o', label='Writing Score')
plt.plot(prep_score['test preparation course'], prep_score['average score'], marker='s', label='Average Score')

plt.title('测试准备课程和成绩之间的关系')
plt.xlabel('完成与否')
plt.ylabel('成绩')

plt.legend()
plt.show()
```

![下载 (10)](image\下载 (10).png)

#### 六、父母的教育水平/学生是否完成测试准备课程的饼图

```
df_pre.groupby('test preparation course')[['math score', 'reading score', 'writing score']].agg(['var', 'std'])

par_test_count = df_pre[['parental level of education', 'test preparation course']].value_counts().to_frame().reset_index().rename(columns={0:'Count'}).sort_values('Count', ascending=False)

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,4))

# Create the first pie chart for the count of students who completed the test preparation course
ax1.pie(par_test_count[par_test_count['test preparation course']=='completed']['Count'],
        labels=par_test_count[par_test_count['test preparation course']=='completed']['parental level of education'],
        autopct='%1.2f%%')
ax1.set_title('父母的教育水平的饼图  学生完成测试准备课程')

# Create the second pie chart for the count of students who did not complete the test preparation course
ax2.pie(par_test_count[par_test_count['test preparation course']=='none']['Count'],
        labels=par_test_count[par_test_count['test preparation course']=='none']['parental level of education'],
        autopct='%1.2f%%')
ax2.set_title('父母的教育水平的饼图 学生没有完成测试准备课程')

# Show the plot
plt.show()
```

![下载 (11)](image\下载 (11).png)

#### **七、比较男性和女性之间的数学分数**

```
df_pre.groupby('gender').mean()

sns.violinplot(x='gender', y='math score', data=df_pre)

# Add labels and title
plt.xlabel('Gender')
plt.ylabel('Math Score')
plt.title('比较男性和女性之间的数学分数')
# Show the plot
plt.show()
```

![下载 (12)](image\下载 (12).png)

#### 八、基于性别数学分数的散点图

```
plt.figure(figsize=(10,5))
sns.scatterplot(x=range(0, len(df_pre)), y="math score", hue="gender", data=df_pre)

# Add labels and title
plt.title('基于性别数学分数的散点图')
plt.xlabel('学生数')
plt.ylabel('成绩')

# Show the plot
plt.show()
```

![下载 (13)](image\下载 (13).png)

#### 九、学生各科成绩分布图

```
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))

# Plot for math
ax1.set_title('数学成绩的分布')
ax1.hist(df_pre['math score'], edgecolor='black')

# Plot for reading
ax2.set_title('阅读成绩的分布')
ax2.hist(df_pre['reading score'], edgecolor='black')

# Plot for writing
ax3.hist(df_pre['writing score'], edgecolor='black')
ax3.set_title('写作成绩的分布')

# Show plots
plt.show()
```

![下载 (14)](image\下载 (14).png)

####  十、机器学习模型比较

```
df = pd.get_dummies(df_pre)


# Assign variables
X = df.drop('math score', axis=1)
y = df['math score']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), SVR(kernel='linear'), SVR(kernel='poly'), SVR(kernel='rbf')]

# Use cross-validation to compute the R-squared score for each model
cv_scores = []
for model in models:
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)
    cv_scores.append(scores.mean())

# Plot the results
fig, ax = plt.subplots(figsize=(15, 6))
rects = ax.bar(['Linear', 'Decision Tree', 'Random Forest', 'SVR - Linear', 'SVR - Poly', 'SVR - Rbf'], cv_scores, color='orange')
ax.set_ylim(0, 1)
ax.set_title('回归模型的比较')
ax.set_xlabel('Model')
ax.set_ylabel('R-squared')

# Add labels above each bar
for rect in rects:
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height:.5f}', ha='center', va='bottom')

# Show the plot
plt.show()
```

![下载 (15)](image\下载 (15).png)

## 5-天气预测系列:利用数据集可视化分析数据，并预测某个城市的天气情况

### 一、准备工作

首先，我们需要了解一下数据集中包含哪些信息。原始数据集可能包含多个变量，但我们主要关注年平均温度和湿度这两个因素对天气状况的影响。年平均温度和湿度可以很好地反映该城市的气候状况，因此它们是预测天气状况的重要变量。我们会对数据集中的各种字段进行分析。

在数据预处理和分析完成之后，我们可以使用各种机器学习算法进行预测。这些算法可以分为有监督学习和无监督学习。有监督学习算法需要使用标记数据集进行训练，以生成预测模型。常用的有监督学习算法包括线性回归、决策树、随机森林、向量机分类模型(SVC算法)等。无监督学习算法则不需要标记数据集，而是通过发现数据集中的潜在规律进行预测。常用的无监督学习算法包括聚类、降维等。本文采用向量机分类模型进行分类预测。

### 二、代码实践

#### **1.数据导入**

```
import pandas as pd 

import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.sans-serif']=['SimHei']

df = pd.read_csv('weather_dataset.csv')
labels = pd.read_csv('weather_labels.csv')
```



![Snipaste_2025-03-14_10-35-33](image\Snipaste_2025-03-14_10-35-33.png)

#### **2.2000年的温度变化图**

```
df_budapest = pd.concat([df.iloc[:,:2],df.iloc[:,11:19]],axis=1)
df_budapest['DATE'] = pd.to_datetime(df_budapest['DATE'],format='%Y%m%d')

def mean_for_mth(feature):
    mean = []
    for x in range(12):
        mean.append(
            float("{:.2f}".format(df_budapest[df_budapest['MONTH'] == (x+1)][feature].mean())))
    return mean

df_budapest.drop(['MONTH'],axis=1).describe()

#sns.set(style="darkgrid")
plt.figure(figsize=(12,6))
plt.plot(df_budapest['DATE'][:365],df_budapest['BUDAPEST_temp_mean'][:365])
plt.title('2000年的温度变化图')
plt.xlabel('DATE')
plt.ylabel('DEGREE')

plt.show()
```

![下载 (16)](image\下载 (16).png)

#### 3**.**布达佩斯(匈牙利首都)年平均温度

```
months = ['Jan', 'Febr', 'Mar', 'Apr', 'May', 'Jun', 'Jul',
          'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
mean_temp = mean_for_mth('BUDAPEST_temp_mean')

plt.figure(figsize=(12,6))
bar = plt.bar(x = months,height = mean_temp, width = 0.8, color=['thistle','mediumaquamarine',                'orange'])
plt.xticks(rotation = 45)
plt.xlabel('MONTHS')
plt.ylabel('DEGREES')
plt.title('布达佩斯(匈牙利首都)年平均温度')
plt.bar_label(bar)
plt.show()
```

![下载 (17)](image\下载 (17).png)

#### 4**.**布达佩斯(匈牙利首都)的年平均湿度

```
mean_temp = mean_for_mth('BUDAPEST_humidity')
plt.figure(figsize=(12,6))
bar = plt.bar(x = months, height = mean_temp, width = 0.8, color=['thistle','mediumaquamarine',                'orange'])
plt.xticks(rotation = 45)
plt.xlabel('MONTHS')
plt.ylabel('HUMIDITY')
plt.title('布达佩斯(匈牙利首都)的年平均湿度')
plt.bar_label(bar)
plt.show()
```

![下载 (18)](image\下载 (18).png)

#### 5**.**各指标的分布次数图

```
fig, axs = plt.subplots(2, 2, figsize=(12,8))
fig.suptitle('各指标的分布次数图')
sns.histplot(data = df_budapest, x ='BUDAPEST_pressure', ax=axs[0,0], color='red', kde=True)
sns.histplot(data = df_budapest, x ='BUDAPEST_humidity', ax=axs[0,1], color='orange', kde=True)
sns.histplot(data = df_budapest, x ='BUDAPEST_temp_mean', ax=axs[1,0], kde=True)
sns.histplot(data = df_budapest, x ='BUDAPEST_global_radiation', ax=axs[1,1], color='green', kde=True)
plt.show()
```

![下载 (19)](image\下载 (19).png)

#### 6**.** 分析天气标签提供一些基本的数据探索和采样策略

针对数据进行了一些数据探索和取样，具体包括以下步骤：

1.统计 天气标签（labels）的数量分布并使用 seaborn 绘制了计数图；

2.计算天气标签（labels）为真和为假的百分比；

3.将 天气标签转换为整数，并使用 seaborn 绘制了标签（labels）与温度的关系；

4.统计了过采样后的天气标签（labels）的数量分布并使用 seaborn 绘制了计数图；

5.绘制了数据集的特征之间的相关性热图。

```
labels_budapest = labels['BUDAPEST_BBQ_weather']
sns.set(style="darkgrid")
plt.figure(figsize=(12,6))
sns.countplot(x = labels_budapest).set(title='Labels for BUDAPEST')

true_val = len(labels_budapest[labels_budapest == True])
false_val = len(labels_budapest[labels_budapest == False])
print('Precent of True values: {0:.2f}%'.format(true_val/(true_val+false_val)*100))
print('Precent of False values: {0:.2f}%'.format(false_val/(true_val+false_val)*100))


labels_budapest = labels_budapest.astype(int)
plt.figure(figsize=(12,6))
sns.set(style="darkgrid")
sns.boxplot(y = df_budapest['BUDAPEST_temp_mean'], x = labels_budapest).set(title='Relation between the temperature and the bbq weather')

labels_budapest = labels_budapest.astype(int)
df_budapest = df_budapest.drop(['DATE'],axis=1)
from imblearn.over_sampling import RandomOverSampler
oversample = RandomOverSampler()
ovrspl_X, ovrspl_y  = oversample.fit_resample(df_budapest, labels_budapest)

labels_budapest = labels['BUDAPEST_BBQ_weather']
sns.set(style="darkgrid")
plt.figure(figsize=(12,6))
sns.countplot(x = ovrspl_y).set(title='Oversampled Labels for BUDAPEST')

true_val = len(ovrspl_y[ovrspl_y == 1])
false_val = len(ovrspl_y[ovrspl_y == 0])
print('Precent of True values: {0:.1f}%'.format(true_val/(true_val+false_val)*100))
print('Precent of False values: {0:.1f}%'.format(false_val/(true_val+false_val)*100))

plt.figure(figsize=(12,6))
sns.set(style="darkgrid")
sns.heatmap(df_budapest.corr(),annot=True,cmap='coolwarm').set(title='Correlation between features')
plt.show()
```



![下载 (20)](image\下载 (20).png)

![下载 (21)](image\下载 (21).png)

![下载 (22)](image\下载 (22).png)

![下载 (23)](image\下载 (23).png)

#### 7**.** 基于SVC模型的机器学习预测天气分类

```
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
norm_X = scaler.fit_transform(ovrspl_X)
norm_X = pd.DataFrame(norm_X, columns=df_budapest.columns)
norm_X.describe()

from sklearn.model_selection import train_test_split
# Splitting dataset on training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(ovrspl_X,ovrspl_y, test_size = 0.3, random_state = 42)
print('Training set: ' + str(len(X_train)))
print('Testing set: ' + str(len(X_test)))

from sklearn.svm import SVC

model = SVC(verbose=True, kernel = 'linear', random_state = 0)
model.fit(X_train,y_train)

y_predict = model.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print('Classification report--------------------------------')
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict), annot=True, fmt='g').set(title='Confusion Matrix')

print('Model accuracy is: {0:.2f}%'.format(accuracy_score(y_test, y_predict)*100))
```

 运行结果：

```
Training set: 3235
Testing set: 1387
[LibSVM]................................*...............................*................................................*
optimization finished, #iter = 110434
obj = -581.485644, rho = -4.010761
nSV = 647, nBSV = 635
Total nSV = 647
Classification report--------------------------------
              precision    recall  f1-score   support

           0       0.98      0.91      0.95       712
           1       0.92      0.98      0.95       675

    accuracy                           0.95      1387
   macro avg       0.95      0.95      0.95      1387
weighted avg       0.95      0.95      0.95      1387

Model accuracy is: 94.66%
```

## 6-糖尿病疾病的预测与分析(随机森林算法）

通过分析糖尿病的高危因素，利用随机森林的集成学习特性，建立预测模型，旨在早期发现疾病并采取干预措施。文章详细讲解了随机森林的工作原理，并提供了预测糖尿病的代码示例，整体测试准确率达到0.83

### 一、糖尿病预测项目背景

由于人们的生活方式、环境和基因等多种因素的影响，全球范围内糖尿病患病率不断上升。糖尿病对健康的危害性很大，包括心血管疾病、肾脏疾病、失明等，给患者带来了极大的身体和心理负担。针对这一问题，进行糖尿病预测可以在早期发现疾病，并采取有效的干预措施，以降低患病风险和减轻疾病对患者的危害。糖尿病预测项目可以利用机器学习算法，通过分析患者的生理特征和历史病史等数据，建立糖尿病预测模型，实现对糖尿病的早期预测和筛查。

该项目可以应用到医疗领域，帮助医生更加准确地诊断糖尿病，提高诊疗效率；同时也可以应用到公共卫生领域，对糖尿病的流行趋势进行分析，并制定相应的预防和控制策略，为大众提供更加全面的健康保障。

### 二、糖尿病发病的高危因素

糖尿病发病与遗传及生活方式等多种因素相关。潜在风险因子包括但不限于以下几个方面：

**体重过重或肥胖：**肥胖是糖尿病的主要风险因素之一。

**年龄：**随着年龄的增长，糖尿病的发病率也有所升高。

**高血压：**有高血压病史的人群更容易患上糖尿病。

**不良饮食习惯：**进食高糖和脂肪食品，缺乏膳食纤维等均会增加患糖尿病的风险。

**缺乏运动：**缺乏身体锻炼的人群更容易患上糖尿病。

**非酒精性脂肪肝：**该状态被认为是糖尿病的独立危险因素。

**家族史：**有家庭糖尿病史的人群更容易患上糖尿病。

除此之外，还有生活环境等诸多因素对糖尿病的发生和发展有影响。

健康的生活方式、适当的体重控制、规律的运动以及避免吸烟等习惯可以减少糖尿病的发病风险。对具有潜在危险因素的人群，及时进行早期筛查和干预，有助于预防和控制糖尿病的发生。

### 三、随机森林算法

随机森林是一种集成学习算法，它将多个决策树结合起来进行预测，具有高准确率、鲁棒性强、不易过拟合等优点，被广泛应用于分类、回归和特征选择等领域。

**随机森林的算法步骤：**

随机森林由多个决策树组成，每个决策树都是基于训练样本随机抽样得到的子集建立而成。这样可以保证每个决策树之间的差异性，避免了过拟合现象的出现。

对于每个决策树，它是通过对训练数据进行递归划分得到的。在每次划分时，随机从所有特征中选择一部分特征进行评估，选取最优的特征进行分裂。这样可以有效减小每个决策树的方差，进而提升整个模型的泛化能力。

在测试样本上进行预测时，每个决策树会输出一个类别或数值。对于分类问题，采用投票机制确定最终结果；对于回归问题，采用平均值的方法计算最终结果。

随机森林还可以用于特征选择。通过对每个特征的重要性进行评估，我们可以得到哪些特征对预测结果影响最大，从而可以进行特征筛选和优化。

![下载 (24)](image\下载 (24).png)

**针对每个决策树，参数的选择和更新过程通常如下：**

首先选择一个特征作为当前节点的划分特征，以该特征的取值作为分割点，把数据集分成两类。

然后计算划分后的**信息增益或基尼指数**等指标，以评估当前节点划分的好坏。

如果当前节点划分后的**信息增益或基尼指数**高于某个预设阈值，则继续对两个子节点进行上述步骤，直到满足停止条件，例如达到最大深度、样本数量少于某个值等。

在生成完整棵树之后，我们可以使用剪枝等策略来对树进行优化和修剪，从而提高其泛化能力。

对于决策树的参数更新，通常采用交叉验证等方法进行模型选择和调参，以获得更好的训练效果。

需要注意的是，不同的决策树算法可能会有不同的参数更新方式和策略。例如，CART算法使用基尼指数选择最优划分点，而ID3和C4.5算法使用信息增益作为评估指标。因此，在实际应用中需要根据具体算法和问题来选择合适的参数和更新策略。

### **四、利用随机森林算法预测糖尿病代码**

数据代码下载地址：链接：https://pan.baidu.com/s/1amVR1A91OKJ3wSuRN5HWlw?pwd=bknl 
提取码：bknl

```
import pandas as pd
import numpy as np
from PIL import Image


diab_df= pd.read_csv('diabetes.csv')

print(diab_df.describe())

from sklearn.model_selection import train_test_split
x = diab_df.drop(['Outcome'],axis=1)
y = diab_df['Outcome']

from sklearn.preprocessing import StandardScaler

sc= StandardScaler()
x_scaled= sc.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=0)
from sklearn.metrics import confusion_matrix, accuracy_score

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(criterion = "gini",
                                       min_samples_leaf = 5,
                                       min_samples_split = 10,
                                       n_estimators=100,
                                       max_features='auto',
                                       oob_score=True,
                                       random_state=1,
                                       n_jobs=-1)

random_forest.fit(x_train, y_train)
y_pred = random_forest.predict(x_test)

confmat1 = confusion_matrix(y_pred, y_test)
from sklearn import metrics
cm=metrics.ConfusionMatrixDisplay(confusion_matrix=metrics.confusion_matrix(y_pred,y_test,labels=random_forest.classes_),
                              display_labels=random_forest.classes_)
cm.plot(cmap="magma").figure_.savefig("confusion_matrix.png")

Image.open("confusion_matrix.png").show()

print(accuracy_score(y_pred, y_test))
```

随机森林分类器RandomForestClassifier各个参数的含义及作用：

`criterion`：衡量分裂质量的度量标准。可选的值有"gini"和"entropy"，默认为"gini"。它们都是计算每个节点的不纯度，并使用最小化不纯度来确定最佳的分裂方式。

`min_samples_leaf`：叶子节点所需的最小样本数。如果某个叶子节点的样本数小于该值，则不会继续进行分割，默认为1。

`min_samples_split`：内部节点再划分所需的最小样本数。如果某个内部节点的样本数小于该值，则不会继续进行分割，默认为2。

`n_estimators`：决策树的数量。即随机森林中包含多少棵决策树，默认值为100。

`max_features`：寻找最佳分割时考虑的特征数量。可以是整数、浮点数或字符串。如果是整数，则表示考虑的特征数量；如果是浮点数，则表示特征数量的百分比；如果是"auto"或"sqrt"，则表示考虑的特征数量约为总特征数量的平方根；如果是"log2"，则表示考虑的特征数量约为总特征数量的以2为底的对数。默认为"auto"。

`oob_score`：是否使用袋外样本来估计泛化误差。默认为False。

`random_state`：随机种子。控制随机性，确保结果可重复。如果设置为1，则每次运行时生成的随机数相同。

`n_jobs`：并行度。在拟合和预测期间使用的CPU核心数量。-1表示使用所有可用核心。默认值为1。

**运行结果：**

![下载 (25)](image\下载 (25).png)

最后在测试集的整体表现准确率为：0.837662

## 7-基于机器学习算法预测相亲成功率随机森林

如何使用随机森林算法对相亲成功进行预测，通过分析男女双方的房子、车子、长相、家庭条件、父母情况、生活习惯、学历、性格、兴趣等因素，来预测相亲是否成功。

### 一、设定条件的合并

相亲过程，如果是设定单个条件的话是比较好找，但是将几个条件综合的话概率就低了。真正符合你设定的所有条件的人其实很少了。之前有过这样的一个统计：

某个城市要找一个170以上，本科学历，月薪5000以上，有房，中等长相，无不良嗜好的男生只剩150了。某城市主城区内常住人口300万，男性占一半的话，剩150万，其中年龄段在25-35之间的占20%， 还剩30万，其中还有排除已经结婚，有女朋友的，剩10万。中国南方男性平均身高168.5，170以上的占40%多，还剩4万，其中有独立房的占10%，剩4000，本科及以上占50%,剩2000，月薪5000及以上占50%，剩1000,中等长相的占50%，剩500，父母有体面工作的，健康，家庭和睦，有退休金占60%，剩300，无不良嗜好，不抽烟，不喝酒，顾家的再占一半，剩150。这样看来就是这样简简单单条件的都这么少了。

### 二、相亲数据集的描述

**1.本文使用的数据集为CSV格式，包含以下字段：**

房子（house）：是否拥有房子（0：无，1：有）

车子（car）：是否拥有车子（0：无，1：有）

长相（appearance）：长相评分（1-10）

家庭条件（family_status）：家庭经济条件评分（1-10）

父母情况（parents_status）：父母健康状况评分（1-10）

生活习惯（lifestyle）：生活习惯评分（1-10）

学历（education）：学历等级（1：小学，2：初中，3：高中，4：大学，5：硕士，6：博士）

性格（personality）：性格评分（1-10）

兴趣（interests）：兴趣相似度评分（1-10）

相亲成功（success）：是否相亲成功（0：失败，1：成功）

**2.数据样例：**

数据集为男生条件的数据，在那是有这些条件下的时候相亲成功的情况：

```
house,car,appearance,family_status,parents_status,lifestyle,education,personality,interests,success
1,0,7,6,8,5,4,7,6,1
0,1,5,4,7,6,3,6,5,0
1,1,8,7,9,7,5,8,7,1
0,0,4,3,6,4,2,5,4,0
1,0,6,5,7,5,4,6,5,1
1,1,6,8,5,8,6,6,5,0
0,1,5,4,7,6,3,6,5,0
1,0,7,6,8,5,4,7,6,1
0,0,4,3,6,4,2,5,4,0
1,1,8,7,9,7,5,8,7,1
0,1,5,4,7,6,3,6,5,0
1,0,7,6,8,5,4,7,6,1
1,1,5,8,9,8,6,9,8,1
0,0,9,3,6,4,2,9,4,1
1,0,6,5,7,5,4,6,5,1
0,1,5,4,7,6,3,6,5,0
1,1,8,7,9,7,5,8,7,1
1,0,7,6,8,5,4,7,6,1
0,0,4,3,6,4,2,5,4,0
1,1,9,8,9,8,6,9,8,1
1,0,6,5,8,5,4,6,5,1
0,1,5,4,3,6,3,6,5,0
1,1,6,7,9,7,7,8,7,1
1,0,7,6,8,5,4,7,6,1
0,0,9,4,6,7,7,9,8,1
1,0,9,8,6,7,6,9,7,1
1,1,6,7,9,4,5,5,4,0
1,1,6,7,8,3,5,4,5,0
0,1,7,5,8,4,5,4,6,0
0,0,7,3,6,4,7,5,4,0
```

**2.数据预处理**

在进行预测之前，需要对数据进行预处理，包括缺失值处理、数据归一化等。

**3. 随机森林算法简介**

随机森林是一种集成学习算法，它将多个决策树组合在一起进行分类或回归预测。每个决策树都是基于随机的样本和随机的特征选择建立的，这样可以降低模型的方差，提高模型的泛化能力。 在随机森林中，有两种方法可以增加模型的随机性。一种是随机采样，即在训练过程中随机选择一部分样本进行训练，这样可以降低模型的过拟合程度。另一种是随机特征选择，即在每个节点上，随机选择一部分特征进行划分，这样可以避免某些特征对模型的过度依赖。 随机森林的预测结果是基于所有决策树的预测结果的综合得出的，通常是取所有决策树预测结果的平均值（回归问题）或多数投票（分类问题）。随机森林的优点包括易于实现，不需要对数据进行特征缩放，可以处理高维数据和大量样本，同时还可以估计每个特征的重要性。

![下载 (26)](image\下载 (26).png)

### 三、代码实现

以下是使用Python和scikit-learn库实现的随机森林算法预测相亲成功的完整代码：

```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 读取数据
data = pd.read_csv('data.csv')

# 数据预处理
data.fillna(data.mean(), inplace=True)

# 划分训练集和测试集
X = data.drop('success', axis=1)
y = data['success']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练随机森林模型
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 预测
y_pred = rf.predict(X_test)

# 评估
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

运行结果：

```
Accuracy: 1.0
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         2
           1       1.00      1.00      1.00         4

    accuracy                           1.00         6
   macro avg       1.00      1.00      1.00         6
weighted avg       1.00      1.00      1.00         6

Sample prediction: Failure

Process finished with exit code 0
```

预测数据1：

```
# 输入样例数据进行预测
sample_data = pd.DataFrame({'house': [0],
                            'car': [0],
                            'appearance': [5],
                            'family_status': [5],
                            'parents_status': [5],
                            'lifestyle': [4],
                            'education': [4],
                            'personality': [7],
                            'interests': [4]})

sample_pred = rf.predict(sample_data)
print("Sample prediction:", "Success" if sample_pred[0] == 1 else "Failure")
```

运行结果：

```
Sample prediction: Failure
```

预测数据2：

```
# 输入样例数据进行预测
sample_data2= pd.DataFrame({'house': [0],
                            'car': [0],
                            'appearance': [9],
                            'family_status': [5],
                            'parents_status': [5],
                            'lifestyle': [9],
                            'education': [5],
                            'personality': [8],
                            'interests': [8]})

sample_pred2 = rf.predict(sample_data2)
print("Sample prediction:", "Success" if sample_pred2[0] == 1 else "Failure")
```

运行结果：

```
Sample prediction: Failure
```

我们可以看到男生房车，生活习惯，兴趣爱好比较不如意的条件下，就比较难成功。

当男生无房车，但是生活习惯，兴趣爱好与女生很匹配，学历优势，能力优势，也是可以成功的，所以重要的还是看自己的后台能力与性格，能力决定你的幸福。

## 8-基于XGBoost和LSTM的台风强度预测模型训练与应用

![下载 (27)](image\下载 (27).png)

### 1. 引言

随着气候变化的影响，台风对人们的生命、财产以及社会经济带来的影响越来越大。因此，预测台风强度成为了一个非常重要的问题。本文基于XGBoost和LSTM算法，提出了一种台风强度预测模型，通过对历史台风强度数据的分析，可以对未来的台风强度进行预测，为国家和地区的防灾减灾工作提供帮助。

### 2. 台风强度预测模型项目介绍

在本项目中，我们使用了XGBoost和LSTM两种算法进行台风强度预测。其中，XGBoost是一种集成学习算法，可以通过多个分类器的加权组合来提高预测准确率。LSTM是一种递归神经网络，可以处理序列数据，适用于对时间序列数据进行预测。通过比较这两种算法的预测效果，可以得出哪种算法更适合于台风强度预测的结论。

### 3. XGBoost原理

#### 3.1 XGBoost算法简介

XGBoost（Extreme Gradient Boosting）是一种集成学习算法，可以通过多个分类器的加权组合来提高预测准确率。XGBoost使用了一种叫做GBDT（Gradient Boosting Decision Tree）的算法，它是一种决策树的集成算法，可以通过不断迭代来提高模型的准确率。在每一次迭代中，GBDT会根据之前预测结果的误差来调整下一棵决策树的参数，从而减小误差，提高预测准确率。

其数学原理可以分为两个部分：加法建模和损失函数优化。

加法建模（Additive Modeling）：

XGBoost采用的是加法建模的思想，即通过将多个弱模型迭代地叠加在一起来构建一个强大的整体模型。每次迭代都在现有模型的基础上添加一个新的模型，使得整体模型逐步逼近真实结果。最终的预测结果是所有弱模型的加权和。

损失函数优化：

XGBoost通过优化损失函数来训练模型参数，其中使用了泰勒展开式来逼近损失函数。具体来说，XGBoost采用了二阶泰勒展开式，将损失函数在当前模型参数处进行二阶近似展开，并通过最小化近似损失函数的方式来更新模型参数。这种方法能够更准确地拟合损失函数，从而提高模型性能。

![下载 (28)](image\下载 (28).png)

#### 3.2 XGBoost的主要特点

XGBoost的主要特点如下：

- 高效：XGBoost在分布式计算中具有优异的效率和可扩展性，可以处理大规模数据。
- 准确：XGBoost通过集成多个分类器的结果来提高准确率，同时也可以避免过拟合。
- 灵活：XGBoost支持自定义损失函数和评估指标，可以根据具体问题进行优化。
- 可解释性：XGBoost能够对模型的特征重要性进行评估，帮助用户理解模型的预测过程。

#### 3.3 XGBoost的优点和缺点

XGBoost的优点和缺点如下：

优点：

- 高效：XGBoost在处理大规模数据时具有优异的效率和可扩展性。
- 准确：XGBoost通过集成多个分类器的结果来提高准确率，同时也可以避免过拟合。
- 灵活：XGBoost支持自定义损失函数和评估指标，可以根据具体问题进行优化。
- 可解释性：XGBoost能够对模型的特征重要性进行评估，帮助用户理解模型的预测过程。

缺点：

- 参数调整：XGBoost有很多参数需要调整，需要对算法有深入的了解才能得到最佳的参数设置。
- 训练时间长：在处理大规模数据时，XGBoost需要较长的训练时间。

### 4. LSTM原理

#### 4.1 LSTM算法简介

LSTM（Long Short-Term Memory）是一种递归神经网络，可以处理序列数据，适用于对时间序列数据进行预测。LSTM通过引入门控机制来解决传统递归神经网络的梯度消失问题，可以处理长序列数据，并可以学习长期依赖关系。

![下载 (29)](image\下载 (29).png)

#### 4.2 LSTM的主要特点

LSTM的主要特点如下：

- 长短期记忆：LSTM通过引入门控机制，可以记忆较长时间的信息，有助于处理长序列数据。
- 梯度消失问题：LSTM引入门控机制，可以避免传统递归神经网络的梯度消失问题。
- 可解释性：LSTM可以对模型的预测过程进行解释，有助于理解模型的预测结果。

#### 4.3 LSTM的优点和缺点

LSTM的优点和缺点如下：

优点：

- 可处理长序列数据：LSTM通过引入门控机制，可以记忆较长时间的信息，有助于处理长序列数据。
- 避免梯度消失问题：LSTM引入门控机制，可以避免传统递归神经网络的梯度消失问题。
- 可解释性：LSTM可以对模型的预测过程进行解释，有助于理解模型的预测结果。

缺点：

- 训练时间长：LSTM在处理大规模数据时需要较长的训练时间。
- 参数调整：LSTM有很多参数需要调整，需要对算法有深入的了解才能得到最佳的参数设置。

### 5. 台风强度预测数据样例

本项目使用台风强度的csv数据，数据如下所示：

```
Date,Lat,Lon,Wind,Pres,Center
202006010000,20,120,25,1008,0
202006010600,20.2,120.3,30,1004,0
202006011200,20.5,120.6,35,1000,0
202006011800,21,121,40,996,0
202006020000,21.5,121.5,45,992,0
202006020600,22,122,50,990,0
202006021200,22.5,123,55,988,0
202006021800,23,124,60,984,0
202006030000,23.5,125,65,980,0
202006030600,24,126,70,975,0
202006031200,25,128,75,965,0
202006031800,25,129,80,950,0
```

该数据包含了每个台风的日期、时间、经纬度、风速、气压等信息。

### 6. 数据加载与预处理

#### 6.1 数据加载

我们使用Pandas库来加载csv格式的数据。代码如下：

```python
import pandas as pd

# 加载数据
data = pd.read_csv('data.csv')
```

#### 6.2 数据预处理

在预测之前，我们需要对数据进行预处理。具体来说，我们需要将日期和时间列合并为一个时间戳，同时对经纬度进行归一化处理。代码如下：

```python
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# 加载数据
data = pd.read_csv('data.csv')

# 将日期和时间列合并为一个时间戳
data['Timestamp'] = pd.to_datetime(data['Date'].astype(str) + data['Time'].astype(str), format='%Y%m%d%H%M')

# 对经纬度进行归一化处理
scaler = MinMaxScaler()
data[['Lat', 'Lon']] = scaler.fit_transform(data[['Lat', 'Lon']])
```

### 7. XGBoost模型训练与预测

#### 7.1 XGBoost模型训练

我们使用XGBoost库来训练模型。代码如下：

```python
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(data[['Lat', 'Lon', 'Pres']], data['Wind'], test_size=0.2, random_state=42)

# 训练XGBoost模型
xgb = XGBRegressor(n_estimators=1000, learning_rate=0.01, random_state=42)
xgb.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = xgb.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f'MSE: {mse}')
```

#### 7.2 XGBoost模型预测

我们使用训练好的XGBoost模型来进行预测。代码如下：

```python
# 对新数据进行预测
new_data = pd.DataFrame({'Lat': [0.5], 'Lon': [0.5], 'Pres': [1000]})
new_data_pred = xgb.predict(new_data)
print(f'New data prediction: {new_data_pred}')
```

### 8. LSTM模型训练与预测

#### 8.1 LSTM模型训练

我们使用pytorch框架来训练LSTM模型。代码如下：

```python
import torch
import torch.nn as nn
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# 加载数据
data = pd.read_csv('data.csv')

# 提取特征和目标值
features = data[['Lat', 'Lon', 'Pres', 'Center']].values
target = data[['Wind']].values

# 数据归一化
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(features)
scaled_target = scaler.fit_transform(target)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(scaled_features, scaled_target,
                                                    test_size=0.2, random_state=42)

# 转换为PyTorch张量
X_train_tensor = torch.Tensor(X_train).unsqueeze(1)
y_train_tensor = torch.Tensor(y_train).unsqueeze(1)
X_test_tensor = torch.Tensor(X_test).unsqueeze(1)
y_test_tensor = torch.Tensor(y_test).unsqueeze(1)


# 定义LSTM模型
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out


# 定义模型参数
input_size = 4  # 输入特征维度
hidden_size = 32  # 隐藏层维度
num_layers = 2  # LSTM层数
output_size = 1  # 输出维度

# 实例化模型
model = LSTM(input_size, hidden_size, num_layers, output_size)

# 设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    outputs = model(X_train_tensor.to(device))
    loss = criterion(outputs, y_train_tensor.to(device))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item():.6f}')
```

#### 8.2 LSTM模型预测

我们使用训练好的LSTM模型来进行预测。代码如下：

```python
# 使用模型进行预测
new_data = [[24.5, 127.5, 975, 0]]  # 待预测的新数据

model.eval()
with torch.no_grad():
    prediction = model(torch.Tensor(new_data).unsqueeze(0).to(device))
    unscaled_prediction = scaler.inverse_transform(prediction.cpu().numpy())
    print(f'Wind Speed Prediction for New Data: {unscaled_prediction[0][0]:.2f}')
```

运行结果：

```python
Epoch: 10/100, Loss: 0.147660
Epoch: 20/100, Loss: 0.122539
Epoch: 30/100, Loss: 0.099956
Epoch: 40/100, Loss: 0.082207
Epoch: 50/100, Loss: 0.073938
Epoch: 60/100, Loss: 0.074107
Epoch: 70/100, Loss: 0.073730
Epoch: 80/100, Loss: 0.073326
Epoch: 90/100, Loss: 0.073319
Epoch: 100/100, Loss: 0.073234
Wind Speed Prediction for New Data: 67.60
```

### 9. 模型评估与对比分析

我们通过计算均方误差来评估模型预测的准确性，并对比分析XGBoost模型和LSTM模型的预测表现。

在本例中，LSTM模型的均方误差为0.073234。从均方误差的结果来看，LSTM模型的性能优于XGBoost模型。

然而，需要注意的是，评估模型的性能仅仅使用均方误差是不够的，因为它并不能完全反映模型的表现。在实际应用中，我们需要综合考虑多种评估指标，如准确率、召回率、F1值等。

此外，在选择模型时，还需要考虑数据的特点和应用场景。例如，如果数据具有时序性质，那么LSTM模型可能更适合；如果数据具有稀疏性质，那么XGBoost模型可能更适合。

## 10-基于spark大数据技术与机器学习的结合应用实战

### 1.大数据技术介绍

大数据技术是指为了处理和分析大规模数据而发展的一系列技术和工具。随着互联网、物联网和各种传感器技术的发展，我们能够采集到越来越多的数据。这些数据通常规模庞大、复杂多样，并且具有高速增长的特点。大数据技术致力于解决如何高效地存储、处理和分析这些海量数据的问题。

以下是几种常见的大数据技术：

1.分布式存储系统：大规模数据的存储需要使用分布式存储系统，以提供高容量、高可靠性和高扩展性。例如，Hadoop分布式文件系统（HDFS）和Apache Cassandra等分布式数据库系统。

2.分布式计算框架：大数据处理过程中需要进行分布式计算，以实现对数据的高效处理和分析。Hadoop MapReduce是最早的分布式计算框架，而Apache Spark则是目前流行的快速、通用的大数据处理框架。

3.数据管理和治理工具：大规模数据管理和治理是一个复杂的任务。数据管理工具帮助组织和管理数据，包括数据的采集、清洗、转换和整合等。数据治理工具则关注数据质量、安全性和合规性等方面。

4.数据仓库和数据湖：数据仓库是一种用于存储和管理结构化数据的系统，提供了灵活的查询和分析功能。数据湖是一个集中存储各种类型数据的汇聚地，可以在需要时进行处理和分析。

5.数据挖掘和机器学习：大数据技术可以用于数据挖掘和机器学习，帮助从大规模数据中发现有价值的信息和模式。常见的工具和算法包括Apache Hadoop、Apache Spark的机器学习库（MLlib）、TensorFlow等。

6.数据可视化和报告工具：数据可视化工具帮助将数据转化为可视化图表和仪表板，使数据更易于理解和分析。报告工具则可以生成数据分析结果的报告和展示。

大数据技术的应用非常广泛，涵盖了各个行业和领域。例如，在金融领域，大数据技术可以用于风险管理和欺诈检测；在医疗领域，可以用于医学图像分析和疾病预测；在社交媒体领域，可以用于用户行为分析和个性化推荐等。

### 2.Spark的特点

快速性能：Spark使用内存计算（in-memory computing）技术，将数据存储在集群的内存中，从而加速数据处理过程。此外，Spark还利用了RDD（弹性分布式数据集）这一抽象概念，通过内存的数据共享和数据分片的方式，实现了高效的并行计算。

1.多种数据处理支持：Spark支持多种数据处理模型，包括批处理、交互式查询、流处理和机器学习等。你可以使用Spark的API（如DataFrame和SQL）来进行数据处理和分析，还可以结合其他库（如MLlib、GraphX）进行机器学习和图处理。

2.易于使用：Spark提供了易于使用的API，包括Java、Scala、Python和R等编程语言的接口。这使得开发者可以使用自己熟悉的语言来编写Spark应用程序和脚本。

3.可扩展性：Spark可以在各种规模的集群上运行，从小规模的笔记本电脑到大规模的集群。它可以与其他大数据工具和框架（如Hadoop、Hive、HBase等）无缝集成，提供了灵活可扩展的大数据处理解决方案。

![下载 (30)](image\下载 (30).png)

### 3.为什么要用Spark

大数据处理通常涉及到海量数据的存储、处理和分析，而Spark作为一种快速、通用的大数据处理框架，有以下几个重要原因使其成为流行的选择：

1.高性能：Spark使用内存计算技术，在集群的内存中缓存数据，从而加速了数据处理过程。相比于传统的磁盘读写方式，内存计算可以显著提高数据处理速度。此外，Spark还利用RDD这一抽象概念实现了数据共享和并行计算，进一步提高了性能。

2.多种数据处理模型支持：Spark支持批处理、交互式查询、流处理和机器学习等多种数据处理模型。这意味着在同一个框架下可以进行各种类型的大数据处理任务，不再需要使用不同的工具和系统，从而简化了开发和部署的复杂性。

3.易用性和灵活性：Spark提供了易于使用的API，包括Java、Scala、Python和R等编程语言的接口。这使得开发者可以使用自己熟悉的语言来编写Spark应用程序和脚本。同时，Spark与其他大数据工具和框架（如Hadoop和Hive）无缝集成，可以灵活地构建大数据处理流程。

### 4.Spark与Pandas的区别

使用Spark读取CSV文件和使用pandas的pd.read_csv读取CSV文件有哪些区别呢：

1.分布式计算：Spark是一个分布式计算框架，可以处理大规模的数据集。它能够并行处理数据，利用集群中的多个节点进行计算，从而提高处理速度。相比之下，pandas是在单台机器上运行的，对于大规模数据集可能会受到内存限制。

2.数据处理能力：Spark提供了丰富的数据处理功能，包括数据清洗、转换、特征工程等。通过Spark的DataFrame API，你可以使用SQL-like的语法执行各种操作，如过滤、聚合、排序等。相对而言，pandas也提供了类似的功能，但Spark的数据处理能力更强大、更灵活。

3.多语言支持：Spark支持多种编程语言，包括Scala、Java、Python和R等。这意味着你可以使用你最熟悉的编程语言进行数据处理和机器学习。而pandas主要使用Python编写，只支持Python语言。

4.扩展性：Spark可与其他大数据工具和框架（如Hadoop、Hive、HBase等）无缝集成，为构建端到端的大数据处理和机器学习流水线提供了便利。此外，Spark还提供了丰富的机器学习库（如MLlib）和图处理库（如GraphX），方便进行复杂的机器学习任务。

5.数据分片存储：Spark将数据划分为多个分片并存储在集群中的不同节点上。这种数据分片存储方式有助于提高数据的并行读取和处理性能。而pandas将整个数据集加载到内存中，对于大型数据集可能导致内存不足或性能下降。

### 5.使用Python和Spark开发大数据应用

在本文中，我们将探讨如何使用Python和Spark开发一个大数据应用。我们将加载一个CSV文件，执行机器学习算法，然后进行数据预测。对于初学者而言，这是一个很好的入门实例，而对于经验丰富的开发者，也可能会发现新的想法和观点。

#### 环境准备

在开始之前，我们需要安装和配置以下工具和库：

Python3
Apache Spark
PySpark
Jupyter Notebook
在本教程中，我们将使用Jupyter Notebook作为我们的开发环境，因为它便于我们展示和解释代码。

#### 加载CSV文件

首先，我们需要加载一个CSV文件。在这个例子中，我们将使用一个简单的数据集，其中包含一些模拟的用户数据。

### 6.基于spark的机器学习训练代码

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Python Spark CSV").getOrCreate()

# 利用inferSchema参数进行类型推断
df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load('GDM.csv')

df.show()

from pyspark.sql.functions import col

# 删除含有空值的行
df = df.dropna()

# 转换数据类型
df = df.withColumn("AGE", col("AGE").cast("integer"))

df.show()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression

# 选择用于预测的特征列
assembler = VectorAssembler(
    inputCols=["GW", "APB", "APA1", "CRE", "CHOL", "UA","ALP", "TG", "GLU"],
    outputCol="features")

# 将数据集转换为适合机器学习的格式
output_data = assembler.transform(df)

# 构建逻辑回归模型
lr = LogisticRegression(featuresCol='features', labelCol='GDM')

# 划分数据集为训练集和测试集
train_data, test_data = output_data.randomSplit([0.7, 0.3])


# 训练模型
lr_model = lr.fit(train_data)

# 测试模型
predictions = lr_model.transform(test_data)

# 展示预测结果
#predictions.show()

from pyspark.ml.evaluation import BinaryClassificationEvaluator

# 构建评估器
evaluator = BinaryClassificationEvaluator()

predictions = predictions.withColumnRenamed("GDM", "label")
# 计算AUC
auc = evaluator.evaluate(predictions)

print(f"AUC: {auc}")
```

本文用Python和Spark创建一个大数据应用的基本步骤。本文它涵盖了创建大数据应用的基本步骤：加载数据，预处理数据，训练模型，测试模型，以及评估模型

## 11-基于K-means算法的文本聚类分析，生成文本聚类后的文件

介绍了文本聚类分析的核心任务，特别是使用K-means算法进行文本数据的自动分类。通过将文本转化为向量并利用K-means进行聚类，可以发现文本数据中的模式。内容涵盖了K-means的工作原理、文本表示方法如词袋模型和TF-IDF，以及聚类分析的优缺点

### 一、引言

文本聚类分析是一种将文本数据进行分类和组织的技术，它通过发现文本之间的相似性和关联性，将相似的文本归为一类。文本聚类在实际应用中具有重要意义，能够帮助我们理解大规模文本数据的结构和内容，从而发现隐藏在其中的信息和模式。

### 二、文本聚类分析的基础知识

文本聚类是指将文本数据集分成若干个不相交的类别，使得同一类内的文本相似度较高，不同类之间的相似度较低。常用的文本聚类算法包括K-means算法和层次聚类算法。K-means算法通过迭代优化，将文本数据划分为K个簇，每个簇具有相似性；层次聚类算法则通过计算不同文本之间的相似度，逐步合并最相似的文本，直到形成一个完整的聚类树。

在文本聚类中，文本表示是一个关键问题。常用的文本表示方法包括词袋模型和TF-IDF。词袋模型将文本表示为一个向量，其中每个维度表示某个特定词汇在文本中的出现次数；TF-IDF则考虑了词汇的频率和在整个文本集中的重要性。

K-means算法的数学原理可以通过以下公式表示：

给定一个包含n个样本的数据集X = { x 1 , x 2 , . . . , x n } X=\{x_1, x_2, ..., x_n\}X={x1,x2,...,xn}，其中每个样本x i x_ixi是一个d维向量( x i 1 , x i 2 , . . . , x i d ) (x_{i1}, x_{i2}, ..., x_{id})(xi1,xi2,...,xid)。K-means算法旨在将这些样本分为K个簇，其中每个样本属于一个且仅属于一个簇。

首先，我们需要选择K个初始聚类中心μ = { μ 1 , μ 2 , . . . , μ K } \mu=\{\mu_1, \mu_2, ..., \mu_K\}μ={μ1,μ2,...,μK}，其中每个聚类中心是一个d维向量。

然后，算法的迭代过程如下：

1. 对于每个样本x i x_ixi，计算其与各个聚类中心的距离（通常使用欧氏距离或其他距离度量方法），并将其归类到离它最近的聚类中心所对应的簇。
2. 对于每个簇，计算其所有样本的平均值作为新的聚类中心。
3. 重复步骤1和步骤2，直到满足停止条件（例如，达到最大迭代次数或聚类中心不再发生明显变化）。

K-means算法的优化目标是最小化所有样本与其所属簇中心的距离之和，也就是最小化以下目标函数：

![Snipaste_2025-03-14_11-35-59](image\Snipaste_2025-03-14_11-35-59.png)


其中，r i j r_{ij}rij​表示样本x i x_ixi​归属于簇j jj的指示变量，若x i x_ixi​属于簇j jj则r i j = 1 r_{ij}=1rij​=1，否则r i j = 0 r_{ij}=0rij​=0。

通过迭代的优化过程，K-means算法将不断更新聚类中心，直到找到一组使目标函数J JJ最小化的最终聚类结果。

需要注意的是，K-means算法对于不同的初始聚类中心选择可能收敛到不同的局部最优解。为了克服这个问题，可以使用多次运行或其他启发式方法来改善聚类结果。

### 三、文本聚类分析项目的设计与实施

在进行文本聚类分析项目时，首先需要进行数据收集与预处理。数据可以来自各种渠道，如新闻报道、社交媒体等，但需要进行清洗和去除噪声。接下来是文本特征提取与表示，可以使用词袋模型或TF-IDF方法将文本转化为向量表示。然后需要选择适合的聚类算法，并进行参数调优。最后，对聚类结果进行评估指标和可视化展示，以便更好地理解和解释聚类结果。

### 四、文本聚类分析实现代码案例

这里可以给出一个具体的文本聚类分析实现代码案例，例如使用Python语言和scikit-learn库实现K-means聚类算法，将新闻文本数据集进行聚类。

```python
#coding utf-8
import csv
import jieba
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import os
import re
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 对中文文本进行分词
def tokenize_text(text):
    return " ".join(jieba.cut(text))

# 去除标点符号
def remove_punctuation(text):
    punctuation = '!"#，。、$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    text = re.sub(r'[{}]+'.format(punctuation), '', text)
    return text

# 将分词后的文本转化为tf-idf矩阵
def text_to_tfidf_matrix(texts):
    tokenized_texts = [tokenize_text(remove_punctuation(text)) for text in texts]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(tokenized_texts)
    return tfidf_matrix

# 聚类函数
def cluster_texts(tfidf_matrix, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(tfidf_matrix)
    return kmeans.labels_

# 保存聚类结果到新的CSV文件
def save_clusters_to_csv(filename, texts, labels):
    base_filename, ext = os.path.splitext(filename)
    output_filename = f"{base_filename}_clusters{ext}"
    with open(output_filename, "w", encoding="utf-8", newline="") as csvfile:
        csvwriter = csv.writer(csvfile)
        for text, label in zip(texts, labels):
            csvwriter.writerow([text, label])
    return output_filename

# 输出聚类结果
def print_cluster_result(texts, labels):
    clusters = {}
    for i, label in enumerate(labels):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(texts[i])

    for label, text_list in clusters.items():
        print(f"Cluster {label}:")
        for text in text_list:
            print(f"  {text}")

def text_KMeans(filename,n_clusters):
    df = pd.read_csv(filename, encoding='utf-8')  # 读取csv文件
    texts = df['text'].tolist()  # 提取文本数据为列表格

    print(df.iloc[:, [0, -1]])
    # 将文本转化为tf-idf矩阵
    tfidf_matrix = text_to_tfidf_matrix(texts)
    # 进行聚类
    labels = cluster_texts(tfidf_matrix, n_clusters)
    clusters = []
    for i, label in enumerate(labels):
        clusters.append(label)

    df['cluster'] = clusters

    output = 'data_clustered.csv'
    df.to_csv('data_clustered.csv', index=False, encoding='utf-8')

    return output,labels,tfidf_matrix

def pca_picture(labels,tfidf_matrix):
    # 进行降维操作并将结果保存到DataFrame中
    pca = PCA(n_components=3)
    result = pca.fit_transform(tfidf_matrix.toarray())
    result_df = pd.DataFrame(result, columns=['Component1', 'Component2', 'Component3'])

    # 将聚类结果添加到DataFrame中
    result_df['cluster'] = labels

    # 绘制聚类图形
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    colors = ['red', 'blue', 'green']
    for i in range(3):
        subset = result_df[result_df['cluster'] == i]
        ax.scatter(subset['Component1'], subset['Component2'], subset['Component3'], color=colors[i], s=50)
    ax.set_xlabel("Component 1")
    ax.set_ylabel("Component 2")
    ax.set_zlabel("Component 3")
    plt.show()

if __name__ == "__main__":
    # 加载中文文本
    filename = "data.csv"
    n_clusters =3
    output,labels,tfidf_matrix = text_KMeans(filename, n_clusters)
    pca_picture(labels, tfidf_matrix)
```

运行利用PCA算法生成3D图像：

![下载 (31)](image\下载 (31).png)

### 五、文本聚类分析的优缺点与挑战

文本聚类分析具有以下优点：能够提供洞察力，帮助我们了解文本数据的结构和内容；能够实现自动化聚类，减少人工干预；能够高效处理大规模数据，加快分析速度。

然而，文本聚类也存在一些缺点：由于聚类是基于相似性的，因此对于主观性较强的文本数据，可能会出现分类不准确的情况；聚类算法通常需要标注数据进行训练和调优，这在某些场景下可能难以获取；处理噪声和冗余信息也是一个挑战。

此外，文本聚类还面临一些挑战：高维度问题，即当文本特征维度较高时，聚类结果可能不准确或难以解释；语义相似性问题，由于自然语言的复杂性，文本之间的语义相似性难以捕捉；类别不平衡问题，即不同类别的文本样本数量差异较大，可能影响聚类的效果。

### 六、文本聚类分析的未来发展趋势

未来，文本聚类分析可能朝着以下方向发展：（可以提出一些观点，如结合深度学习方法改进文本特征表示、应用领域的拓展等）

### 七、结论

文本聚类分析是一种重要的技术，能够帮助我们理解和组织大规模文本数据。通过选择合适的算法和特征表示方法，并克服相关挑战，我们可以获得准确和可解释的聚类结果。随着技术的不断进步，文本聚类分析在各个领域都有着广泛的应用前景。

## 12-基于历史数据的台风的预测与分析(2023年第5号台风杜苏芮将登陆福建)

通过历史数据研究1951-2022年期间在亚洲地区生成的所有台风进行分析预测，研究台风路径。

![下载 (32)](image\下载 (32).png)

### 台风形成的原因

台风是由一系列复杂的气象和海洋因素共同作用而形成的。以下是台风形成的主要原因：

温暖的海水温度：台风需要温暖的海水作为能量源。通常，海水温度超过26.5摄氏度才有利于台风的发展。热带和亚热带地区的海洋通常具有足够的温暖水域来支持台风的形成。

湿润的大气条件：台风形成需要湿润的大气条件，即较高的水汽含量。当海水蒸发进入大气中时，形成对流云团。这些云团会随着更多水汽凝结而逐渐增长，并最终形成台风。

强水平风切变的缺乏：水平风切变指的是不同高度水平风速的变化。如果水平风切变较小，台风会更容易形成。较小的风切变能够帮助保持台风的整体结构和旋转。

初始扰动：台风的形成通常需要一个初始扰动或扰动波作为起始。这个扰动可以是热带气旋、气象前线交汇或其他天气系统的相互作用。它们能够提供初始的旋转和上升气流，从而促进台风的发展。

应季的环流条件：台风的形成通常在夏季和初秋最为活跃，因为这时候热带和亚热带地区的环流条件更加有利于气旋的生成和增强。

### 台风预测与分析意义

保护人民生命财产安全：通过预测台风的路径和强度，可以提前向受影响地区发布警报，让人们有时间采取防护措施，最大程度上减少台风可能带来的人员伤亡和财产损失。

气象科学研究：台风是一个复杂的气象系统，研究台风的形成、发展和消散过程，有助于深入理解大气环流和热力学过程。通过对台风的观测和分析，可以提高对天气和气候系统的认识，改进气象预报模型和算法。

为灾害应对提供支持：通过台风预测和分析，可以为应急管理部门提供科学依据，帮助他们制定灾害应对方案和紧急救援计划。同时，也能够提前预测受影响地区的灾情，为救援力量的调度和资源的调配提供有针对性的指导。

气候变化研究：随着全球气候变化的加剧，台风的频率和强度可能会发生变化。通过对台风预测与分析的长期观测和研究，可以探讨气候变化对台风活动的影响，为应对气候变化提供科学参考和决策支持。

### 台风历史数据分析

1951-2022年期间在亚洲地区生成的所有台风数据表：typhoon_data.csv

下载地址：链接：https://pan.baidu.com/s/1FjMSnngby4qUNL2fVCg-Jg?pwd=rxfs
提取码：rxfs

台风索引信息表：typhoon_info.csv
下载地址：链接：链接：https://pan.baidu.com/s/1giaOVpDEx9XHzCkXJs_6XQ?pwd=2a3o
提取码：2a3o

数据字段结构

```python
,International number ID,year,month,day,hour,grade,Latitude of the center,Longitude of the center,Central pressure,Maximum sustained wind speed,Direction of the longest radius of 50kt winds or greater,The longeast radius of 50kt winds or greater,The shortest radius of 50kt winds or greater,Direction of the longest radius of 30kt winds or greater,The longeast radius of 30kt winds or greater,The shortest radius of 30kt winds or greater,Indicator of landfall or passage
0,5101,1951,2,19,6,Tropical Depression,200,1385,1010,,,,,,,, 
1,5101,1951,2,19,12,Tropical Depression,200,1385,1010,,,,,,,, 
2,5101,1951,2,19,18,Tropical Depression,230,1421,1000,,,,,,,, 
3,5101,1951,2,20,0,Tropical Cyclone of TS intensity or higher,250,1460,994,,,,,,,, 
4,5101,1951,2,20,6,Tropical Cyclone of TS intensity or higher,276,1506,994,,,,,,,, 
5,5101,1951,2,20,12,Tropical Cyclone of TS intensity or higher,289,1533,994,,,,,,,, 
6,5101,1951,2,20,18,Tropical Cyclone of TS intensity or higher,313,1575,992,,,,,,,, 
7,5101,1951,2,21,0,Tropical Cyclone of TS intensity or higher,326,1621,990,,,,,,,, 
```

数据加载分析

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import folium
import seaborn as sns

data = pd.read_csv("typhoon_data.csv", index_col=0)
info = pd.read_csv("typhoon_info.csv", index_col=0)

print(data.shape)
data.sample(5)
print(info.shape)
info.sample(5)

print(len(data["International number ID"].unique()))
vc = data["International number ID"].value_counts()

# 数据处理
data[data["year"] < 1977]["grade"].unique()
data[data["year"] >= 1977]["grade"].unique()
data = data[data["year"] >= 1977]


typhoons = data["International number ID"].unique() # Unique IDs of typhoons
info[info["Name"].str.strip() == "MAEMI"]
```

我们知道今年第5号台风“杜苏芮”在菲律宾以东洋面生成，位置大致在东经132.8°北纬13.9°

现在我们查找在132，1300附近位置上生成的台风生成的情况路径：

```python
similar_data = data[((data["Latitude of the center"] < 140) & (data["Latitude of the center"] > 110)) & \
                   ((data["Longitude of the center"] < 1440) & (data["Longitude of the center"] > 1240))]
similar_data.to_csv("similar_data.csv")
```

similar_data.csv中生成找到多个在这附近生成的台风：

![下载 (33)](image\下载 (33).png)

选择其中四个台风分析路径，其中查看1911台风，意思是2019年的11号台风白鹿，生成路径分析图：

轨迹图：

```python
maemi_data = data.copy()[data["International number ID"]==1911]

maemi_data["Elapsed Hour"] = (maemi_data["day"]-4)*24 + maemi_data["hour"]
maemi_data.head()

maemi_data = maemi_data.drop(["International number ID", "year", "month", "day", "hour"], axis=1)
maemi_data.head()

maemi_data["Longitude of the center"] /= 10
maemi_data["Latitude of the center"] /= 10

fig = plt.figure(figsize=(3,3))
plt.xlim(120, 170)
plt.ylim(5, 55)
plt.plot(maemi_data["Longitude of the center"], maemi_data["Latitude of the center"], "red")
plt.show()
```

![下载 (34)](image\下载 (34).png)

散点图

```python
scale = maemi_data["The longeast radius of 30kt winds or greater"].fillna(60)
fig = plt.figure(figsize=(5,5))
plt.xlim(120, 170)
plt.ylim(5, 55)
plt.plot(maemi_data["Longitude of the center"], maemi_data["Latitude of the center"], "black", linewidth=1)
plt.scatter(maemi_data["Longitude of the center"], maemi_data["Latitude of the center"], s=scale/3, c=maemi_data["Maximum sustained wind speed"])
plt.show()
```

![下载 (35)](image\下载 (35).png)

地图风圈

```python
import matplotlib.cm as cm

c = maemi_data.copy()["Maximum sustained wind speed"]
cmap = cm.jet

def rgb_to_hex(rgb):
    return '#%02x%02x%02x' % rgb
m = folium.Map(location=[38.9, 153.2], zoom_start=3, width=600, height=600)
for i in range(len(maemi_data)):
    color = cmap(c.iloc[i]/c.max())
    color = tuple([int(c*255) for c in color[:3]])
    color = rgb_to_hex(color)
    folium.Circle(location=[maemi_data.iloc[i]["Latitude of the center"], maemi_data.iloc[i]["Longitude of the center"]],
                 radius=scale.iloc[i]*1852,  #  nautical mile to meter
                 fill=True,
                 color="black",
                 fill_color=color).add_to(m)
m.save('map.html')  # 保存地图为HTML文件
import webbrowser
webbrowser.open('map.html', new=2)  # 在浏览器中打开地图
```

![下载 (36)](image\下载 (36).png)

台风中心气压和风力变化情况

```python
plt.title("Typhoon's Central Pressure in hPa")
plt.plot(maemi_data["Elapsed Hour"],maemi_data["Central pressure"])
plt.xlabel("Elapsed Hour")
plt.ylabel("Central Pressure")
plt.legend()
plt.show()


plt.title("Typhoon's Wind Speed in knot(kt)")
plt.plot(maemi_data["Elapsed Hour"],maemi_data["Maximum sustained wind speed"])
plt.xlabel("Elapsed Hour")
plt.ylabel("Maximum sustained wind speed")
plt.legend()
plt.show()


sns.heatmap(maemi_data.corr(), annot=True)
plt.show()
```

![下载 (37)](image\下载 (37).png)

![下载 (38)](image\下载 (38).png)

### 总结

台风预测是基于多源数据和分析手段，通过对台风路径、强度和可能影响的预测，为相关地区提供早期警报和决策支持，以最大程度减少台风可能造成的人员伤亡和财产损失。然而，由于台风的复杂性和不确定性，持续的观测和改进预测技术仍然是提高预测准确性的重要方向。

## 14-在日本福岛核电站排放污水的背景下,核电站对人口影响的分析实践

机器学习在分析日本福岛核电站排放污水对人口健康影响的实践，探讨了放射性物质的类型、核反应机制，以及核电站数据加载与世界分布。同时，文章关注了核污染对健康风险、社会心理影响和经济后果的深入剖析。

### 一、放射性物质

放射性物质 存在着三种主要的射线类型，它们分别是阿尔法射线（α）、贝塔射线（β）和伽马射线（γ）：
1.阿尔法射线（α \alpha*α*射线）：阿尔法射线是由氦原子核组成的带电粒子束。由于它们包含两个质子和两个中子，因此具有正电荷。阿尔法射线的穿透能力较弱，一般只能穿透数厘米的空气或者几个微米的固体，因此阿尔法射线通常不能通过人体或纸张等薄材料。然而，如果被内部摄入或吸入，则可能对人体造成较大的伤害。

2.贝塔射线（β \beta*β*射线）：贝塔射线是由带电的高速电子或正电子组成的粒子束。电子射线称为β − \beta^-*β*−射线，而正电子射线称为β + \beta^+*β*+射线。贝塔射线比阿尔法射线具有更强的穿透能力，可以穿透空气和一些较薄的固体物质。然而，贝塔射线的穿透能力仍然相对有限，在适当的屏蔽下可以有效地阻挡。

3.伽马射线（γ \gamma*γ*射线）：伽马射线是高能电磁辐射，类似于X射线。与阿尔法射线和贝塔射线不同，伽马射线不携带任何电荷或粒子，因此不受电场或磁场的影响。伽马射线具有很强的穿透能力，可以穿透大部分常见物质，包括人体组织。为了有效屏蔽伽马射线，通常需要使用较厚的铅、混凝土或其他密度较高的材料。

### 二、三种射线的核反应

以下是三种射线的典型核反应方程式的示例：

1.阿尔法射线 (α \alpha*α*) 反应方程：
Z A X → Z − 2 A − 4 Y + 2 4 α \begin{equation} _{Z}^{A}X \rightarrow _{Z-2}^{A-4}Y + _{2}^{4}\alpha \end{equation}*Z**A*​*X*→*Z*−2*A*−4​*Y*+24​*α*​​

这里 X X*X* 代表起始元素，Y Y*Y* 代表产生的元素，Z A _{Z}^{A}*Z**A* 表示原子序数为 Z Z*Z*，质量数为 A A*A* 的核。

2.贝塔射线 (β \beta*β*) 反应方程：
Z A X → Z + 1 A Y + e − + ν e ˉ \begin{equation} _{Z}^{A}X \rightarrow _{Z+1}^{A}Y + e^{-} + \bar{\nu_e} \end{equation}*Z**A*​*X*→*Z*+1*A*​*Y*+*e*−+*ν**e*​ˉ​​​

这里 X X*X* 代表起始元素，Y Y*Y* 代表产生的元素，Z A _{Z}^{A}*Z**A* 表示原子序数为 Z Z*Z*，质量数为 A A*A* 的核。e − e^{-}*e*− 表示负电子（电子），ν e ˉ \bar{\nu_e}*ν**e*ˉ 表示反中微子。

3.伽马射线 (γ \gamma*γ*) 反应方程：
Z A X ∗ → Z A X + γ \begin{equation} _{Z}^{A}X^{*} \rightarrow _{Z}^{A}X + \gamma \end{equation}*Z**A*​*X*∗→*Z**A*​*X*+*γ*​​

这里 X ∗ X^{*}*X*∗ 表示激发态的核，X X*X* 表示基态的核，γ \gamma*γ* 表示伽马射线。

### 三、核电站的数据加载

数据下载地址：链接：https://pan.baidu.com/s/1wz5L2ykpjUNlKs2icTWkNg?pwd=2j0r
提取码：2j0r

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('nuclear.csv', delimiter=',')

countries_shortNames = [['UNITED STATES OF AMERICA', 'USA'], \
                        ['RUSSIAN FEDERATION', 'RUSSIA'], \
                        ['IRAN, ISLAMIC REPUBLIC OF', 'IRAN'], \
                        ['KOREA, REPUBLIC OF', 'SOUTH KOREA'], \
                        ['TAIWAN, CHINA', 'CHINA']]
for shortName in countries_shortNames:
    df = df.replace(shortName[0], shortName[1])
```

### 三、核电站的世界分布

```python
import folium
import matplotlib.cm as cm
import matplotlib.colors as colors

latitude, longitude = 40, 10.0
map_world_NPP = folium.Map(location=[latitude, longitude], zoom_start=2)

viridis = cm.get_cmap('viridis', df['NumReactor'].max())
colors_array = viridis(np.arange(df['NumReactor'].min() - 1, df['NumReactor'].max()))
rainbow = [colors.rgb2hex(i) for i in colors_array]

for nReactor, lat, lng, borough, neighborhood in zip(df['NumReactor'].astype(int), df['Latitude'].astype(float),
                                                     df['Longitude'].astype(float), df['Plant'], df['NumReactor']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=3,
        popup=label,
        color=rainbow[nReactor - 1],
        fill=True,
        fill_color=rainbow[nReactor - 1],
        fill_opacity=0.5).add_to(map_world_NPP)

# 在地图上显示
map_world_NPP.save('world_map.html')  # 保存为 HTML 文件
# 然后打开world_map.html 文件 可以看到
```

![下载 (39)](image\下载 (39).png)

### 四、拥有最多核反应堆的20个国家对比

```python
countries = df['Country'].unique()
df_count_reactor = [[i, df[df['Country'] == i]['NumReactor'].sum(), df[df['Country'] == i]['Region'].iloc[0]] for i in
                    countries]
df_count_reactor = pd.DataFrame(df_count_reactor, columns=['Country', 'NumReactor', 'Region'])
df_count_reactor = df_count_reactor.set_index('Country').sort_values(by='NumReactor', ascending=False)[:20]
ax = df_count_reactor.plot(kind='bar', stacked=True, figsize=(10, 3),
                           title='The 20 Countries With The Most Nuclear Reactors in 2010')
ax.set_ylim((0, 150))
for p in ax.patches:
    ax.annotate(str(p.get_height()), xy=(p.get_x(), p.get_height() + 2))
df_count_reactor['Country'] = df_count_reactor.index
sns.set(rc={'figure.figsize': (11.7, 8.27)})
sns.set_style("whitegrid")
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
ax = sns.barplot(x="NumReactor", y="Country", hue="Region", data=df_count_reactor, dodge=False, orient='h')
ax.set_title('2010年拥有最多核反应堆的20个国家', fontsize=16)
ax.set_xlabel('Reactors', fontsize=16)
ax.set_ylabel('')
ax.legend(fontsize='14')

plt.show()
```

![下载 (40)](image\下载 (40).png)

### 五、核电站暴露人口的分析

```python
def getMostExposedNPP(Exposedradius):
    df_pop_sort = df.sort_values(by=str('p10_' + str(Exposedradius)), ascending=False)[:10]
    df_pop_sort['Country'] = df_pop_sort['Plant'] + ',\n' + df_pop_sort['Country']
    df_pop_sort = df_pop_sort.set_index('Country')
    df_pop_sort = df_pop_sort.rename(
        columns={str('p90_' + str(Exposedradius)): '1990', str('p00_' + str(Exposedradius)): '2000',
                 str('p10_' + str(Exposedradius)): '2010'})
    df_pop_sort = df_pop_sort[['1990', '2000', '2010']] / 1E6
    ax = df_pop_sort.plot(kind='bar', stacked=False, figsize=(10, 4))
    ax.set_ylabel('Population Exposure in millions', size=14)
    ax.set_title(
        'Location of nuclear power plants \n with the most exposed population \n within ' + Exposedradius + ' km radius',
        size=16)
    print(df_pop_sort['2010'])

getMostExposedNPP('30')


latitude, longitude = 40, 10.0
map_world_NPP = folium.Figure(width=100, height=100)
map_world_NPP = folium.Map(location=[latitude, longitude], zoom_start=2)

for nReactor, lat, lng, borough, neighborhood in zip(df['NumReactor'].astype(int), df['Latitude'].astype(float),
                                                     df['Longitude'].astype(float), df['Plant'], df['NumReactor']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.Circle(
        [lat, lng],
        radius=30000,
        popup=label,
        color='grey',
        fill=True,
        fill_color='grey',
        fill_opacity=0.5).add_to(map_world_NPP)

Exposedradius = '30'
df_sort = df.sort_values(by=str('p10_' + str(Exposedradius)), ascending=False)[:10]

for nReactor, lat, lng, borough, neighborhood in zip(df_sort['NumReactor'].astype(int),
                                                     df_sort['Latitude'].astype(float),
                                                     df_sort['Longitude'].astype(float), df_sort['Plant'],
                                                     df_sort['NumReactor']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='red',
        fill=True,
        fill_color='red',
        fill_opacity=0.25).add_to(map_world_NPP)

for nReactor, lat, lng, borough, neighborhood in zip(df_sort['NumReactor'].astype(int),
                                                     df_sort['Latitude'].astype(float),
                                                     df_sort['Longitude'].astype(float), df_sort['Plant'],
                                                     df_sort['NumReactor']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.Circle(
        [lat, lng],
        radius=30000,
        popup=label,
        color='red',
        fill=True,
        fill_color='red',
        fill_opacity=0.25).add_to(map_world_NPP)
# 在地图上显示
map_world_NPP.save('world_map2.html')  # 保存为 HTML 文件
```

![下载 (41)](image\下载 (41).png)

### 六、总结

如果核电站靠近人口密集区，核污染水排海可能对周边人口产生一些严重影响：

1.健康风险：放射性物质对人体健康产生潜在威胁。如果核污染水排入海洋，有可能通过海洋食物链的途径进入人类的食物供应链中，从而增加食物中放射性物质的摄入风险。不当接触或摄入这些物质可能导致慢性疾病，如癌症和其他与放射性物质相关的健康问题。

2.社会心理影响：核事故可能引发社会心理压力和不安感。居住在福岛核电站附近的居民可能面临被迫疏散、失去家园、生活不稳定等问题，这对他们的心理健康和社会适应能力造成挑战。

3.经济影响：核事故对当地经济造成了持续的冲击。核电站事故导致了大量的停工和疏散措施，对当地居民和企业的生计和经济活动造成了严重影响。此外，核事故还对当地旅游业、农业和渔业等行业带来负面影响，进一步加剧了经济困难。

## 15-推荐算法-协同过滤在电影推荐中的应用实践



![下载 (42)](image\下载 (42).png)

### 1. 背景与应用场景

推荐系统是一种信息过滤系统，旨在解决信息过载问题。在电影推荐领域，推荐系统能够根据用户的兴趣和历史行为，为用户推荐可能感兴趣的电影。例如，当用户在电影平台上浏览电影时，推荐系统可以根据用户的观看历史、评分、搜索记录等信息，为用户推荐相似或相关的电影。

### 2. 推荐算法的数学原理

推荐算法主要分为协同过滤和基于内容的推荐两种方法。下面我们将分别介绍这两种方法的数学原理。

#### 2.1 协同过滤

协同过滤（Collaborative Filtering, CF）是一种基于用户历史行为数据的推荐方法。其基本思想是：如果两个用户在过去的某些项目上表现出相似的兴趣，那么他们在未来的项目上也可能表现出相似的兴趣。协同过滤主要包括用户基于的协同过滤（User-based CF）和物品基于的协同过滤（Item-based CF）两种方法。

#### 2.1.1 用户基于的协同过滤

用户基于的协同过滤通过计算用户之间的相似度，找到与目标用户相似的用户群体，然后根据这些相似用户的兴趣推荐项目。用户之间的相似度可以通过余弦相似度、皮尔逊相关系数等方法计算。假设我们有一个用户-物品评分矩阵R ∈ R m × n R \in \mathbb{R}^{m \times n}*R*∈R*m*×*n*，其中m m*m*表示用户数，n n*n*表示物品数，R i j R_{ij}*R**ij*表示用户i i*i*对物品j j*j*的评分。用户i i*i*和用户j j*j*之间的余弦相似度可以表示为：
sim ( i , j ) = ∑ k = 1 n R i k ⋅ R j k ∑ k = 1 n R i k 2 ⋅ ∑ k = 1 n R j k 2 \text{sim}(i, j) = \frac{\sum_{k=1}^{n} R_{ik} \cdot R_{jk}}{\sqrt{\sum_{k=1}^{n} R_{ik}^2} \cdot \sqrt{\sum_{k=1}^{n} R_{jk}^2}}sim(*i*,*j*)=∑*k*=1*n*​*R**ik*2​​⋅∑*k*=1*n*​*R**jk*2​​∑*k*=1*n*​*R**ik*​⋅*R**jk*​​

#### 2.1.2 物品基于的协同过滤

物品基于的协同过滤通过计算物品之间的相似度，找到与目标物品相似的物品群体，然后根据用户对这些相似物品的评分预测用户对目标物品的评分。物品之间的相似度可以通过余弦相似度、调整余弦相似度等方法计算。假设我们有一个物品-用户评分矩阵R ∈ R n × m R \in \mathbb{R}^{n \times m}*R*∈R*n*×*m*，物品i i*i*和物品j j*j*之间的余弦相似度可以表示为：
sim ( i , j ) = ∑ k = 1 m R k i ⋅ R k j ∑ k = 1 m R k i 2 ⋅ ∑ k = 1 m R k j 2 \text{sim}(i, j) = \frac{\sum_{k=1}^{m} R_{ki} \cdot R_{kj}}{\sqrt{\sum_{k=1}^{m} R_{ki}^2} \cdot \sqrt{\sum_{k=1}^{m} R_{kj}^2}}sim(*i*,*j*)=∑*k*=1*m*​*R**ki*2​​⋅∑*k*=1*m*​*R**kj*2​​∑*k*=1*m*​*R**ki*​⋅*R**kj*​​

#### 2.2 基于内容的推荐

基于内容的推荐（Content-based Filtering）是一种基于项目特征的推荐方法。其基本思想是：如果用户喜欢某个项目，那么具有相似特征的其他项目也可能受到用户的喜爱。基于内容的推荐主要包括项目特征提取、用户兴趣建模和推荐生成三个步骤。

### 3. 电影推荐的实例

以电影推荐为例，我们可以将电影的特征分为导演、演员、类型、年代等。假设我们有一个电影-特征矩阵F ∈ R n × p F \in \mathbb{R}^{n \times p}*F*∈R*n*×*p*，其中p p*p*表示特征数，F i j F_{ij}*F**ij*表示电影i i*i*在第j j*j*个特征上的取值。根据用户的历史行为，我们可以得到一个用户-特征偏好矩阵P ∈ R m × p P \in \mathbb{R}^{m \times p}*P*∈R*m*×*p*，其中P i j P_{ij}*P**ij*表示用户i i*i*对特征j j*j*的偏好程度。那么，用户i i*i*对电影j j*j*的兴趣可以表示为：
兴趣 ( i , j ) = ∑ k = 1 p F j k ⋅ P i k \text{兴趣}(i, j) = \sum_{k=1}^{p} F_{jk} \cdot P_{ik}兴趣(*i*,*j*)=*k*=1∑*p*​*F**jk*​⋅*P**ik*​
根据计算得到的兴趣值，我们可以为用户推荐兴趣值最高的电影。

### 4. 电影推荐的实例代码实现

数据csv样例：movie_data2.csv

```python
user_id,movie_id,rating,title,genre,release_year
1,1,5,"The Shawshank Redemption","Drama",1994
1,2,4,"The Godfather","Crime",1972
1,3,3,"Pulp Fiction","Crime",1994
1,4,2,"Forrest Gump","Drama",1994
2,1,4,"The Shawshank Redemption","Drama",1994
2,2,5,"The Godfather","Crime",1972
2,3,2,"Pulp Fiction","Crime",1994
2,5,4,"The Dark Knight","Action",2008
3,1,5,"The Shawshank Redemption","Drama",1994
3,6,4,"Inception","Sci-Fi",2010
3,7,3,"The Matrix","Sci-Fi",1999
4,2,4,"The Godfather","Crime",1972
4,4,5,"Forrest Gump","Drama",1994
4,5,3,"The Dark Knight","Action",2008
4,6,4,"Inception","Sci-Fi",2010
5,3,3,"Pulp Fiction","Crime",1994
5,7,4,"The Matrix","Sci-Fi",1999
5,8,5,"Fight Club","Drama",1999
```

实现代码如下：

```python
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# 假设df是已经读取的CSV数据，直接运行
df = pd.DataFrame({
    'user_id': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3],
    'movie_id': [1, 2, 3, 4, 1, 2, 3, 5, 1, 6, 7],
    'rating': [5, 4, 3, 2, 4, 5, 2, 4, 5, 4, 3],
    'title': ["The Shawshank Redemption", "The Godfather", "Pulp Fiction", "Forrest Gump",
              "The Shawshank Redemption", "The Godfather", "Pulp Fiction", "The Dark Knight",
              "The Shawshank Redemption", "Inception", "The Matrix"],
    'genre': ["Drama", "Crime", "Crime", "Drama", "Drama", "Crime", "Crime", "Action", "Drama", "Sci-Fi", "Sci-Fi"],
    'release_year': [1994, 1972, 1994, 1994, 1994, 1972, 1994, 2008, 1994, 2010, 1999]
})
print(df )

# 如果是读取movie_data.csv，可以根据以上数据构建出movie_data.csv
# data = pd.read_csv('movie_data.csv')
# df = data
# print(df )

# 构建一个用户-电影评分矩阵
user_movie_matrix = df.pivot_table(index='user_id', columns='movie_id', values='rating')

# 填充缺失值为0
user_movie_matrix = user_movie_matrix.fillna(0)

# 计算余弦相似度
user_similarity = cosine_similarity(user_movie_matrix)

# 转换为DataFrame以便查看
user_similarity_df = pd.DataFrame(user_similarity, index=user_movie_matrix.index, columns=user_movie_matrix.index)

# 找到与用户1最相似的用户
most_similar_users = user_similarity_df.loc[1].sort_values(ascending=False).index[1:]  # 排除用户自己

# 找出这些用户评价较高的电影
recommended_movies = df[(df['user_id'].isin(most_similar_users)) &
                        ~(df['movie_id'].isin(df[df['user_id'] == 1]['movie_id']))].groupby('movie_id').mean().sort_values(by='rating', ascending=False)

print(recommended_movies)
```

运行结果：
movie_id
5 2.0 4.0 2008.0
6 3.0 4.0 2010.0
7 3.0 3.0 1999.0

## 16-关于自适应增强AdaBoost模型的实际应用，AdaBoost模型模型结构介绍

### 一、AdaBoost模型概述

#### AdaBoost起源与发展

AdaBoost（Adaptive Boosting）是一种迭代式集成学习方法，由Yoav Freund和Robert Schapire在1995年提出。它通过结合多个弱分类器来构建一个强分类器，其核心思想是赋予对当前数据集分类错误率较高的样本更大的权重，然后训练下一个弱分类器，使其更关注这些难以分类的样本。在每一轮迭代后， AdaBoost会调整样本权重并结合所有弱分类器的结果，形成最终的强分类器。

AdaBoost的发展历程中，从最初的二类分类问题扩展到了多类问题，并在实际应用中如计算机视觉、生物信息学等领域取得了显著效果。此外，AdaBoost还启发了后续一系列提升方法的研究，如Gradient Boosting等。

假设我们正在训练一个系统来识别水果，初始阶段可能使用一个简单的分类器，比如形状规则度来区分苹果和橙子。然而，有些形状不规则的苹果可能会被误判为橙子。这时，AdaBoost算法就会增加这些误分类苹果的“重要性”，在下一轮训练中，引入新的弱分类器，比如颜色特征，这次会更加关注那些形状不规则但颜色像苹果的样本。经过多轮迭代，综合各个弱分类器的结果，系统就能更准确地识别出各种形态的苹果和橙子，从而提高整体的分类性能。

#### AdaBoost基本原理

AdaBoost（Adaptive Boosting）是一种迭代算法，主要用于集成学习，通过结合多个弱分类器来构建一个强分类器。其基本原理是：在每一轮训练中，AdaBoost会根据上一轮各个弱学习器的预测准确率分配不同的权重给样本，对那些被错误分类或者难以分类的样本提高其权重，然后基于调整后的样本分布训练新的弱学习器。这样，后续的学习器会更加关注之前被错误分类的样本，从而逐步提升整体分类效果。

具体流程如下：首先，所有训练样本的初始权重均匀分布；然后，在每一轮迭代中，训练一个弱学习器，并计算该学习器在当前权重分布下的错误率；接着，根据错误率更新样本权重，错误分类的样本权重增大，正确分类的样本权重减小；最后，将新训练出的弱学习器依据其错误率赋予一个权重，并加入到最终的强学习器集合中。经过多轮迭代后， AdaBoost将所有弱学习器的结果按照其权重进行加权求和，形成最终的预测结果。

假设我们要组织一场运动会，需要选拔出能赢得比赛的选手。AdaBoost就像是教练团队，每个弱学习器就像一位教练。一开始，每位参赛者（样本）都有相同的机会被选中（权重均匀）。在第一轮选拔中，某位教练选出了一批他认为有潜力的选手，但在实际比赛中这批选手并未取得理想成绩（即分类错误）。于是，在下一轮选拔时，教练团队会更加重视上次未被正确识别出实力的选手，给他们更多机会参加选拔和训练。经过多轮这样的过程，教练团队综合多位教练的意见（弱学习器的结果），最终确定出最有竞争力的选手阵容（强学习器）。

### 二、AdaBoost模型结构特点

#### 弱学习器序列集成

AdaBoost（Adaptive Boosting）是一种迭代算法，其核心思想是通过构建并结合多个弱学习器来形成一个强学习器。在AdaBoost中，每个弱学习器都以加权的方式参与最终决策，并且这些权重是由前一轮的学习结果和错误率动态调整的。具体步骤如下：

1. 初始化训练样本的权重分布，所有样本权重均等。
2. 对于每一轮t=1,2,…T：
   a. 根据当前权重分布训练一个弱学习器ht。
   b. 计算ht在训练集上的错误率。
   c. 根据错误率计算ht的权重αt，错误率越低，权重越大。
   d. 更新样本权重分布，将被ht误分类的样本权重增大，正确分类的样本权重减小。
3. 最终的强学习器H(x)为各个弱学习器结果的加权求和。

假设我们要预测一场足球比赛的胜负，初始时我们认为每支球队获胜的概率相同。我们首先请一位初级分析师（弱学习器ht）预测，如果他预测准确，我们就更信任他的判断（增大其权重αt），并将下一次预测时他看好的队伍权重提高；如果预测错误，则降低该队伍在他下次预测中的权重，并邀请下一位分析师进行预测。经过多轮多位初级分析师的预测后，我们将他们每个人的意见按照准确度赋予不同的权重，综合得出最终的比赛结果预测，这就是AdaBoost的工作原理。

#### 样本权重动态调整

AdaBoost（Adaptive Boosting）是一种迭代算法，其核心特点是样本权重的动态调整机制。在每一轮训练中，AdaBoost首先根据当前各样本的权重分布训练一个弱学习器。对于本轮训练错误分类的样本，AdaBoost会增大它们在下一轮训练中的权重，而正确分类的样本权重则会相应减小。这样，后续的学习器会更加关注那些先前难以被正确分类的样本，从而逐步提升整体模型的预测能力。

假设我们正在训练一个机器人识别水果的任务，初始时所有水果（样本）被赋予相同的关注度（权重）。在第一轮训练后，机器人对苹果和香蕉识别准确，但对梨子识别较差。这时，AdaBoost算法就会在下一轮训练中增加梨子的关注度（权重），让机器更专注于学习梨子的特点；同时减少苹果和香蕉的关注度，因为它们已经被较好地识别。通过这样的迭代过程，机器人最终能有效提高对各类水果的识别准确率。

#### 最终决策规则

AdaBoost（Adaptive Boosting）是一种迭代算法，通过结合多个弱分类器来构建强分类器。其核心思想是赋予错误分类样本更大的权重，使得后续的弱分类器能更关注这些难以分类的样本，从而逐步提升整体分类性能。

在AdaBoost模型中，首先所有训练样本的初始权重均匀分布。每一轮迭代，AdaBoost会选择一个弱学习器，并依据该学习器对训练样本的分类误差调整样本权重：分类错误的样本权重增加，正确分类的样本权重减小。这样，在下一轮迭代中，模型将更加关注那些先前分类错误的样本。重复此过程，直到达到预设的迭代次数或满足一定的精度要求。

最后，AdaBoost会将各个弱学习器的预测结果进行加权融合，形成最终的强分类决策规则，其中每个弱学习器的权重与其在训练过程中的错误率成反比，即错误率越低的弱学习器在最终决策中的影响力越大。

假设我们要通过观察特征（如年龄、收入等）来判断一个人是否可能购买某款高端产品。AdaBoost就像一个团队，每个成员（弱学习器）都有自己的判断标准（如年龄小于30岁的年轻人、月收入高于5万的人可能购买）。在初期，所有人意见同等重要。但随着讨论（迭代）的进行，对于那些之前判断错误的人（如高收入的年轻人并未购买），团队会更多地倾听他们的观点（增大其权重），并据此调整判断策略。最后，团队综合每位成员的意见（弱学习器的加权融合），得出最有可能购买这款产品的群体。

### 三、AdaBoost模型的实际应用举例

#### 人脸识别

AdaBoost（Adaptive Boosting）是一种迭代算法，通过结合多个弱分类器来构建强分类器，广泛应用于各类机器学习任务中，包括人脸识别。

在人脸识别场景下，AdaBoost的工作原理如下：首先，算法会对训练集中的每一张人脸图像进行处理，并利用一个基础的弱分类器（如决策树、线性模型等）进行初步识别。由于单个弱分类器可能存在误判，AdaBoost会根据每个弱分类器在训练集上的表现（准确率），赋予不同的权重。对于那些能正确区分人脸和非人脸或者不同个体的弱分类器，其权重会增大；反之，权重则会减小。

接下来，AdaBoost会基于调整过权重的训练集再次训练新的弱分类器，如此迭代多次，形成一系列弱分类器的集合。最后，当新的样本输入时，这一系列弱分类器会依据各自的权重对样本进行投票，最终结果由多数决定，从而实现高效且精准的人脸识别。

假设在一个大型聚会中，AdaBoost模型被用于门禁系统进行人脸识别。初次扫描时，某个弱分类器可能只关注眼睛部分特征进行识别，但存在误识风险。而AdaBoost会不断迭代优化，后续的弱分类器可能更侧重于鼻子、嘴巴或脸部轮廓等其他特征。当有人试图进入时，所有弱分类器共同“表决”，只有当大多数分类器都确认这是已授权人员的脸部信息时，门禁才会开启。这样，通过集成多个视角和特征，AdaBoost显著提高了人脸识别系统的准确性和鲁棒性。

#### 信用评分卡构建

AdaBoost（Adaptive Boosting）是一种集成学习方法，常用于提高弱分类器的预测精度，尤其适用于信用评分卡构建。在信用评估领域，银行或金融机构需要对申请贷款的客户进行信用风险评估，以预测其未来违约的可能性。通过构建信用评分卡，可以将复杂的非线性关系和多因素影响转化为一个简洁的分数。

具体应用中，首先收集大量客户的信用历史、收入水平、负债情况、职业稳定性等信息作为特征数据。然后利用AdaBoost算法训练模型，每个弱分类器会侧重于那些在前一轮被错误分类或者分类难度较大的样本，逐步迭代优化，最终形成强分类器。这个强分类器能够综合所有弱分类器的结果，给出一个反映客户信用状况的评分。

假设小明打算向银行申请个人贷款，银行运用基于AdaBoost的信用评分系统对其信用情况进行评估。该系统首先分析了小明的各项信息，如工资收入、是否有稳定工作、过往还款记录等。在AdaBoost模型中，若某个弱分类器发现小明的月收入较低，可能会初步判断其信用风险较高；但在后续迭代过程中，如果其他弱分类器发现小明有良好的按时还款记录且工作稳定，那么他的信用评分可能会上升。最后，经过一系列这样的“投票”，银行综合得出小明的信用评分，决定是否批准贷款以及设定合理的利率水平。

#### 医学诊断

在医学诊断中，AdaBoost模型是一种强大的机器学习工具，能够有效提升诊断的准确性和效率。例如，在肿瘤识别领域，通过收集大量的病理切片图像数据，包括正常细胞和癌变细胞的图片。AdaBoost模型可以结合多个弱分类器（如决策树），每个分类器都会对样本进行权重分配，错误分类的样本权重会增加，使得后续的分类器更加关注这些难以区分的样本。经过多轮迭代训练后，众多弱分类器的组合形成一个强分类器，从而更准确地识别出肿瘤细胞。

假设我们要通过观察水果表面特征来判断其是否成熟。单凭一种特征（比如颜色）可能无法准确判断，因为有些水果未熟时颜色就偏深，而有的水果即使熟透颜色也较浅。这时，我们可以借助AdaBoost模型，将“颜色”、“硬度”、“重量”等多种特征作为弱分类器，对于那些我们初次判断错误的水果，加大其在下次判断时的权重。经过多次迭代学习，模型就能综合各种特征，更准确地判断水果是否成熟，就像经验丰富的果农一样。

### 四、AdaBoost模型的数学原理

在AdaBoost（Adaptive Boosting）算法中，其核心思想是通过迭代训练多个弱分类器，并赋予每个分类器一个权重，最终将这些弱分类器组合起来形成一个强分类器。以下为AdaBoost的数学原理概述：

假设我们有一个训练数据集 D = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x m , y m ) } D = \{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\}*D*={(*x*1,*y*1),(*x*2,*y*2),...,(*x**m*,*y**m*)}，其中x i x_i*x**i*是特征向量，y i ∈ { − 1 , 1 } y_i \in \{-1, 1\}*y**i*∈{−1,1}是对应的标签。

在第t t*t*轮迭代中：

1. 初始化样本权重分布 D t ( i ) = 1 m D_t(i) = \frac{1}{m}*D**t*(*i*)=*m*1，表示所有样本在本轮开始时的重要性相同。
2. 使用当前权重分布训练出一个弱学习器（如决策树）h t ( x ) h_t(x)*h**t*(*x*)，使其在当前权重分布下的训练误差最小：
   min ⁡ h t ϵ t = ∑ i = 1 m D t ( i ) I ( y i ≠ h t ( x i ) ) \min_{h_t} \epsilon_t = \sum_{i=1}^{m} D_t(i) I(y_i \neq h_t(x_i))*h**t*​min​*ϵ**t*​=*i*=1∑*m*​*D**t*​(*i*)*I*(*y**i*​=*h**t*​(*x**i*​))
   其中，I ( ⋅ ) I(\cdot)*I*(⋅) 是指示函数，当括号内条件成立时取值为1，否则为0。
3. 计算该弱学习器的错误率 ϵ t \epsilon_t*ϵ**t*，并计算其权重 α t = 1 2 ln ⁡ ( 1 − ϵ t ϵ t ) \alpha_t = \frac{1}{2}\ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)*α**t*=21ln(*ϵ**t*1−*ϵ**t*)。
4. 更新样本权重分布 D t + 1 ( i ) D_{t+1}(i)*D**t*+1(*i*)，使得被误分类样本的权重增大，正确分类样本的权重减小：
   D t + 1 ( i ) = D t ( i ) Z t exp ⁡ ( − α t y i h t ( x i ) ) D_{t+1}(i) = \frac{D_t(i)}{Z_t} \exp(-\alpha_t y_i h_t(x_i))*D**t*+1​(*i*)=*Z**t*​*D**t*​(*i*)​exp(−*α**t*​*y**i*​*h**t*​(*x**i*​))
   其中，Z t Z_t*Z**t*​ 是规范化因子，确保 D t + 1 D_{t+1}*D**t*+1​ 是一个概率分布。
5. 重复步骤2-4直到达到预设的最大迭代次数T。

最后，AdaBoost模型的预测函数为各个弱学习器结果的加权和：
F ( x ) = s i g n ( ∑ t = 1 T α t h t ( x ) ) F(x) = sign\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)*F*(*x*)=*s**i**g**n*(*t*=1∑*T*​*α**t*​*h**t*​(*x*))

以上就是AdaBoost算法的主要数学原理，通过迭代优化， AdaBoost能够有效地提升弱分类器的整体性能，从而构建出强大的集成模型。

### 五、AdaBoost模型的代码实现

在PyTorch中实现AdaBoost模型时，我们通常不会直接实现AdaBoost算法本身，而是利用现有的集成学习库如Scikit-Learn中的AdaBoostClassifier或AdaBoostRegressor与PyTorch模型结合使用。以下是一个使用PyTorch作为基础模型，然后在Scikit-Learn中使用AdaBoost分类器的简单示例：

```python
import torch
from torch import nn
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 定义一个简单的PyTorch神经网络模型
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.fc2 = nn.Linear(16, 3)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 数据预处理
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 将数据转换为PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

# 创建并训练PyTorch模型
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(100):  # 假设我们训练100个epoch
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

# 将训练好的PyTorch模型作为弱学习器用于AdaBoost
model.eval()
weak_learner = lambda X: model(torch.tensor(X, dtype=torch.float32)).detach().numpy().argmax(axis=1)

# 使用AdaBoostClassifier
ada_boost = AdaBoostClassifier(base_estimator=weak_learner, n_estimators=50)
ada_boost.fit(X_train.numpy(), y_train.numpy())

# 预测并评估
predictions = ada_boost.predict(X_test.numpy())
accuracy = (predictions == y_test.numpy()).mean() * 100
print(f"Accuracy of AdaBoost with PyTorch model: {accuracy:.2f}%")
```

大家注意：代码首先训练了一个简单的PyTorch神经网络作为弱学习器，然后将其应用于AdaBoostClassifier中。在实际应用中，你可能需要根据具体任务调整模型结构、训练参数以及AdaBoost的相关参数。

### 六、总结

AdaBoost是一种名为“自适应增强”的迭代式集成学习算法，它通过整合多个弱分类器构建强大的预测模型，以提升模型准确性和预测性能。在AdaBoost的每一次迭代中，它会根据上一轮各弱学习器的表现，针对错误率较高的样本调整其权重，从而让后续的弱分类器更专注于解决先前未被有效分类的问题。这一动态权重调整机制使得每个新增的弱分类器都能有针对性地改进模型性能。经过多轮迭代后，所有弱分类器依据其准确性和赋予的权重综合决定最终结果，形成强分类器。由于其高效稳定、实施便捷等优点，AdaBoost在人脸识别、文本分类、医学诊断等领域得到了广泛应用。

## 17-基于TensorFlow图像识别系统的设计与开发

作者：禅与计算机程序设计艺术 / Zen and the Art of Computer Programming

### 1. 背景介绍

#### 1.1 问题的由来

随着计算机视觉技术的飞速发展，图像识别已成为人工智能领域的一个重要分支。从早期的基于传统算法的图像识别，到如今深度学习时代的卷积神经网络（Convolutional Neural Networks, CNNs），图像识别技术在许多领域都取得了显著的成果，如安防监控、医疗诊断、自动驾驶等。

TensorFlow作为Google开源的深度学习框架，因其易用性、高性能和强大的社区支持，成为了图像识别领域最受欢迎的工具之一。本文将详细介绍基于TensorFlow的图像识别系统的设计与开发，旨在为广大开发者提供参考和借鉴。

#### 1.2 研究现状

近年来，图像识别技术在以下几个方面取得了显著进展：

1. **深度学习技术的发展**：深度学习，尤其是卷积神经网络，在图像识别任务上取得了显著的突破，性能大幅提升。
2. **大数据的推动**：大规模图像数据集的涌现，为图像识别研究提供了丰富的资源，推动了算法的改进和模型的发展。
3. **算法的优化**：针对图像识别任务的优化算法，如注意力机制、迁移学习等，提高了模型的效率和准确性。

#### 1.3 研究意义

本文旨在通过对TensorFlow图像识别系统的设计与开发，使读者深入了解以下内容：

1. TensorFlow框架在图像识别领域的应用；
2. 图像识别系统的设计与开发流程；
3. 图像识别算法的原理和实现；
4. 图像识别系统的实际应用场景。

#### 1.4 本文结构

本文分为八个章节，具体如下：

- 1. 核心概念与联系
- 1. 核心算法原理 & 具体操作步骤
- 1. 数学模型和公式 & 详细讲解 & 举例说明
- 1. 项目实践：代码实例和详细解释说明
- 1. 实际应用场景
- 1. 工具和资源推荐
- 1. 总结：未来发展趋势与挑战

### 2. 核心概念与联系

#### 2.1 图像识别

图像识别是计算机视觉领域的一个基本任务，旨在从图像或视频中提取有用信息，识别图像中的目标、场景和对象。根据不同的应用场景，图像识别可以分为以下几类：

1. **目标识别**：识别图像中的特定目标。
2. **场景识别**：识别图像中的场景类型，如城市、乡村、室内、室外等。
3. **对象检测**：检测图像中的多个目标，并给出每个目标的位置和类别。

#### 2.2 卷积神经网络

卷积神经网络（Convolutional Neural Networks, CNNs）是一种专门针对图像数据设计的深度学习模型，具有以下特点：

1. **局部连接**：CNN通过局部连接机制，能够有效地提取图像中的局部特征。
2. **权值共享**：CNN通过权值共享机制，能够减少模型参数数量，提高计算效率。
3. **层次化特征提取**：CNN通过多层的卷积和池化操作，能够提取图像的层次化特征。

#### 2.3 TensorFlow

TensorFlow是Google开源的深度学习框架，具有以下特点：

1. **易用性**：TensorFlow提供了丰富的API和工具，方便开发者进行深度学习应用的开发。
2. **高性能**：TensorFlow具有高效的计算性能，能够处理大规模的数据集和复杂的模型。
3. **可扩展性**：TensorFlow支持分布式计算，能够方便地扩展到多台机器上。

### 3. 核心算法原理 & 具体操作步骤

#### 3.1 算法原理概述

图像识别系统通常由以下三个主要部分组成：

1. **数据预处理**：对图像数据进行预处理，包括图像尺寸调整、归一化、裁剪等。
2. **模型训练**：使用训练数据对模型进行训练，包括损失函数的选择、优化算法的选择等。
3. **模型评估**：使用测试数据对模型进行评估，包括准确率、召回率等指标的计算。

#### 3.2 算法步骤详解

##### 3.2.1 数据预处理

数据预处理是图像识别系统的重要组成部分，其目的是提高模型的性能和鲁棒性。以下是一些常见的数据预处理方法：

1. **图像尺寸调整**：将图像缩放到固定尺寸，以便在后续处理中使用。
2. **归一化**：将图像像素值归一化到[0,1]范围内。
3. **裁剪**：从图像中裁剪出感兴趣的区域。
4. **数据增强**：通过随机旋转、翻转、缩放等操作增加数据多样性。

##### 3.2.2 模型训练

模型训练是图像识别系统的核心环节，其目的是通过训练数据学习到有效的特征表示和分类器。以下是一些常见的模型训练步骤：

1. **选择模型架构**：根据任务需求选择合适的模型架构，如CNN、RNN等。
2. **数据加载**：加载训练数据和标签。
3. **损失函数设计**：设计损失函数，如交叉熵损失、平方误差损失等。
4. **优化算法选择**：选择合适的优化算法，如SGD、Adam等。
5. **模型训练**：使用训练数据对模型进行训练，并调整模型参数。

##### 3.2.3 模型评估

模型评估是检查模型性能的重要环节，以下是一些常见的模型评估指标：

1. **准确率（Accuracy）**：模型正确识别样本的比例。
2. **召回率（Recall）**：模型正确识别正样本的比例。
3. **F1分数（F1 Score）**：准确率和召回率的调和平均。

#### 3.3 算法优缺点

##### 3.3.1 优点

1. **强大的特征提取能力**：CNN能够有效地提取图像的层次化特征，提高模型的识别性能。
2. **高灵活性**：TensorFlow框架提供了丰富的API和工具，方便开发者进行模型设计和开发。
3. **良好的生态**：TensorFlow拥有庞大的社区支持和丰富的资源，便于开发者学习和交流。

##### 3.3.2 缺点

1. **计算量较大**：CNN模型通常需要大量的计算资源进行训练。
2. **模型可解释性较差**：CNN模型作为黑盒模型，其内部机理难以解释。
3. **数据需求量大**：图像识别任务通常需要大量的数据集进行训练。

#### 3.4 算法应用领域

基于TensorFlow的图像识别系统在以下领域有着广泛的应用：

1. **安防监控**：人脸识别、车辆识别等。
2. **医疗诊断**：疾病检测、医学图像分析等。
3. **自动驾驶**：车道线检测、行人检测等。
4. **内容审核**：违禁内容检测、图像风格识别等。

### 4. 数学模型和公式 & 详细讲解 & 举例说明

#### 4.1 数学模型构建

图像识别系统的数学模型主要包括以下几部分：

1. **卷积层**：卷积层通过卷积操作提取图像特征。
2. **池化层**：池化层通过池化操作降低特征维度，提高模型鲁棒性。
3. **全连接层**：全连接层通过全连接操作将特征映射到输出结果。
4. **激活函数**：激活函数为神经网络提供非线性变换。

#### 4.2 公式推导过程

以下是一些常见的数学公式及其推导过程：

##### 4.2.1 卷积公式

卷积公式如下：

$$f(x, y) = \sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} \times g(x-i, y-j)$$

其中，$f(x, y)$表示输出特征，$w_{ij}$表示卷积核，$g(x, y)$表示输入特征。

##### 4.2.2 池化公式

池化公式如下：

$$p(x, y) = \max_{k \in K} g(x+k, y+k)$$

其中，$p(x, y)$表示池化后的输出特征，$K$表示池化窗口大小。

##### 4.2.3 激活函数公式

常见的激活函数及其公式如下：

1. **ReLU（Rectified Linear Unit）**： $$f(x) = \max(0, x)$$
2. **Sigmoid**： $$f(x) = \frac{1}{1+e^{-x}}$$
3. **Tanh**： $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

#### 4.3 案例分析与讲解

以下是一个简单的图像识别案例，使用TensorFlow构建一个分类模型，识别猫和狗的图片。

##### 4.3.1 数据集

使用CIFAR-10数据集，该数据集包含10个类别的60,000张32x32像素的彩色图像，每个类别包含6,000张图像。

##### 4.3.2 模型架构

使用一个简单的卷积神经网络，包含两个卷积层、两个池化层和一个全连接层。

```python
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = tf.keras.Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
```

##### 4.3.3 训练与评估

使用CIFAR-10数据集对模型进行训练和评估。

```python
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical

# 加载数据集
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# 数据预处理
x_train = x_train / 255.0
x_test = x_test / 255.0
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# 模型编译
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 模型训练
model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))

# 模型评估
loss, accuracy = model.evaluate(x_test, y_test)
print(f"测试集准确率：{accuracy * 100}%")
```

#### 4.4 常见问题解答

1. **问：为什么选择CIFAR-10数据集**？

答：CIFAR-10数据集是一个常用的图像识别数据集，包含10个类别的60,000张32x32像素的彩色图像，非常适合作为图像识别任务的研究和训练。

1. **问：为什么使用卷积神经网络**？

答：卷积神经网络能够有效地提取图像的层次化特征，提高模型的识别性能。

1. **问：如何提高模型的性能**？

答：提高模型性能的方法包括：选择合适的网络架构、调整超参数、使用数据增强等。

### 5. 项目实践：代码实例和详细解释说明

#### 5.1 开发环境搭建

1. 安装TensorFlow：

```bash
pip install tensorflow
```

1. 安装必要的依赖库：

```bash
pip install -r requirements.txt
```

#### 5.2 源代码详细实现

以下是一个简单的图像识别项目示例，使用TensorFlow实现猫和狗的分类。

```python
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Sequential

# 构建模型
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# 编译模型
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 加载数据集
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# 数据预处理
x_train = x_train / 255.0
x_test = x_test / 255.0
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

# 训练模型
model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))

# 评估模型
loss, accuracy = model.evaluate(x_test, y_test)
print(f"测试集准确率：{accuracy * 100}%")
```

#### 5.3 代码解读与分析

1. **导入库**：导入所需的TensorFlow库。
2. **构建模型**：定义一个序列模型，包含卷积层、池化层、全连接层和激活函数。
3. **编译模型**：设置模型优化器、损失函数和评估指标。
4. **加载数据集**：加载数据集并进行预处理。
5. **训练模型**：使用训练数据对模型进行训练。
6. **评估模型**：使用测试数据评估模型的性能。

#### 5.4 运行结果展示

运行以上代码，输出模型在测试集上的准确率：

```
测试集准确率：61.38%
```

这个结果说明，该模型能够以61.38%的准确率识别出猫和狗的图片。

### 6. 实际应用场景

#### 6.1 安防监控

在安防监控领域，图像识别技术可以用于人脸识别、车辆识别等。通过部署基于TensorFlow的图像识别系统，可以实现以下功能：

1. **人脸识别**：识别监控区域的人员，实现门禁控制、考勤管理等。
2. **车辆识别**：识别监控区域的车辆，实现交通管理、违章抓拍等。

#### 6.2 医疗诊断

在医疗诊断领域，图像识别技术可以用于疾病检测、医学图像分析等。通过部署基于TensorFlow的图像识别系统，可以实现以下功能：

1. **疾病检测**：识别医学影像中的病变区域，如肿瘤、心脏病等。
2. **医学图像分析**：对医学影像进行分析，如X光片、CT、MRI等。

#### 6.3 自动驾驶

在自动驾驶领域，图像识别技术可以用于车道线检测、行人检测等。通过部署基于TensorFlow的图像识别系统，可以实现以下功能：

1. **车道线检测**：识别道路上的车道线，实现自动驾驶车辆在车道内行驶。
2. **行人检测**：识别道路上的行人，实现自动驾驶车辆的避让。

#### 6.4 内容审核

在内容审核领域，图像识别技术可以用于违禁内容检测、图像风格识别等。通过部署基于TensorFlow的图像识别系统，可以实现以下功能：

1. **违禁内容检测**：识别图像中的违禁内容，如暴力、色情等。
2. **图像风格识别**：识别图像的风格，如卡通、油画等。

### 7. 工具和资源推荐

#### 7.1 学习资源推荐

1. **TensorFlow官方文档**：https://www.tensorflow.org/tutorials
2. **TensorFlow教程系列**：https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs
3. **深度学习入门**：https://www.deeplearning.ai/

#### 7.2 开发工具推荐

1. **TensorBoard**：TensorFlow可视化工具，用于可视化模型和训练过程。
2. **TensorFlow Dataset**：TensorFlow数据集API，用于数据预处理和加载。
3. **TensorFlow Model Garden**：TensorFlow模型库，提供各种预训练模型。

#### 7.3 相关论文推荐

1. Krizhevsky, A., Sutskever, I., Hinton, G. E.: ImageNet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems. pp. 1097–1105 (2012)
2. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in videos. In: Proceedings of the Advances in Neural Information Processing Systems. pp. 567–575 (2014)
3. Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). ImageNet: a large-scale hierarchical image database. IEEE computer society conference on computer vision and pattern recognition, 248–255.

#### 7.4 其他资源推荐

1. **GitHub**：https://github.com
2. **Stack Overflow**：https://stackoverflow.com
3. **Reddit**：https://www.reddit.com/r/deeplearning

### 8. 总结：未来发展趋势与挑战

#### 8.1 研究成果总结

本文介绍了基于TensorFlow的图像识别系统的设计与开发，涵盖了图像识别的基本概念、TensorFlow框架、模型训练、模型评估等方面。通过一个简单的图像识别案例，展示了TensorFlow在图像识别领域的应用。

#### 8.2 未来发展趋势

1. **模型轻量化**：为了降低计算成本和功耗，模型轻量化将是未来研究的一个重要方向。
2. **多模态学习**：结合图像、文本、语音等多种模态信息，实现更全面的图像理解。
3. **可解释性**：提高模型的解释性，使得模型的决策过程更加透明和可靠。
4. **迁移学习**：通过迁移学习，利用已有模型的知识来加速新任务的训练。

#### 8.3 面临的挑战

1. **计算资源**：大规模的图像识别任务需要大量的计算资源，如何高效地利用计算资源是一个挑战。
2. **数据标注**：高质量的图像数据集需要大量的标注工作，如何提高数据标注的效率和准确性是一个挑战。
3. **模型泛化能力**：如何提高模型的泛化能力，使其能够适应不同的任务和数据集是一个挑战。

#### 8.4 研究展望

随着深度学习技术的不断发展，基于TensorFlow的图像识别系统将在未来取得更大的突破。通过不断的研究和创新，图像识别技术将在更多领域发挥重要作用。

### 9. 附录：常见问题与解答

#### 9.1 问：为什么选择TensorFlow作为图像识别框架？

答：TensorFlow具有以下优点：

1. **易用性**：TensorFlow提供了丰富的API和工具，方便开发者进行模型设计和开发。
2. **高性能**：TensorFlow具有高效的计算性能，能够处理大规模的数据集和复杂的模型。
3. **可扩展性**：TensorFlow支持分布式计算，能够方便地扩展到多台机器上。
4. **社区支持**：TensorFlow拥有庞大的社区支持和丰富的资源，便于开发者学习和交流。

#### 9.2 问：如何提高图像识别模型的性能？

答：提高图像识别模型性能的方法包括：

1. **选择合适的网络架构**：根据任务需求选择合适的网络架构，如CNN、RNN等。
2. **调整超参数**：调整学习率、批处理大小、优化器等超参数，提高模型性能。
3. **数据增强**：通过随机旋转、翻转、缩放等操作增加数据多样性，提高模型的鲁棒性。
4. **迁移学习**：利用已有模型的知识来加速新任务的训练。

#### 9.3 问：图像识别系统在实际应用中面临哪些挑战？

答：图像识别系统在实际应用中面临以下挑战：

1. **数据标注**：高质量的图像数据集需要大量的标注工作，如何提高数据标注的效率和准确性是一个挑战。
2. **模型泛化能力**：如何提高模型的泛化能力，使其能够适应不同的任务和数据集是一个挑战。
3. **计算资源**：大规模的图像识别任务需要大量的计算资源，如何高效地利用计算资源是一个挑战。
4. **模型可解释性**：提高模型的解释性，使得模型的决策过程更加透明和可靠。

#### 9.4 问：未来图像识别技术将如何发展？

答：未来图像识别技术将朝着以下方向发展：

1. **模型轻量化**：为了降低计算成本和功耗，模型轻量化将是未来研究的一个重要方向。
2. **多模态学习**：结合图像、文本、语音等多种模态信息，实现更全面的图像理解。
3. **可解释性**：提高模型的解释性，使得模型的决策过程更加透明和可靠。
4. **迁移学习**：通过迁移学习，利用已有模型的知识来加速新任务的训练。













# 深度学习实战

## 1-(keras框架)企业数据分析与预测

深度学习框架keras的实战项目，用于基本的企业数据分析，预测企业净利润情况，并利用灰色预测函数GM11进行预测模型。我们拿到企业数据，这里参数抽象成x1-x9，y表示净利润，数据如下：

![344db2886617e0930ee10682e7db0cf9](image\344db2886617e0930ee10682e7db0cf9.png)

下面我们开始编写代码进行分析，首先我们引入库：

```haskell
import numpy as np



import pandas as pd



from keras.models import Sequential  



from keras.layers.core import Dense, Activation



import matplotlib.pylab as plt   # 绘制图像库
```

 编写GM11灰色预测函数, 灰色预测函数是通过少量的、不完全的信息，建立数学模型并做出预测的一种预测方法。它是基于客观事物的过去和现在的发展规律，利用科学的方法对未来的发展趋势和状况进行描述和分析，灰色预测函数如下：

```cobol
def GM11(x0): #自定义灰色预测函数



  import numpy as np



  x1 = x0.cumsum() #1-AGO序列



  z1 = (x1[:len(x1)-1] + x1[1:])/2.0 



  z1 = z1.reshape((len(z1),1))



  B = np.append(-z1, np.ones_like(z1), axis = 1)



  Yn = x0[1:].reshape((len(x0)-1, 1))



  [[a],[b]] = np.dot(np.dot(np.linalg.inv(np.dot(B.T, B)), B.T), Yn) #计算参数



  f = lambda k: (x0[0]-b/a)*np.exp(-a*(k-1))-(x0[0]-b/a)*np.exp(-a*(k-2)) #还原值



  delta = np.abs(x0 - np.array([f(i) for i in range(1,len(x0)+1)]))



  C = delta.std()/x0.std()



  P = 1.0*(np.abs(delta - delta.mean()) < 0.6745*x0.std()).sum()/len(x0)



  return f, a, b, x0[0], C, P #返回灰色预测函数、a、b、首项、方差比、小残差概率
```

 读取数据：

```cobol
data = pd.read_csv('data.csv') #读取数据



data.index = range(2000,2020) # 标注索引信息年份
```

 数据操作，预测2020-2022这三年的企业各参数的预测值

```cobol
data.loc[2020] = None



data.loc[2021] = None



data.loc[2022] = None



l = ['x1', 'x2', 'x3', 'x4', 'x5', 'x7']



l1 = ['x3','x5','x7']



for i in l1:



  f,_,_,_,C,_ = GM11(data[i].loc[range(2000,2020)].values)



  print("%s后验差比值：%0.4f"%(i,C))   #后验差比值c，即：真实误差的方差同原始数据方差的比值。



  data[i].loc[2020] = f(len(data)-2) #2014年预测结果



  data[i].loc[2021] = f(len(data)-1) #2015年预测结果



  data[i].loc[2022] = f(len(data))  # 2016年预测结果



  data[i] = data[i].round(2) #保留两位小数



 



data[l1+['y']].to_csv('GM11.csv') #结果输出
```

生成的数据并读取：

```cobol
data = pd.read_csv('GM11.csv',index_col = 0) #读取数据



feature = ['x3','x5','x7']  # 提取特征
```

 取2020年前的数据进行建模，数据标准化 后进行训练

```cobol
data_train = data.loc[range(2000,2020)] #取2014年前的数据建模



print(data_train)



data_mean = data_train.mean()



data_std = data_train.std()



data_train = (data_train - data_mean)/data_std  #数据标准化 后进行训练



 



x_train = data_train[feature].values #特征数据



y_train = data_train['y'].values #标签数据
```

 建立keras神经网络

```cobol
model = Sequential() #建立模型
model.add(Dense(12,activation='relu',input_dim=3))



model.add(Dense(24,activation='relu'))  # 隐藏层



model.add(Dense(1))  # 输出层



model.compile(loss='mean_squared_error', optimizer='adam') #编译模型



model.fit(x_train, y_train, epochs = 10000, batch_size = 16,verbose=2) #训练模型，训练1000次



model.save_weights('net.model') #保存模型参数
```

 训练过程：

```cobol
Epoch 1/10000



2/2 - 0s - loss: 0.6044 - 421ms/epoch - 210ms/step



Epoch 2/10000



2/2 - 0s - loss: 0.5691 - 996us/epoch - 498us/step



Epoch 3/10000



2/2 - 0s - loss: 0.5365 - 2ms/epoch - 975us/step



Epoch 4/10000



2/2 - 0s - loss: 0.5037 - 3ms/epoch - 1ms/step



Epoch 5/10000



2/2 - 0s - loss: 0.4765 - 2ms/epoch - 997us/step



Epoch 6/10000



2/2 - 0s - loss: 0.4492 - 2ms/epoch - 1ms/step



Epoch 7/10000



2/2 - 0s - loss: 0.4218 - 2ms/epoch - 997us/step



Epoch 8/10000



2/2 - 0s - loss: 0.3960 - 2ms/epoch - 998us/step



Epoch 9/10000



2/2 - 0s - loss: 0.3678 - 993us/epoch - 496us/step



Epoch 10/10000



2/2 - 0s - loss: 0.3436 - 2ms/epoch - 980us/step
```

 模型预测与保存

```cobol
x = ((data[feature] - data_mean[feature])/data_std[feature]).values



data[u'y_pred'] = model.predict(x) * data_std['y'] + data_mean['y']



data.to_csv('result.csv')
```

 预测结果绘图：

```cobol
p = pd.read_csv('result.csv')



p = p[['y','y_pred']].copy()



p.index=range(2000,2023)



p.plot(style=['b-o','r-*'],xticks=p.index,figsize=(15,5))



plt.xlabel("Year")



plt.show()
```

 ![f176f4fc405595fe1766f0be622c2e13](image\f176f4fc405595fe1766f0be622c2e13.png)

可以看出2000-2019预测值基本与真实值吻合，2020-2022预测结果呈现上升趋势。

## 2-(keras框架)企业信用评级与预测

企业信用评级目前是一个完整的体系，包括信用评级的要素和指标、信用评级的等级和标准、信用评级的方法和模型等方面的内容。信用评级指标是根据企业的各个指标进行综合评分，这里涉及的指标维度较多，今天我们来做一个简单的信用评级模型，抽象了几个标准化的数据指标，下面是详细流程：

**一、引入库包**

```typescript
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
import matplotlib .pyplot as plt
import pandas as pd
import csv
```

**二、导入数据**

```cobol
data = pd.read_csv('train_new.csv',encoding = 'utf-8')
```

[数据模型](https://so.csdn.net/so/search?q=数据模型&spm=1001.2101.3001.7020)如下表：

![7344b1810262754f2c1cd214b4e5388e (1)](image\7344b1810262754f2c1cd214b4e5388e (1).png)

 **三、数据预处理**

```
#提取数据特性x1,x2,x3,x4,作为训练集
train = data[['x1','x2','x3','x4']]

#设置标签值  one-hot编码
y_train= np.zeros((len(data),5),dtype = np.int)
for i in range(len(data)):
    y_train[i][data['class'][i]]=1
print(np.array(y_train))
```

数据中"class"字段中0表示"A"级别，1表示"B"级别，2表示"C"级别，3表示"D"级别，4表示"E"级别.

以上数据进行了one-hot 编码，one-hot 编码类似于虚拟变量（dummy variables），是一种将分类变量转换为几个二进制列的方法。其中 1 代表某个输入属于该类别。

**四、构建神经网络模型**

```
model=Sequential()
model.add(Dense(input_dim=4,units=666,activation='relu'))
model.add(Dropout(0.5))  # Dropout(0.5) 表示随机丢弃50%的神经元，防止过拟合
model.add(Dense(units=666,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=666,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=5,activation='softmax')) #输出层 输出5个等级结果
```

**五、训练模型**

```
model.compile(loss='mse',optimizer='adam',metrics=['acc'])
history = model.fit(train,y_train,batch_size=123,epochs=500,validation_split=0.2) #训练500次
```

**六、模型评估**

```
weights = np.array(model.get_weights())
result2=model.evaluate(train,y_train)

#绘制图形函数
def show_train_history(history,train,validation):
    plt.plot(history.history[train])
    plt.plot(history.history[validation])
    plt.title('Train History')
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train','validation'],loc='upper left')
    plt.show()
```

**七、验证集与测试集检验**

```
show_train_history(history,'acc','val_acc')
```

![下载 (43)](image\下载 (43).png)

损失函数图像

```
show_train_history(history,'loss','val_loss')
```

![下载 (44)](image\下载 (44).png)

训练结果：[0.039951469749212265, 0.8893280625343323]

损失值 loss: 0.03995，准确率 acc: 0.8893

**八、测试集检验结果**

```
with open('test.csv', encoding='utf-8-sig') as f:
    reader = csv.reader(f)
    for j in reader:
        test.append(list(map(float,j)))
    test1=np.array(test)
    # print(np.shape(BB))

pre=model.predict(test1)
print(pre)
```

预测结果基本满足项目要求，待读可以自己训练次数多于500，准确率可能会继续提升。

## 3-文本卷积神经网络（TextCNN）新闻文本分类

简单的中文新闻分类模型，利用[TextCNN](https://so.csdn.net/so/search?q=TextCNN&spm=1001.2101.3001.7020)模型进行训练，TextCNN的主要流程是：获取文本的局部特征：通过不同的卷积核尺寸来提取文本的N-Gram信息，然后通过最大池化操作来突出各个卷积操作提取的最关键信息，拼接后通过全连接层对特征进行组合，最后通过交叉熵损失函数来训练模型。

![1a2332e14cf266c27410e94c9e1c8c5f](image\1a2332e14cf266c27410e94c9e1c8c5f.png)

   textCNN的模型架构
注：N-Gram是大词汇连续[语音识别](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB)中常用的一种语言模型。⼜被称为⼀阶马尔科夫链。它的基本思想是将⽂本⾥⾯的内容按照字节进行大小为 N 的滑动窗⼝操作，形成了长度是 N 的字节⽚段序列。每⼀个字节⽚段称为 gram，对所有的 gram 的出现频度进⾏统计，并且按照事先设定好的阈值进⾏过滤，形成关键 gram 列表，是这个⽂本的向量特征空间。列表中的每⼀种 gram 就是⼀个特征向量维度。

**一、前期工作**

1. 设置GPU

如果使用的是CPU可以注释掉这部分的代码。

```
import tensorflow as tf
gpus = tf.config.list_physical_devices("GPU")

if gpus:
    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU
    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用
    tf.config.set_visible_devices([gpu0],"GPU")

#导入库包
import tensorflow.keras as keras
from config import Config
import os
from sklearn import metrics
import numpy as np
from keras.models import Sequential
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding,Dropout,Conv1D,ReLU,GlobalMaxPool1D,InputLayer
```

2. 导入预处理词库类

```
trainingSet_path = "cnews.train.txt"
valSet_path = "cnews.val.txt"
model_save_path = "CNN_model.h5"
testingSet_path = "cnews.test.txt"


#创建 文本处理类：preprocesser
class preprocesser(object):
    def __init__(self):
        self.config = Config()

    # 读取文本txt 函数
    def read_txt(self, txt_path):
        with open(txt_path, "r", encoding='utf-8') as f:
            data = f.readlines()
        labels = []
        contents = []
        for line in data:
            label, content = line.strip().split('\t')
            labels.append(label)
            contents.append(content)
        return labels, contents
    # 读取分词文档
    def get_vocab_id(self):

        vocab_path = "cnews.vocab.txt"
        with open(vocab_path, "r", encoding="utf-8") as f:
            infile = f.readlines()
        vocabs = list([word.replace("\n", "") for word in infile])
        vocabs_dict = dict(zip(vocabs, range(len(vocabs))))
        return vocabs, vocabs_dict

    # 获取新闻属性id 函数
    def get_category_id(self):
     
        categories = ["体育", "财经", "房产", "家居", "教育", "科技", "时尚", "时政", "游戏", "娱乐"]
        cates_dict = dict(zip(categories, range(len(categories))))
        return cates_dict

    #将语料中各文本转换成固定max_length后返回各文本的标签与文本tokens
    def word2idx(self, txt_path, max_length):

        # vocabs:分词词汇表
        # vocabs_dict:各分词的索引
        vocabs, vocabs_dict = self.get_vocab_id()
        # cates_dict:各分类的索引
        cates_dict = self.get_category_id()

        # 读取语料
        labels, contents = self.read_txt(txt_path)
        # labels_idx：用来存放语料中的分类
        labels_idx = []
        # contents_idx:用来存放语料中各样本的索引
        contents_idx = []

        # 遍历语料
        for idx in range(len(contents)):
            # tmp:存放当前语句index
            tmp = []
            # 将该idx(样本)的标签加入至labels_idx中
            labels_idx.append(cates_dict[labels[idx]])
            # contents[idx]:为该语料中的样本遍历项
            # 遍历contents中各词并将其转换为索引后加入contents_idx中
            for word in contents[idx]:
                if word in vocabs:
                    tmp.append(vocabs_dict[word])
                else:
                    # 第5000位设置为未知字符
                    tmp.append(5000)
            # 将该样本index后结果存入contents_idx作为结果等待传回
            contents_idx.append(tmp)

        # 将各样本长度pad至max_length
        x_pad = keras.preprocessing.sequence.pad_sequences(contents_idx, max_length)
        y_pad = keras.utils.to_categorical(labels_idx, num_classes=len(cates_dict))

        return x_pad, y_pad
    
    def word2idx_for_sample(self, sentence, max_length):
        # vocabs:分词词汇表
        # vocabs_dict:各分词的索引
        vocabs, vocabs_dict = self.get_vocab_id()
        result = []
        # 遍历语料
        for word in sentence:
            # tmp:存放当前语句index
                if word in vocabs:
                    result.append(vocabs_dict[word])
                else:
                    # 第5000位设置为未知字符，实际中为vocabs_dict[5000]，使得vocabs_dict长度变成len(vocabs_dict+1)
                    result.append(5000)

        x_pad = keras.preprocessing.sequence.pad_sequences([result], max_length)
        return x_pad

pre = preprocesser() # 实例化preprocesser()类
```

数据集样式：

![下载 (67)](image\下载 (67).png)

**二、参数设定**

```
num_classes = 10     # 类别数
vocab_size = 5000    #语料词大小
seq_length = 600     #词长度

conv1_num_filters = 128   # 第一层输入卷积维数
conv1_kernel_size = 1     # 卷积核数

conv2_num_filters = 64   # 第二层输入卷维数
conv2_kernel_size = 1    # 卷积核数

hidden_dim = 128         # 隐藏层维度
dropout_keep_prob = 0.5  # dropout层丢弃0.5

batch_size = 64     # 每次训练批次数  
```

**四、创建模型**

```
def TextCNN():
    #创建模型序列
    model = Sequential()
    model.add(InputLayer((seq_length,)))
    model.add(Embedding(vocab_size+1, 256, input_length=seq_length))
    model.add(Conv1D(conv1_num_filters, conv1_kernel_size, padding="SAME"))
    model.add(Conv1D(conv2_num_filters, conv2_kernel_size, padding="SAME"))
    model.add(GlobalMaxPool1D())
    model.add(Dense(hidden_dim))
    model.add(Dropout(dropout_keep_prob))
    model.add(ReLU())
    model.add(Dense(num_classes, activation="softmax"))
    model.compile(loss="categorical_crossentropy",
                  optimizer="adam",
                  metrics=["acc"])
    print(model.summary())

    return model
```

![下载 (68)](image\下载 (68).png)

**五、训练模型函数**

```
def train(epochs):

    model = TextCNN()
    model.summary()

    x_train, y_train = pre.word2idx(trainingSet_path, max_length=seq_length)
    x_val, y_val = pre.word2idx(valSet_path, max_length=seq_length)

    model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_val, y_val))

    model.save(model_save_path, overwrite=True)
```

 **六、测试模型函数**

```
def test():

    if os.path.exists(model_save_path):
        model = keras.models.load_model(model_save_path)
        print("-----model loaded-----")
        model.summary()

    x_test, y_test = pre.word2idx(testingSet_path, max_length=seq_length)
    print(x_test.shape)
    print(type(x_test))
    print(y_test.shape)
    # print(type(y_test))
    pre_test = model.predict(x_test)
    # print(pre_test.shape)
    # metrics.classification_report(np.argmax(pre_test, axis=1), np.argmax(y_test, axis=1), digits=4, output_dict=True)
    print(metrics.classification_report(np.argmax(pre_test, axis=1), np.argmax(y_test, axis=1)))
```

**七、训练模型与预测**

```
if __name__ == '__main__':

    train(20)  # 训练模型

Epoch 1/20
782/782 [==============================] - 119s 152ms/step - loss: 0.7380 - accuracy: 0.7696 - val_loss: 0.5568 - val_accuracy: 0.8334
Epoch 2/20
782/782 [==============================] - 122s 156ms/step - loss: 0.3898 - accuracy: 0.8823 - val_loss: 0.4342 - val_accuracy: 0.8588
Epoch 3/20
782/782 [==============================] - 121s 154ms/step - loss: 0.3382 - accuracy: 0.8979 - val_loss: 0.4154 - val_accuracy: 0.8648
Epoch 4/20
782/782 [==============================] - 116s 148ms/step - loss: 0.3091 - accuracy: 0.9055 - val_loss: 0.4408 - val_accuracy: 0.8688
Epoch 5/20
782/782 [==============================] - 117s 150ms/step - loss: 0.2904 - accuracy: 0.9116 - val_loss: 0.3880 - val_accuracy: 0.8844
Epoch 6/20
782/782 [==============================] - 119s 153ms/step - loss: 0.2724 - accuracy: 0.9153 - val_loss: 0.4412 - val_accuracy: 0.8664
Epoch 7/20
782/782 [==============================] - 117s 149ms/step - loss: 0.2601 - accuracy: 0.9206 - val_loss: 0.4217 - val_accuracy: 0.8726
Epoch 8/20
782/782 [==============================] - 116s 149ms/step - loss: 0.2423 - accuracy: 0.9243 - val_loss: 0.4205 - val_accuracy: 0.8760
Epoch 9/20
782/782 [==============================] - 117s 150ms/step - loss: 0.2346 - accuracy: 0.9275 - val_loss: 0.4022 - val_accuracy: 0.8808
Epoch 10/20
782/782 [==============================] - 116s 148ms/step - loss: 0.2249 - accuracy: 0.9301 - val_loss: 0.4297 - val_accuracy: 0.8726
....
   
    model = keras.models.load_model(model_save_path)
    print("-----model loaded-----")
    model.summary()
    test = preprocesser()

    # 测试文本
    x_test = '5月6日，上海莘庄基地田径特许赛在第二体育运动学校鸣枪开赛。男子110米栏决赛，19岁崇明小囡秦伟搏以13.35秒的成绩夺冠，创造本赛季亚洲最佳。谢文骏迎来赛季首秀，以13.38秒获得亚军'
    x_test = test.word2idx_for_sample(x_test, 600)

    categories = ["体育", "财经", "房产", "家居", "教育", "科技", "时尚", "时政", "游戏", "娱乐"]

    pre_test = model.predict(x_test)

    index = int(np.argmax(pre_test, axis=1)[0])

    print('该新闻为:', categories[index])
```

训练20次后，训练集损失函数loss: 0.1635 ，训练集准确率：accuracy: 0.9462

验证集函数：val_loss: 0.4554 验证集准确率 val_accuracy: 0.8820

运行结果：该新闻为: 体育

## 4-卷积神经网络（DenseNet）数学图形识别+题目模式识别

解数学题不是直接算公式，而是需要你去理解文字，此外，有的题还需要联系上下文，并且包括了一些隐含条件。比如，一道概率论的题目，问：“在扑克游戏中，拿到两副对子的概率是多少？”这道题在人看来很清楚，但是对于计算机来说，其实是有很多隐含条件的。比如，一副扑克有54张牌、4种花色+两张鬼牌等等。人工智能不知道这些隐含条件，就没法算题。

### 一、前期工作

#### 1.导入库

```
import tensorflow as tf
# from keras import keras.layers
import matplotlib.pyplot as plt
from time import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,MaxPooling2D,Flatten,Conv2D,experimental
```

#### 2. 数据集加载

```
data_dir = "./data/图形识别训练"
 
batch_size = 12
img_height = 224
img_width = 224

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="training",
        seed=12,
        image_size=(img_height, img_width),
        batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",
        seed=12,
        image_size=(img_height, img_width),
        batch_size=batch_size)
```

### 二、构建DenseNet模型

```
model = tf.keras.applications.DenseNet121(weights='imagenet')
model.summary()

# 设置初始学习率
initial_learning_rate = 1e-3

lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate,
        decay_steps=5,      # 敲黑板！！！这里是指 steps，不是指epochs
        decay_rate=0.96,     # lr经过一次衰减就会变成 decay_rate*lr
        staircase=True)

# 将指数衰减学习率送入优化器
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(optimizer=optimizer,
              loss     ='sparse_categorical_crossentropy',
              metrics  =['accuracy'])
```

### 三、训练过程的曲线函数

```
def show_loss_acc(history):
    # 从history中提取模型训练集和验证集准确率信息和误差信息
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    # 按照上下结构将图画输出
    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.ylabel('Accuracy')
    plt.ylim([min(plt.ylim()), 1])
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.ylabel('Cross Entropy')
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.savefig('results/results_cnn.png', dpi=100)
    plt.show()
```

### 四、训练模型函数

```
def train(epochs):
    # 开始训练，记录开始时间
    begin_time = time()
    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
    # print(class_names)
    # 加载模型
    # model = model_load(class_num=len(class_names))
    # 指明训练的轮数epoch，开始训练
    history = model.fit(train_ds,validation_data=val_ds,epochs=epochs)
    # todo 保存模型， 修改为你要保存的模型的名称
    model.save("models/cnn_fv.h5")
    # 记录结束时间
    end_time = time()
    run_time = end_time - begin_time
    print('该循环程序运行时间：', run_time, "s")  # 该循环程序运行时间： 1.4201874732
    # 绘制模型训练过程图
    show_loss_acc(history)
```

图形识别训练数集里面共有4类：circular(圆形),parabola(抛物线),square(正方形)，triangle(三角形)

![下载 (69)](image\下载 (69).png)

![下载 (70)](image\下载 (70).png)

![下载 (71)](image\下载 (71).png)

### 五、训练模型与结果

```
train(epochs=6)   #训练6次

41/41 [==============================] - 218s 5s/step - loss: 1.5289 - accuracy: 0.7224 - val_loss: 21.6089 - val_accuracy: 0.0410
Epoch 2/6
41/41 [==============================] - 206s 5s/step - loss: 0.2921 - accuracy: 0.9204 - val_loss: 4.2023 - val_accuracy: 0.6393
Epoch 3/6
41/41 [==============================] - 210s 5s/step - loss: 0.0962 - accuracy: 0.9673 - val_loss: 0.4482 - val_accuracy: 0.9180
Epoch 4/6
41/41 [==============================] - 209s 5s/step - loss: 0.0406 - accuracy: 0.9898 - val_loss: 0.0980 - val_accuracy: 0.9672
Epoch 5/6
41/41 [==============================] - 205s 5s/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 0.9918
Epoch 6/6
41/41 [==============================] - 220s 5s/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9918
该循环程序运行时间： 1269.1464262008667 s
```

测试和验证结果准确率图：

![下载 (72)](image\下载 (72).png)

迭代6次后，训练集准确率高达100%，验证集准确率高达99.18%

### 六、验证

输入图片：

![下载 (73)](image\下载 (73).png)

预测结果：parabola(抛物线)
后续将会通过OCR识别题目中的文字，以及Latex数学公式的识别，识别题目含义，最终结果调用抛物线的解题程序，进行简单的解题。

## 5-卷积神经网络（CNN）中文OCR识别项目

![下载 (74)](image\下载 (74).png)

### 一、前期工作

#### 1.导入库

```
import numpy as np 
from PIL import Image, ImageDraw, ImageFont
import cv2
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

#导入字体
DroidSansFallbackFull = ImageFont.truetype("DroidSansFallback.ttf", 36, 0)
fonts = [DroidSansFallbackFull,]
```

#### 2.数据集生成函数

```
#生成图片，48*48大小
def affineTrans(image, mode, size=(48, 48)):
    # print("AffineTrans ...")
    if mode == 0:  # padding移动
        which = np.array([0, 0, 0, 0])
        which[np.random.randint(0, 4)] = np.random.randint(0, 10)
        which[np.random.randint(0, 4)] = np.random.randint(0, 10)
        image = cv2.copyMakeBorder(image, which[0], which[0], which[0], which[0], cv2.BORDER_CONSTANT, value=0)
        image = cv2.resize(image, size)
    if mode == 1:
        scale = np.random.randint(48, int(48 * 1.4))
        center = [scale / 2, scale / 2]
        image = cv2.resize(image, (scale, scale))
        image = image[int(center[0] - 24):int(center[0] + 24), int(center[1] - 24):int(center[1] + 24)]

    return image

#图片处理 除噪
def noise(image, mode=1):
    # print("Noise ...")
    noise_image = (image.astype(float) / 255) + (np.random.random((48, 48)) * (np.random.random() * 0.3))
    norm = (noise_image - noise_image.min()) / (noise_image.max() - noise_image.min())
    if mode == 1:
        norm = (norm * 255).astype(np.uint8)
    return norm
    
#绘制中文的图片
def DrawChinese(txt, font):
    # print("DrawChinese...")
    image = np.zeros(shape=(48, 48), dtype=np.uint8)
    x = Image.fromarray(image)
    draw = ImageDraw.Draw(x)
    draw.text((8, 2), txt, (255), font=font)
    result = np.array(x)
    return result

#图片标准化
def norm(image):
    # print("norm...")
    return image.astype(np.float) / 255
```

#### 3.导入数据

```
char_set = open("chinese.txt",encoding = "utf-8").readlines()[0]
print(len(char_set[0]))  # 打印字的个数
```

#### 4.生成数据集函数

```
# 生成数据：训练集和标签
def Genernate(batchSize, charset):
    # print("Genernate...")
    #    pass
    label = [];
    training_data = [];

    for x in range(batchSize):
        char_id = np.random.randint(0, len(charset))
        font_id = np.random.randint(0, len(fonts))
        y = np.zeros(dtype=np.float, shape=(len(charset)))
        image = DrawChinese(charset[char_id], fonts[font_id])
        image = affineTrans(image, np.random.randint(0, 2))
        # image = noise(image)
        # image = augmentation(image,np.random.randint(0,8))
        image = noise(image)
        image_norm = norm(image)
        image_norm = np.expand_dims(image_norm, 2)

        training_data.append(image_norm)
        y[char_id] = 1
        label.append(y)

    return np.array(training_data), np.array(label)

def Genernator(charset,batchSize):
    print("Generator.....")

    while(1):
        label = [];
        training_data = [];
        for i in range(batchSize):
            char_id = np.random.randint(0, len(charset))
            font_id = np.random.randint(0,len(fonts))
            y = np.zeros(dtype=np.float,shape=(len(charset)))
            image = DrawChinese(charset[char_id],fonts[font_id])
            image = affineTrans(image,np.random.randint(0,2))
            #image = noise(image)
            #image = augmentation(image,np.random.randint(0,8))
            image = noise(image)
            image_norm = norm(image)
            image_norm  = np.expand_dims(image_norm,2)
            y[char_id] = 1
            training_data.append(image_norm)
            label.append(y)

        y[char_id] = 1
        yield (np.array(training_data),np.array(label))
```

### 二、CNN模型建立

```
def Getmodel(nb_classes):

    img_rows, img_cols = 48, 48
    nb_filters = 32
    nb_pool = 2
    nb_conv = 4

    model = Sequential()
    print("sequential..")
    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
                            padding='same',
                            input_shape=(img_rows, img_cols, 1)))
    print("add convolution2D...")
    model.add(Activation('relu'))
    print("activation ...")
    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
    model.add(Dropout(0.25))
    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,padding='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(nb_classes))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model

#评估模型
def eval(model, X, Y):
    print("Eval ...")
    res = model.predict(X)
```

### 三、训练模型函数

```
#训练函数
def Training(charset):
    model = Getmodel(len(charset))
    while (1):
        X, Y = Genernate(64, charset)
        model.train_on_batch(X, Y)
        print(model.loss)

#训练生成模型
def TrainingGenerator(charset, test=1):

    set = Genernate(64, char_set)
    model = Getmodel(len(charset))
    BatchSize = 64
    model.fit_generator(generator=Genernator(charset, BatchSize), steps_per_epoch=BatchSize * 10, epochs=15,
                        validation_data=set)

    model.save("ocr.h5")

    X = set[0]
    Y = set[1]
    if test == 1:
        print("============6 Test == 1 ")
        for i, one in enumerate(X):
            x = one
            res = model.predict(np.array([x]))
            classes_x = np.argmax(res, axis=1)[0]
            print(classes_x)

            print(u"Predict result：", char_set[classes_x], u"Real result：", char_set[Y[i].argmax()])
            image = (x.squeeze() * 255).astype(np.uint8)
            cv2.imwrite("{0:05d}.png".format(i), image)
```

### 四、训练模型与结果

```
TrainingGenerator(char_set)  #函数TrainingGenerator 开始训练
Epoch 1/15
640/640 [==============================] - 63s 76ms/step - loss: 8.1078 - accuracy: 3.4180e-04 - val_loss: 8.0596 - val_accuracy: 0.0000e+00
Epoch 2/15
640/640 [==============================] - 102s 159ms/step - loss: 7.5234 - accuracy: 0.0062 - val_loss: 6.2163 - val_accuracy: 0.0781
Epoch 3/15
640/640 [==============================] - 38s 60ms/step - loss: 5.9793 - accuracy: 0.0425 - val_loss: 4.1687 - val_accuracy: 0.3281
Epoch 4/15
640/640 [==============================] - 45s 71ms/step - loss: 5.0450 - accuracy: 0.0889 - val_loss: 3.1590 - val_accuracy: 0.4844
Epoch 5/15
640/640 [==============================] - 37s 58ms/step - loss: 4.5251 - accuracy: 0.1292 - val_loss: 2.5326 - val_accuracy: 0.5938
Epoch 6/15
640/640 [==============================] - 38s 60ms/step - loss: 4.1708 - accuracy: 0.1687 - val_loss: 1.9666 - val_accuracy: 0.7031
Epoch 7/15
640/640 [==============================] - 35s 54ms/step - loss: 3.9068 - accuracy: 0.1951 - val_loss: 1.8039 - val_accuracy: 0.7812
...

910
Predict result： 妻 Real result： 妻
1835
Predict result： 莱 Real result： 莱
3107
Predict result： 阀 Real result： 阀
882
Predict result： 培 Real result： 培
1241
Predict result： 鼓 Real result： 鼓
735
Predict result： 豆 Real result： 豆
1844
Predict result： 巾 Real result： 巾
1714
Predict result： 跌 Real result： 跌
2580
Predict result： 骄 Real result： 骄
1788
Predict result： 氧 Real result： 氧
```

生成字体图片：

![下载 (75)](D:\learn\笔记\人工智能\人工智能笔记\image\下载 (75).png)

### 五、验证

```
model = tf.keras.models.load_model("ocr.h5")
img1 = cv2.imread('00001.png',0)
img = cv2.resize(img1,(48,48))

print(img.shape)
img2 = tf.expand_dims(img, 0)
res = model.predict(img2)
classes_x = np.argmax(res, axis=1)[0]
print(classes_x)
print(u"Predict result：", char_set[classes_x]) 
```

中文字：

![下载 (76)](image\下载 (76).png)

预测结果为”莱；

## 6-卷积神经网络(Pytorch)+聚类分析实现空气质量与天气预测

我们知道雾霾天气是一种大气污染状态，PM2.5被认为是造成雾霾天气的“元凶”，PM2.5日均值越小，空气质量越好．
空气质量评价的主要污染物为细颗粒物(PM2.5)、可吸入颗粒物(PM10)、二氧化硫(SO2)、二氧化氮(NO2)、臭氧(O3)、一氧化碳(CO)等六项。

![下载 (77)](image\下载 (77).png)

现在我们收集了多个城市的天气指标数据，数据样例如下：

![下载 (78)](image\下载 (78).png)

### 一、前期工作

#### 1. 导入库包

```
import torch
import torch.nn as nn
import torch.utils.data as Data
import numpy as np
import pymysql
import datetime
import csv
import time
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
```

#### 2. 导入数据

```
data =pd.read_csv("weather.csv",encoding='gb18030')
data = data.drop(columns=data.columns[-1])
print(data)
```

#### 3. 主成分分析(PCA)

天气数据中变量有6个，这增加分析问题的难度与复杂性，而且数据中多个变量之间是具有一定的相关关系的。 因此，我们想到能否在相关分析的基础上，用较少的新变量代替原来较多的旧变量，这里就采用了主成分分析原理（PCA）,PCA的基本思想就是降维。下面代码将原理6个变量通过变化映射成两个新变量。

```
pca = PCA(n_components=2)
new_pca = pd.DataFrame(pca.fit_transform(data))
X = new_pca.values
print(new_pca)
```

#### 4. 聚类分析(K-means)

K-means是一种迭代求解的聚类分析算法，主要步骤是：我们将设定分组类K，系统随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象重新计算。这个过程将不断重复直到满足某个终止条件。聚类算法这边直接调用sklearn.cluster中的KMeans算法。当数据没有类别标注的时候，我们可以采用无监督学习聚类分析进行标注每条数据的簇类。

```
kms = KMeans(n_clusters=6)  # 6表示聚类的个数
#获取类别标签
Y= kms.fit_predict(data)
data['class'] = Y
data.to_csv("weather_new.csv",index=False) #保存文件

#绘制聚类发布图
d = new_pca[Y == 0]
plt.plot(d[0], d[1], 'r.')
d = new_pca[Y == 1]
plt.plot(d[0], d[1], 'g.')
d = new_pca[Y == 2]
plt.plot(d[0], d[1], 'b.')
d = new_pca[Y == 3]
plt.plot(d[0], d[1], 'y.')
d = new_pca[Y == 4]
plt.plot(d[0], d[1],'c.')
d = new_pca[Y == 5]
plt.plot(d[0], d[1],'k.')
plt.show()
```

![下载 (79)](image\下载 (79).png)

图中将数据用不同颜色分为6类，从图中可直观得看到数据位置相近的分为一类。

### 二、神经网络模型建立

```
class MyNet(nn.Module):
    def __init__(self):
        super(MyNet, self).__init__()
        self.con1 = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.MaxPool1d(kernel_size=1),
            nn.ReLU(),
        )
        self.con2 = nn.Sequential(
            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.MaxPool1d(kernel_size=1),
            nn.ReLU(),
        )
        self.fc = nn.Sequential(
            # 线性分类器
            nn.Linear(128*6*1, 128),  
            nn.ReLU(),
            nn.Linear(128, 6),
            # nn.Softmax(dim=1),
        )
        self.mls = nn.MSELoss()
        self.opt = torch.optim.Adam(params=self.parameters(), lr=1e-3)
        self.start = datetime.datetime.now()

    def forward(self, inputs):
        out = self.con1(inputs)
        out = self.con2(out)
        out = out.view(out.size(0), -1)  # 展开成一维
        out = self.fc(out)
        return out

    def train(self, x, y):
        out = self.forward(x)
        loss = self.mls(out, y)
        self.opt.zero_grad()
        loss.backward()
        self.opt.step()

        return loss

    def test(self, x):
        out = self.forward(x)
        return out

    def get_data(self):
        with open('weather_new.csv', 'r') as f:
            results = csv.reader(f)
            results = [row for row in results]
            results = results[1:1500]
        inputs = []
        labels = []
        for result in results:
            # one-hot独热编码
            one_hot = [0 for i in range(6)]
            index = int(result[6])-1
            one_hot[index] = 1
            labels.append(one_hot)
            input = result[:6]
            input = [float(x) for x in input]
            
            inputs.append(input)
        
        inputs = np.array(inputs)
        labels = np.array(labels)
        inputs = torch.from_numpy(inputs).float()
        inputs = torch.unsqueeze(inputs, 1)

        labels = torch.from_numpy(labels).float()
        return inputs, labels

    def get_test_data(self):
        with open('weather_new.csv', 'r') as f:
            results = csv.reader(f)
            results = [row for row in results]
            results = results[1500: 1817]
        inputs = []
        labels = []
        for result in results:
            label = [result[6]]
            input = result[:6]
            input = [float(x) for x in input]
            label = [float(y) for y in label]
            inputs.append(input)
            labels.append(label)
        inputs = np.array(inputs)
        
        inputs = torch.from_numpy(inputs).float()
        inputs = torch.unsqueeze(inputs, 1)
        labels = np.array(labels)
        labels = torch.from_numpy(labels).float()
        return inputs, labels
```

### 三、训练模型

```
EPOCH = 100
BATCH_SIZE = 50

net = MyNet()
x_data, y_data = net.get_data()
torch_dataset = Data.TensorDataset(x_data, y_data)
loader = Data.DataLoader(
        dataset=torch_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2,
    )
for epoch in range(EPOCH):
        for step, (batch_x, batch_y) in enumerate(loader):
            # print(step)
            # print(step,'batch_x={};  batch_y={}'.format(batch_x, batch_y))
            a = net.train(batch_x, batch_y)
            print('step:',step,a)
 # 保存模型
torch.save(net, 'net.pkl')
```

开始训练：

```
step: 0 tensor(3.6822, grad_fn=<MseLossBackward0>)
step: 1 tensor(61.2186, grad_fn=<MseLossBackward0>)
step: 2 tensor(18.2877, grad_fn=<MseLossBackward0>)
step: 3 tensor(4.3641, grad_fn=<MseLossBackward0>)
step: 4 tensor(6.7846, grad_fn=<MseLossBackward0>)
step: 5 tensor(9.4255, grad_fn=<MseLossBackward0>)
step: 6 tensor(5.4232, grad_fn=<MseLossBackward0>)
step: 7 tensor(4.1342, grad_fn=<MseLossBackward0>)
step: 8 tensor(2.0944, grad_fn=<MseLossBackward0>)
step: 9 tensor(1.4549, grad_fn=<MseLossBackward0>)
step: 10 tensor(0.9372, grad_fn=<MseLossBackward0>)
step: 11 tensor(1.0688, grad_fn=<MseLossBackward0>)
step: 12 tensor(0.6717, grad_fn=<MseLossBackward0>)
step: 13 tensor(0.6158, grad_fn=<MseLossBackward0>)
step: 14 tensor(0.6889, grad_fn=<MseLossBackward0>)
step: 15 tensor(0.5306, grad_fn=<MseLossBackward0>)
step: 16 tensor(0.5781, grad_fn=<MseLossBackward0>)
step: 17 tensor(0.3959, grad_fn=<MseLossBackward0>)
step: 18 tensor(0.4629, grad_fn=<MseLossBackward0>)
step: 19 tensor(0.3646, grad_fn=<MseLossBackward0>)
```

### 四、检验模型

```
# 加载模型
net = torch.load('net.pkl')
x_data, y_data = net.get_test_data()
torch_dataset = Data.TensorDataset(x_data, y_data)
 loader = Data.DataLoader(
        dataset=torch_dataset,
        batch_size=100,
        shuffle=False,
        num_workers=1,
    )
num_success = 0
num_sum = 317
for step, (batch_x, batch_y) in enumerate(loader):
     # print(step)
     output = net.test(batch_x)
     # output = output.detach().numpy()
     y = batch_y.detach().numpy()
     for index, i in enumerate(output):
         i = i.detach().numpy()
         i = i.tolist()
         j = i.index(max(i))
         print('输出为{}标签为{}'.format(j+1, y[index][0]))
         loss = j+1-y[index][0]
         if loss == 0.0:
             num_success += 1
     print('正确率为{}'.format(num_success/num_sum))
```

**输出结果：**

```
....
输出为3标签为3.0
输出为4标签为4.0
输出为5标签为5.0
输出为1标签为1.0
输出为3标签为3.0
输出为3标签为3.0
输出为3标签为3.0
输出为4标签为4.0
正确率为0.9495268138801262
```

模型预测结果为94.95%;
数据中字段“fcm”分类，表示数据的类别：
数字1表示：空气质量：优；
数字2表示：空气质量：良；
数字3表示：空气质量：轻度污染；
数字4表示：空气质量：中度污染，雾霾
数字5表示：空气质量：重度污染，雾霾
数字6表示：空气质量：严重污染，雾霾

## 7-电商产品评论的情感分析

在这种电商平台激烈竞争的大背景下，除了提高商品质量、压低商品价格外。了解更多消费者的心声对干店商平台来说也变得越来越有必要，其中非常重要的方式就是对消费者的文本评论等非结构化的数据进行内在信息的数据挖掘和分析，有利于对应商品的生产厂家自身竞争力的提升。
对某商品的评论进行文本挖掘分析，目的是分析用户对某商品的情感倾向，从商品评论中挖掘产品的优点和缺点，提炼出不同品牌商品的卖点。

### 一、前期工作

#### 1. 导入库包

```
import data_loader
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import Flatten
from tensorflow.keras.utils import to_categorical
import numpy as np
```

代码中导入data_loader模型，做数据处理和导入，详细代码包和数据集：https://pan.baidu.com/s/14Do1RrZXOzZokNAcX4Yz2A
提取码：wxai

#### 2. 导入数据

```
x_train,y_train,x_test,y_test =data_loader.load_data()
```

这里选取了某电商平台的各种商品的评论数据，数据样例：

![下载 (80)](image\下载 (80).png)

#### 3. 数据处理

```
#创建评论数据的词库索引
vocalen,word_index = data_loader.createWordIndex(x_train,x_test)
print(vocalen)

#获取训练数据每个词的索引
x_train_index =data_loader.word2Index(x_train,word_index)
x_test_index=data_loader.word2Index(x_test,word_index)

#最大长度的限制
maxlen =25
x_train_index =sequence.pad_sequences(x_train_index,maxlen=maxlen )
x_test_index =sequence.pad_sequences(x_test_index,maxlen=maxlen)
y_train= to_categorical(y_train)
y_test= to_categorical(y_test)
```

### 二、神经网络模型构建

```
model =Sequential()
model.add(Embedding(trainable=False, input_dim= vocalen+1, output_dim=300, input_length=maxlen))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation= 'relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation= 'sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy']) #二分类问题
```

### 三、训练模型与测试

```
model.fit(x_train_index, y_train,batch_size=512, epochs=200)
score, acc = model.evaluate(x_test_index, y_test)
print('Test score:', score)
print('test accuracy:',acc)

test = np.array([x_test_index[1000]])
print(test)
print(test.shape)

predict = model.predict(test)
print(predict)
print(np.argmax(predict,axis=1))
```

运行结果：
Test score: 1.740
test accuracy: 0.8563
在测试集的准确率为0.8563
机器借助模型可以对评论标注情感分析的标签

## 8-生活照片转化漫画照片应用

### **一、论文介绍**

![下载 (45)](image\下载 (45).png)



![下载 (46)](image\下载 (46).png)

### **二、**漫画图片生成原理

主要方法是构建一下生成模型框架，其中 xp 表示输入照片， xc 表示输入的卡通图像，图像通过Modeling网络层后，经过多个S-AdaIN层，输出 xp′ 和 xc′ ，其中 xp′和 xc′表示重建结果， y 表示漫画化的目标结果。

![下载 (47)](image\下载 (47).png)

图像生成的过程就是利用对抗神经网络原理构建。本模型是利用CartonRenderer自动编码器，模型网络将输入图像映射到特征空间。与Adain 6和MUNIT 7中使用的传统编码器不同，我们的建模网络将输入图像映射到多尺度特征空间，是单个固定比例要素空间的。CartonRenderer的参数优化部分是由四个S-AdaIN块组成，对应于特征模型。每个S-AdaIN块用于对齐相应的刻度。其过程还是相对复杂的。

![下载 (48)](image\下载 (48).png)

### 三**、**代码部分

快速开始：安装

pip install "modelscope[cv]" -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html

代码部分：

```
import cv2
from modelscope.outputs import OutputKeys
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

img_cartoon = pipeline(Tasks.image_portrait_stylization,
                       model='damo/cv_unet_person-image-cartoon_compound-models',
                       device='cpu')
# 图像本地路径
img_path = 'input.png'

result = img_cartoon(img_path)
cv2.imwrite('result.png', result[OutputKeys.OUTPUT_IMG])
print('完成!')
```

### 四、生成效果

以下是风景图的漫画风格生成效果：

![下载 (49)](image\下载 (49).png)

![下载 (50)](image\下载 (50).png)

以下是人物的漫画风格生成效果：效果还是挺好的

![下载 (51)](image\下载 (51).png)

大家要学习深度学习图像处理、识别方向，需要熟悉以下模型：VGG-16、ResNet-50 、 Xception、Inception-v4、Inception-ResNet-V2、ResNeXt-50 、RegNet、ConvNeX，这些模型都是近几年流行的图像分类识别模型

## 9-文本生成图像-本地电脑实现text2img

**Stable Diffusion模型**包括两个步骤：

前向扩散——通过逐渐扰动输入数据将数据映射到噪声。这是通过一个简单的随机过程正式实现的，该过程从数据样本开始，并使用简单的高斯扩散核迭代地生成噪声样本。此过程仅在训练期间使用，而不用于推理。

参数化反向——撤消前向扩散并执行迭代去噪。这个过程代表数据合成，并被训练通过将随机噪声转换为真实数据来生成数据。

模型构架：

![下载 (52)](image\下载 (52).png)

下面介绍一下，Stable Diffusion模型的代码实现，主代码：

```
from tensorflow import keras
from stable_diffusion_tf.stable_diffusion import StableDiffusion
import argparse
from PIL import Image
import os
parser = argparse.ArgumentParser()

parser.add_argument(
    "--prompt",
    type=str,
    nargs="?",
    default="Romantic lavender and sunset",
    help="the prompt to render",
)

parser.add_argument(
    "--output",
    type=str,
    nargs="?",
    default="output4.png",
    help="where to save the output image",
)

parser.add_argument(
    "--H",
    type=int,
    default=256,
    help="image height, in pixels",
)

parser.add_argument(
    "--W",
    type=int,
    default=512,
    help="image width, in pixels",
)

parser.add_argument(
    "--scale",
    type=float,
    default=7.5,
    help="unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))",
)

parser.add_argument(
    "--steps", type=int, default=50, help="number of ddim sampling steps"
)

parser.add_argument(
    "--seed",
    type=int,
    help="optionally specify a seed integer for reproducible results",
)

parser.add_argument(
    "--mp",
    default=False,
    action="store_true",
    help="Enable mixed precision (fp16 computation)",
)

args = parser.parse_args()

if args.mp:
    print("Using mixed precision.")
    keras.mixed_precision.set_global_policy("mixed_float16")

# 引入StableDiffusion模型
generator = StableDiffusion(img_height=args.H, img_width=args.W, jit_compile=False)
img = generator.generate(
    args.prompt,
    num_steps=args.steps,
    unconditional_guidance_scale=args.scale,
    temperature=1,
    batch_size=1,
    seed=args.seed,
)

Image.fromarray(img[0]).save(args.output)
print(f"saved at {args.output}")
```

代码下载地址：

链接：[https://pan.baidu.com/s/1ES7Vr_gla5hwPmdkUdR8Qg?pwd=wqmb](https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1ES7Vr_gla5hwPmdkUdR8Qg%3Fpwd%3Dwqmb)

提取码：wqmb

运行案例，大家用GPU跑会快一点。

描述语言: A girl carrying a bag faces the sea as the sun sets（一个女孩背着包面向大海，夕阳西下）

![下载 (53)](image\下载 (53).png)

## 10-数学公式识别-将图片转换为Latex(img2Latex)

关于数学公式识别的实战案例，解决大家在写论文中遇到很多latex输入的问题，而且可以无限次识别哦，因为是代码实现，不用调用外部API.

以前我们知道一个latex识别网页，latex识别网页神器：[https://snip.mathpix.com/](https://link.zhihu.com/?target=https%3A//snip.mathpix.com/)，但是这个识别是有次数限制的，我们如果需要大量的识别的话，这个是不适用的。这个功能识别效果准确率达98%，每个月可识别50次，识别pdf文件20页每月。识别效果：

![下载 (54)](image\下载 (54).png)

今天我来大家实现一个用代码实现数学公式识别的Latexocr模型，实现数学公式识别，可支持一部分的数学手写功能。他是基于本地程序是完全免费的，可以无限次调用。

接下来我将会用两种方法进行latex识别，第二种方法支持中文和公式一起识别。

**方法一：**

官方地址：[https://github.com/lukas-blecher/LaTeX-OCR/](https://link.zhihu.com/?target=https%3A//github.com/lukas-blecher/LaTeX-OCR/)

文件结构：

![下载 (55)](image\下载 (55).png)

运行程序之前，我们要下载训练好的权重参数文件weights.pth，可以私信发给大家，或者

download weights v0.0.1 to path '本地地址信息'， 下载好的权重文件放在checkpoints 文件夹下面。

安装库脚本：pip3 install pix2tex[gui] -i [https://pypi.tuna.tsinghua.edu.cn/simple](https://link.zhihu.com/?target=https%3A//pypi.tuna.tsinghua.edu.cn/simple) some-package

安装好后，我们可以直接运行 gui.py，我们就可以直接通过截图识别啦。识别效果如下：

![下载 (56)](image\下载 (56).png)

可以将latexOCR功能单独拿出，做批量识别。python [pix2tex.py](https://link.zhihu.com/?target=http%3A//pix2tex.py/) -f 123.png , 可加入你要识别的图片名称，可返回Latex结果。

**方法二：**

安装pix2text: pip install pix2text

启动以下代码；会自动下载权重参数。

```
from pix2text import Pix2Text

img_fp = 'gongshi6.png'
p2t = Pix2Text(analyzer_config=dict(model_name='mfd'))
outs = p2t(img_fp, resized_shape=600)  # 也可以使用 `p2t.recognize(img_fp)` 获得相同的结果

# 如果只需要识别出的文字和Latex表示，可以使用下面行的代码合并所有结果
only_text = '\n'.join([out['text'] for out in outs])

print(only_text)
```

图片样例：gongshi6.png



![下载 (57)](image\下载 (57).png)

识别结果：

$$
a\sin x+b\cos x={\sqrt{a^{2}+b^{2}}}\sin\left(x+a r c\tan{\frac{b}{a}}\right)
$$

![a\sin x+b\cos x={\sqrt{a^{2}+b^{2}}}\sin\left(x+a r c\tan{\frac{b}{a}}\right)](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYEAAAAZBAMAAAAlNY+NAAAAMFBMVEX///8AAADMzMx2dnaIiIiqqqoyMjLc3NyYmJju7u5UVFS6uroQEBBEREQiIiJmZmZFmHsKAAAFWklEQVRYCc1WW2gcVRj+dnZmZye7yW6qIQktZoKlQi1xilXbB8vaLlKClC1aKaXIQoUiCE7qi1ZipjTqVqTZpVDFlnYqUmhfulWwfehDaBOwD8UtfdAHQ9Ia8FLBVVJNKQH/c87c9pZkFwMemDP/+f7/+y/nNgOsRNvYuVirjWjVQksi+0wguqRViwaxcnNE2WrOnlufol4xWiAuh9JO00Nt7aDO38GuHob1QYvlynuZ4cNK6w165TgwUm81M60iIy0l3Qi4AIb74GPSNV/3sy8uX1oY04HTlfYDBDVqoUwjTR38e45JSayuVHbBx6S7nk4peeIiglhX3+CeRLREygcaSO0O3p9vYFAHlmwB6lq3EJwMpV7AwwK8SCYwgBsxiJEsVRgB2riSJXSmyqx26PrbWatqiLTrjiphCMGpQJ6hoYs5Juw1lA8MGlUQrqogZnNPgb1ITt4zg56E7FYwXatqiAy7mjFHcCpoL9PYxdTLrhWOeRIT3IgVIHC4qgLJPp0nk/30SJvSBnDllbR61lo3uWeQ4e+MyU4k199E7gGgzqXzeDtnIPZBOkXDXHlHmayr2xcEPJ7ejlhKE6qSeMV3jOU5xuiHeqQ3d4+eZ5oL9Pi+3IgsL+VjZczE8ZwF5c/pB1ByD+Gx5p5j3AV6XsWQBdXG14hb6DPbKCfNai/tYpV4MyI9gkFggxm1pSzezx9CzAA+w5RCWxsLU6w5aVIyJClZtQdrP/2I+4Cj6jdkm2Oc/i1C/+AiM3iLHteXF5HnFY2fL2Rib6i0V9hVshcduscSrl+ibLsRMhHLQkaHhd/ANmsUEUMVFs6MRGewjy1ZokRGbeVvgBRgEWAKM7c/QXESOlAw0a11diYFXhKvY1BucIzTf0GHDX68fiK178uJyPO6XMioOKWrhqhgH4X2WMLnYaAti6s0+P0+WAWzoFyhIVRkBompqR9pdkki5gvQukjqNxEd7/guQ2K++raEtDMLnCDVOcTG6cXawtTUTbFCFyEnOcTpVEFKHIEJAh1fgYg8r0ukOsc5bA2w9kPbZYXov4W87QLiKTxPuuN3rvkVUJ6cRZ0zI/EMS4pd77S7or3a1jU6yXS7V7XoKvGdnEC45KsccZJNF2uc7lfQwzDPlxuR58VSe5Sp+S46YSS8CjjIKxgq4nXau3nt72AFLzoGbgUFnRy5ayAn38WZLFnIrKs8BzehEaiuRiTFVKKJCrRVCKU4wOl+BWwNXF/enIm8WGrisxjKRDGNhG26K8c9sTUomNrqKJQM4n4FKh2uk8LCmZEQlHHgD6BIx73N3g3Q0IyUtaIw8/qNJru4aRMXipYHigqQZGeJNU73K2DnwPflRBR50ZKyz+ITtFcyMrrQbluVFXxCm78Y7v0Kygw+Zztsgp/kSwf3s5PKmuMvYp7W2V2kWbFfMZB/DZqNWPc6Q+ZGga5QZscA97DxpO7BTgUHtLsC4vRZtoH5Sb6AoC8nIs8rliTCD5AsSsR4EtcxUkq5LOHqWVqkudTTdN2O5rZIZ2fXrzG29ukYyZ3J5YWF40+dZ7ev+uUe4Kn0FuzZcSQPdf7o/FFh5feJ3mfYYCQd3uyDTgUjR0yBMfr6x7advR6+8xchtGUDvtyILK+oTeozuW3Uq7dTGN6c2X/QZRFG7bZ4LdKHF9HVU6l92VrYqIWCiHdtcLC5iJNBR/+NfMtq2k9cb5pCBHn7AVqZZCvUxTkDi6vracPleuhS2CaNspdboi7lumm9NtM0hWa/VyrRPWS2QF0Bysst+FRmZAMYbYG5EpQ2vXmvMTtyhf9ENk9dCQb9uTfd5kfvI5FvmvY/JPwLgsZaFyfuz3EAAAAASUVORK5CYII=)

该模型可以识别中文与公式结合的图片，后期会更加优化的。

## 11-BERT模型的微调应用-文本分类案例

今天给大家带来一个基于BERT模型做文本分类的实战案例，在BERT模型基础上做微调，训练自己的数据集，相信之前大家很多都是套用别人的模型直接训练，或者直接用于预训练模型进行预测，没有训练和微调过大模型，因为像BERT这种大模型一般人是训练不了的，我们只能在大模型的基础上进行微调，或者做下游任务改造。

下面来介绍一下BERT模型，BERT是基于transfomer的预训练语言模型，它利用了transfomer中的编码器，进行数据编码，将文本数据转化为词向量。BERT核心内容是利用transfomer中的多头自注意力机制进行编码，关于transfomer的多头自注意力机制详细可以观看网络上的资料。

BERT模型是以两个NLP任务进行训练的，第一个任务是文本中词的预测，将已知训练文本隐掉词的信息，用MASK进行隐码，让模型去预测。第二个任务是在训练数据中随机抽取上下文关系句子或非上下文关系句子，让机器判断是否为上下文关系。BERT模型训练优势是无需进行标注数据。
我们可以利用BERT预训练模型进行下游任务改造，做自己相关任务，比如中文分词、文本分类，命名实体识别，阅读理解，情感分析，文本相似度、信息抽取等任务。

![下载 (58)](image\下载 (58).png)

```
nsformers import BertTokenizer

#加载字典和分词工具
token = BertTokenizer.from_pretrained('bert-base-chinese')
```

#### 2. 定义数据、数据载入

```
#定义数据集
class Dataset(torch.utils.data.Dataset):
    def __init__(self, split):
        self.dataset = load_dataset(path='data', split=split)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        text = self.dataset[i]['text']
        label = self.dataset[i]['label']

        return text, label

dataset = Dataset('train')
print(len(dataset), dataset[0])

def collate_fn(data):
    sents = [i[0] for i in data]
    labels = [i[1] for i in data]

    #编码
    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,
                                   truncation=True,
                                   padding='max_length',
                                   max_length=500,
                                   return_tensors='pt',
                                   return_length=True)

    #input_ids:编码之后的数字
    #attention_mask:是补零的位置是0,其他位置是1
    input_ids = data['input_ids']
    attention_mask = data['attention_mask']
    token_type_ids = data['token_type_ids']
    labels = torch.LongTensor(labels)

    #print(data['length'], data['length'].max())
    return input_ids, attention_mask, token_type_ids, labels

#数据加载器
loader = torch.utils.data.DataLoader(dataset=dataset,
                                     batch_size=10,
                                     collate_fn=collate_fn,
                                     shuffle=True,
                                     drop_last=True)

for i, (input_ids, attention_mask, token_type_ids,
        labels) in enumerate(loader):
    break

print(len(loader))
print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels)
```

这里代码需要在同级文件夹下创建data 文件夹， 放入train.csv、test.csv数据集。

数据集格式如下：

![下载 (59)](image\下载 (59).png)

### 二、BERT模型加载

我们可以在BERT输出端接入一个全连接层，输出2分类问题，也可加入CNN卷积层，这些可以自行操作。

```
from transformers import BertModel

#加载预训练模型
pretrained = BertModel.from_pretrained('bert-base-chinese')

#不训练,不需要计算梯度
for param in pretrained.parameters():
    param.requires_grad_(False)

#模型试算
out = pretrained(input_ids=input_ids,
           attention_mask=attention_mask,
           token_type_ids=token_type_ids)

print(out.last_hidden_state.shape)


#定义下游任务模型
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(768, 2)
        # 可加入CNN卷积层，可以自行操作
        # self.conv1D = torch.nn.Conv1d(in_channels=500, out_channels=500, kernel_size=1)
        # self.MaxPool1D = torch.nn.MaxPool1d(4, stride=2)
        # self.Dropout = torch.nn.Dropout(p=0.5, inplace=False)

    def forward(self, input_ids, attention_mask, token_type_ids):
        with torch.no_grad():
            out = pretrained(input_ids=input_ids,
                       attention_mask=attention_mask,
                       token_type_ids=token_type_ids)
        out = self.fc(out.last_hidden_state[:, 0])
        out = out.softmax(dim=1)
        print(out.shape)
        return out
```

### 三、模型训练

```
model = Model()
print(model)
#model.summary()
model(input_ids=input_ids,
      attention_mask=attention_mask,
      token_type_ids=token_type_ids).shape

from transformers import AdamW
#训练
optimizer = AdamW(model.parameters(), lr=5e-4)
criterion = torch.nn.CrossEntropyLoss()

model.train()
epochs = 30

for i, (input_ids, attention_mask, token_type_ids,
        labels) in enumerate(loader):
    out = model(input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids)

    loss = criterion(out, labels)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    if i % 1 == 0:
        out = out.argmax(dim=1)
        accuracy = (out == labels).sum().item() / len(labels)

        print('epochs:',i, 'loss:',loss.item(),'accuracy:', accuracy)

    if i == epochs:
        torch.save(model, 'text_classfiy.model')
        #model_load = torch.load('model/命名实体识别_中文.model')
        break
```

### 四、模型测试

```python
#测试函数
def test():
    model.eval()
    correct = 0
    total = 0

    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),
                                              batch_size=10,
                                              collate_fn=collate_fn,
                                              shuffle=True,
                                              drop_last=True)

    for i, (input_ids, attention_mask, token_type_ids,
            labels) in enumerate(loader_test):

        if i == 5:
            break

        with torch.no_grad():
            out = model(input_ids=input_ids,
                        attention_mask=attention_mask,
                        token_type_ids=token_type_ids)

        out = out.argmax(dim=1)
        correct += (out == labels).sum().item()
        total += len(labels)

    print(correct / total)
```

可以调用测试函数进行测试，看看模型训练效果。

## 12-利用Dewarp实现文本扭曲矫正

我们在生活中会看到一些拍摄扭曲的图片，我们在通过OCR识别的时候，因为扭曲的厉害，而无法识别，我们需要对图片进行处理。

文件图像的变形有扭曲、折叠、褶皱、透视等多种情况，解决方案可以分为参数化方法和非参数化方法。参数化方法构建只能处理简单场景的低维度的数学模型。在非参数方法中，通常需要创建一对数据集。

假设文档变形用低维参数化模型表示，不能处理失真情况，不能处理折叠变形。参数模型主要考虑参数。
1、旋转矢量r和平移矢量t的三维空间中的页面
2、指定页面表面的两个曲率alpha和beta，并将行文本视为一条曲线。这里假设扭曲后成为三次样条线，由两个系数a和b控制。
3、页面上n个水平跨度的垂直偏移。垂直偏移是分割的每行之间的距离。
4.对于每个跨度，水平跨度上m个点的水平偏移。水平偏移是每条直线上设置的点之间的距离。

![download](image\download.png)

我们可以利用python，安装第三方库：

pip install page-dewarp

外部调用from page_dewarp import __main__，

```
#__main__.main(img)   # 图片文字扭曲处理
```

简单地使用扭曲处理：

```
from page_dewarp import page_dewarp
 
# 输入图片路径
input_image = 'path/to/your/image.jpg'
 
# 进行去扭曲处理
page_dewarp(input_image)
```

扭曲处理的main函数代码详细介绍：

```
from cv2 import namedWindow
import cv2
from .cli import ArgParser
from .debug_utils import cCOLOURS, debug_show
from .image import WarpedImage
from .options import cfg
from .pdf import save_pdf

# for some reason pylint complains about cv2 members being undefined :(
# pylint: disable=E1101


def main(imgfile):
    outfiles = []
    # print(imgfile)
    src = cv2.imread(imgfile)
    # print(src.shape[0],src.shape[1])
    src = src[0:src.shape[0] - 40, 0:src.shape[1]]
    cv_img = cv2.copyMakeBorder(src, 120, 0, 120, 120, cv2.BORDER_REPLICATE)
    cv2.imwrite(imgfile, cv_img)

    processed_img = WarpedImage(imgfile)
    # if processed_img.written:
    #     outfiles.append(processed_img.outfile)
    #     print(f"  wrote {processed_img.outfile}", end="\n\n")
    if cfg.pdf_opts.CONVERT_TO_PDF:
        save_pdf(outfiles)


if __name__ == "__main__":
    main()
```

通过调用，输入图片地址，可生成处理好的图片，保存你们想要的地址。

 图片处理效果如下：

![下载 (60)](image\下载 (60).png)

## 13-文本纠错功能，经常写错别字的小伙伴的福星

我们在日常生活中，经常会写一些文稿，比如：会议纪要，周报，日报，汇报材料，这些文稿里我们会发现有时候出现拼写、语法、标点等错误；其中拼写错误的错别字占大部分。

  经过初步统计：在微博等新媒体领域中，文本敏感和出错概率在2%左右；怎样才能快速解决这个错误问题呢，让机器帮我们找错别字，因为有时候自己写的文章，比较不容易找出错误，如果找出来需要反复通读全文，这也是很费时的一件事情。

  下面我们要用NLP中的文本纠错功能来初步解决这个问题，文本纠错作为自然语言处理最基础的模块，是实现中文语句自动检查、自动纠错的一项重要的自然语言处理技术。

**1. 文本纠错模型介绍**

   文本纠错任务是一项NLP基础任务，我们输入是一个可能含有错别字的中文句子，输出是一个纠正错别字后的中文句子。文本纠错任务也可以纠正语法错误类型的句子，包括有多字、少字等，目前最常见的错误类型是`错别字`。目前主要对错别字这一类型进行研究。

下面我用多个模型实现文本纠错：

**模型1：pycorrector 基础**

安装方式：pip install pycorrector

```
import sys

sys.path.append("..")

import pycorrector

if __name__ == '__main__':

    error_sentences = [
        '他是有明的侦探',
        '这场比赛我甘败下风',
        '这家伙还蛮格尽职守的',
        '报应接中迩来',
        '今天我很高形',
        '少先队员因该为老人让坐',
        '老是在较书。'
    ]
    for line in error_sentences:
        correct_sent, err = pycorrector.correct(line)
        print("{} => {} {}".format(line, correct_sent, err))
```

运行结果：

```
他是有明的侦探 => 他是有名的侦探 [('有明', '有名', 2, 4)]
这场比赛我甘败下风 => 这场比赛我甘拜下风 [('甘败下风', '甘拜下风', 5, 9)]
这家伙还蛮格尽职守的 => 这家伙还蛮恪尽职守的 [('蛮格', '蛮恪', 4, 6), ('格尽职守', '恪尽职守', 5, 9)]
报应接中迩来 => 报应接踵而来 [('接中迩来', '接踵而来', 2, 6)]
今天我很高形 => 今天我很高兴 [('高形', '高兴', 4, 6)]
少先队员因该为老人让坐 => 少先队员应该为老人让座 [('因该', '应该', 4, 6), ('坐', '座', 10, 11)]
老是在较书。 => 老师在较书。 [('老是', '老师', 0, 2)]
```

 我们看到这些错别字大部分被准确地找出，有些还是没有找出，对于其他样本测试，效果还不一定。

**模型2：****macbert4csc**

```
import gradio as gr
import operator
import torch
from transformers import BertTokenizer, BertForMaskedLM

tokenizer = BertTokenizer.from_pretrained("shibing624/macbert4csc-base-chinese")
model = BertForMaskedLM.from_pretrained("shibing624/macbert4csc-base-chinese")


def ai_text(text):
    with torch.no_grad():
        outputs = model(**tokenizer([text], padding=True, return_tensors='pt'))

    def to_highlight(corrected_sent, errs):
        output = [{"entity": "纠错", "word": err[1], "start": err[2], "end": err[3]} for i, err in
                  enumerate(errs)]
        return {"text": corrected_sent, "entities": output}

    def get_errors(corrected_text, origin_text):
        sub_details = []
        for i, ori_char in enumerate(origin_text):
            if ori_char in [' ', '“', '”', '‘', '’', '琊', '\n', '…', '—', '擤']:
                # add unk word
                corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]
                continue
            if i >= len(corrected_text):
                continue
            if ori_char != corrected_text[i]:
                if ori_char.lower() == corrected_text[i]:
                    # pass english upper char
                    corrected_text = corrected_text[:i] + ori_char + corrected_text[i + 1:]
                    continue
                sub_details.append((ori_char, corrected_text[i], i, i + 1))
        sub_details = sorted(sub_details, key=operator.itemgetter(2))
        return corrected_text, sub_details

    _text = tokenizer.decode(torch.argmax(outputs.logits[0], dim=-1), skip_special_tokens=True).replace(' ', '')
    corrected_text = _text[:len(text)]
    corrected_text, details = get_errors(corrected_text, text)
    print(text, ' => ', corrected_text, details)
    return to_highlight(corrected_text, details), details


if __name__ == '__main__':
    print(ai_text('少先队员因该为老人让坐'))

    examples = [
        ['真麻烦你了。希望你们好好的跳无'],
        ['少先队员因该为老人让坐'],
        ['他是有明的侦探'],
        ['今天心情很不搓'],
        ['他法语说的很好，的语也不错'],
        ['这场比赛我甘败下风'],
    ]

    gr.Interface(
        ai_text,
        inputs="textbox",
        outputs=[
            gr.outputs.HighlightedText(
                label="Output",
                show_legend=True,
            ),
            gr.outputs.JSON(
                label="JSON Output"
            )
        ],
        title="中文纠错模型",
        description="输入一段话，判断这段话中是否有错别字或语法错误",
        article="Link to <a href='https://github.com/shibing624/pycorrector' style='color:blue;' target='_blank\'>Github REPO</a>",
        examples=examples
    ).launch()
```

运行后，可以启动可视化网页 http://127.0.0.1:7860

![下载 (61)](image\下载 (61).png)

 **模型3：****MacBertCorrector**

```
# -*- coding: utf-8 -*-

import sys

sys.path.append("..")
from pycorrector.macbert.macbert_corrector import MacBertCorrector


def use_origin_transformers():
    # 原生transformers库调用
    import operator
    import torch
    from transformers import BertTokenizer, BertForMaskedLM
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = BertTokenizer.from_pretrained("shibing624/macbert4csc-base-chinese")
    model = BertForMaskedLM.from_pretrained("shibing624/macbert4csc-base-chinese")
    model.to(device)

    texts = ["今天新情很好", "你找到你最喜欢的工作，我也很高心。", "我不唉“看 琅擤琊榜”"]

    text_tokens = tokenizer(texts, padding=True, return_tensors='pt').to(device)
    with torch.no_grad():
        outputs = model(**text_tokens)

    def get_errors(corrected_text, origin_text):
        sub_details = []
        for i, ori_char in enumerate(origin_text):
            if ori_char in [' ', '“', '”', '‘', '’', '\n', '…', '—', '擤']:
                # add unk word
                corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]
                continue
            if i >= len(corrected_text):
                break
            if ori_char != corrected_text[i]:
                if ori_char.lower() == corrected_text[i]:
                    # pass english upper char
                    corrected_text = corrected_text[:i] + ori_char + corrected_text[i + 1:]
                    continue
                sub_details.append((ori_char, corrected_text[i], i, i + 1))
        sub_details = sorted(sub_details, key=operator.itemgetter(2))
        return corrected_text, sub_details

    result = []
    for ids, (i, text) in zip(outputs.logits, enumerate(texts)):
        _text = tokenizer.decode((torch.argmax(ids, dim=-1) * text_tokens.attention_mask[i]),
                                 skip_special_tokens=True).replace(' ', '')
        corrected_text, details = get_errors(_text, text)
        print(text, ' => ', corrected_text, details)
        result.append((corrected_text, details))
    print(result)
    return result


if __name__ == '__main__':
    # 原生transformers库调用
    use_origin_transformers()

    # pycorrector封装调用
    error_sentences = [
        '他是有明的侦探',
        '这场比赛我甘败下风',
        '这家伙还蛮格尽职守的',
        '报应接中迩来',
        '今天我很高形',
        '少先队员因该为老人让坐',
        '老是在较书。'
    ]

    m = MacBertCorrector()
    for line in error_sentences:
        correct_sent, err = m.macbert_correct(line)
        print("query:{} => {} err:{}".format(line, correct_sent, err))
```

运行结果：

```
query:他是有明的侦探 => 他是有名的侦探 err:[('明', '名', 3, 4)]
query:这场比赛我甘败下风 => 这场比赛我甘败下风 err:[]
query:这家伙还蛮格尽职守的 => 这家伙还蛮格尽职守的 err:[]
query:报应接中迩来 => 报应接中迩来 err:[]
query:今天我很高形 => 今天我很高兴 err:[('形', '兴', 5, 6)]
query:少先队员因该为老人让坐 => 少先队员应该为老人让坐 err:[('因', '应', 4, 5)]
query:老是在较书。 => 老师在教书。 err:[('是', '师', 1, 2), ('较', '教', 3, 4)]
```

**模型4：T5Corrector**

```
import sys

sys.path.append("..")
from pycorrector.t5.t5_corrector import T5Corrector

if __name__ == '__main__':
    # pycorrector封装调用
    error_sentences = [
        '他是有明的侦探',
        '这场比赛我甘败下风',
        '这家伙还蛮格尽职守的',
        '报应接中迩来',
        '今天我很高形',
        '少先队员因该为老人让坐',
        '老是在较书。'
    ]

    m = T5Corrector()
    res = m.batch_t5_correct(error_sentences)
    for line, r in zip(error_sentences, res):
        correct_sent, err = r[0], r[1]
        print("query:{} => {} err:{}".format(line, correct_sent, err))
```

运行结果：

```
query:他是有明的侦探 => 他是有名的侦探 err:[('明', '名', 3, 4)]
query:这场比赛我甘败下风 => 这场比赛我甘败下风 err:[]
query:这家伙还蛮格尽职守的 => 这家伙还蛮格尽职守的 err:[]
query:报应接中迩来 => 报应接中找来 err:[('迩', '找', 4, 5)]
query:今天我很高形 => 今天我很高 err:[]
query:少先队员因该为老人让坐 => 少先队员应该为老人让坐 err:[('因', '应', 4, 5)]
query:老是在较书。 => 老师在教书。 err:[('是', '师', 1, 2), ('较', '教', 3, 4)]
```

## 15-让机器进行阅读理解+你可以变成出题者提问

今天给大家带来一个机器阅读理解的项目，利用ERNIE的预训练模型进行微调训练，添加自己的数据集进行训练，训练好就可以利用功能进行阅读式信息抽取啦，也可以问机器一些简单的问题进行抽取。今天采用的paddle深度学习框架，它和pytorch很像，可以快速迁移。

**1、数据集**

基于语义的阅读理解是检索问答系统中的重要部分，最常见的数据集是单篇章、抽取式阅读理解数据集。关于阅读理解的数据集采用Dureader数据集，它是关注阅读理解模型鲁棒性的中文数据集，在实际业务中，根据需求添加部分业务需求数据集。

**数据样例：**

问题 : 陈明董事长被调整为什么职务

篇章 : 大会决定将**人工智能副会长职务调整为陈明董事长兼任，经公司党委研究后向党委报批。

参考答案 : [‘**人工智能副会长’]

数据格式样例：

```
{
   "context": "大会决定将**人工智能副会长职务调整为陈明董事长兼任，经公司党委研究后向党委报批。", 
                    "qas": [
                        {
                            "question": "陈明董事长被调整为什么职务", 
                            "id": "0a25cb4bc1ab6f474c699884e04601e4", 
                            "answers": [
                                {
                                    "text": "**人工智能副会长职务", 
                                    "answer_start": 6
                                }
                            ]
                        }
                    ]
                }
```

 我们在实际应用过程中，需要评估模型的鲁棒性，让模型可以预测更多不可控的数据，随着当前数据集的扩大、模型的优化、算力的提升，模型可在大量测试集上取得较好的性能，在实际应用中，这些模型所表现出的鲁棒性还未达到预期效果。于是我们要更进一步优化与微调模型，随着模型研发越来越深，模型优化会越来越好的。

![下载 (62)](image\下载 (62).png)

DuReader数据集采用SQuAD数据格式，InputFeature使用滑动窗口的方法生成，即一个example可能对应多个InputFeature。对于过长的文章，采用滑动窗口将文章分成多段，分别与问题组合。再用对应的tokenizer转化为模型可接受的feature。滑动窗口生成InputFeature的过程如下图：

![下载 (63)](image\下载 (63).png)

本文采用paddle.io.BatchSampler和paddlenlp.data中提供的方法把数据组成batch。Batchify过程是把数据集中的数据排到多个列中，在划分成多个大小为 batch_size 的集合。然后使用paddle.io.DataLoader接口多线程异步加载数据。

![下载 (64)](image\下载 (64).png)

DuReader阅读理解任务的本质是答案抽取任务。根据输入的问题和文章，从预训练模型的sequence_output中预测答案在文章中的起始位置和结束位置。模型训练过程采用梯度下载更新权重参数。

![下载 (65)](image\下载 (65).png)

![下载 (66)](image\下载 (66).png)

代码训练部分：

```
import paddle
from paddlenlp.data import Stack, Dict, Pad
import paddlenlp
from paddlenlp.datasets import load_dataset
from utils import prepare_train_features, prepare_validation_features
from functools import partial

train_ds, dev_ds, test_ds = load_dataset('dureader_robust', splits=('train', 'dev', 'test'))

for idx in range(2):
    print(train_ds[idx]['question'])
    print(train_ds[idx]['context'])
    print(train_ds[idx]['answers'])
    print(train_ds[idx]['answer_starts'])
    print()

# 设置模型名称
MODEL_NAME = 'ernie-1.0'
tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)


max_seq_length = 512
doc_stride = 128

train_trans_func = partial(prepare_train_features, 
                           max_seq_length=max_seq_length, 
                           doc_stride=doc_stride,
                           tokenizer=tokenizer)

train_ds.map(train_trans_func, batched=True, num_workers=4)

dev_trans_func = partial(prepare_validation_features, 
                           max_seq_length=max_seq_length, 
                           doc_stride=doc_stride,
                           tokenizer=tokenizer)
                           
dev_ds.map(dev_trans_func, batched=True, num_workers=4)
test_ds.map(dev_trans_func, batched=True, num_workers=4)

for idx in range(2):
    print(train_ds[idx]['input_ids'])
    print(train_ds[idx]['token_type_ids'])
    print(train_ds[idx]['overflow_to_sample'])
    print(train_ds[idx]['offset_mapping'])
    print(train_ds[idx]['start_positions'])
    print(train_ds[idx]['end_positions'])

batch_size = 12

# 定义BatchSampler
train_batch_sampler = paddle.io.DistributedBatchSampler(
        train_ds, batch_size=batch_size, shuffle=True)

dev_batch_sampler = paddle.io.BatchSampler(
    dev_ds, batch_size=batch_size, shuffle=False)

test_batch_sampler = paddle.io.BatchSampler(
    test_ds, batch_size=batch_size, shuffle=False)

# 定义batchify_fn
train_batchify_fn = lambda samples, fn=Dict({
    "input_ids": Pad(axis=0, pad_val=tokenizer.pad_token_id),
    "token_type_ids": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),
    "start_positions": Stack(dtype="int64"),
    "end_positions": Stack(dtype="int64")
}): fn(samples)

dev_batchify_fn = lambda samples, fn=Dict({
    "input_ids": Pad(axis=0, pad_val=tokenizer.pad_token_id),
    "token_type_ids": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)
}): fn(samples)

# 构造DataLoader
train_data_loader = paddle.io.DataLoader(
    dataset=train_ds,
    batch_sampler=train_batch_sampler,
    collate_fn=train_batchify_fn,
    return_list=True)

dev_data_loader = paddle.io.DataLoader(
    dataset=dev_ds,
    batch_sampler=dev_batch_sampler,
    collate_fn=dev_batchify_fn,
    return_list=True)

test_data_loader = paddle.io.DataLoader(
    dataset=test_ds,
    batch_sampler=test_batch_sampler,
    collate_fn=dev_batchify_fn,
    return_list=True)

for step, batch in enumerate(train_data_loader, start=1):

        input_ids, segment_ids, start_positions, end_positions = batch
        print(input_ids)
        break


from paddlenlp.transformers import ErnieForQuestionAnswering
# 模型加载
model = ErnieForQuestionAnswering.from_pretrained(MODEL_NAME)


#损失函数设定
class CrossEntropyLossForRobust(paddle.nn.Layer):
    def __init__(self):
        super(CrossEntropyLossForRobust, self).__init__()

    def forward(self, y, label):
        start_logits, end_logits = y   # both shape are [batch_size, seq_len]
        start_position, end_position = label
        start_position = paddle.unsqueeze(start_position, axis=-1)
        end_position = paddle.unsqueeze(end_position, axis=-1)
        start_loss = paddle.nn.functional.softmax_with_cross_entropy(
            logits=start_logits, label=start_position, soft_label=False)
        start_loss = paddle.mean(start_loss)
        end_loss = paddle.nn.functional.softmax_with_cross_entropy(
            logits=end_logits, label=end_position, soft_label=False)
        end_loss = paddle.mean(end_loss)

        loss = (start_loss + end_loss) / 2
        return loss


# 训练过程中的最大学习率
learning_rate = 3e-5 

# 训练轮次
epochs = 2

# 学习率预热比例
warmup_proportion = 0.1

# 权重衰减系数，类似模型正则项策略，避免模型过拟合
weight_decay = 0.01

num_training_steps = len(train_data_loader) * epochs

# 学习率衰减策略
lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)

decay_params = [
    p.name for n, p in model.named_parameters()
    if not any(nd in n for nd in ["bias", "norm"])
]
optimizer = paddle.optimizer.AdamW(
    learning_rate=lr_scheduler,
    parameters=model.parameters(),
    weight_decay=weight_decay,
    apply_decay_param_fun=lambda x: x in decay_params)


from utils import evaluate

criterion = CrossEntropyLossForRobust()
global_step = 0
for epoch in range(1, epochs + 1):
    for step, batch in enumerate(train_data_loader, start=1):

        global_step += 1
        input_ids, segment_ids, start_positions, end_positions = batch
        logits = model(input_ids=input_ids, token_type_ids=segment_ids)
        loss = criterion(logits, (start_positions, end_positions))

        if global_step % 100 == 0 :
            print("global step %d, epoch: %d, batch: %d, loss: %.5f" % (global_step, epoch, step, loss))

        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.clear_grad()

    evaluate(model=model, data_loader=dev_data_loader) 
```

# 1-基于SqueezeNet的眼疾识别

真正的 SqueezeNet 模型具有独特的结构，其核心是`Fire`模块，该模块由`squeeze`层和`expand`层组成。`squeeze`层使用 1x1 卷积来减少通道数，`expand`层则使用 1x1 和 3x3 卷积来增加通道数，并将两者的结果拼接在一起。以下是一个使用 Keras 构建 SqueezeNet 模型的示例代码：

## 背景

#### 1.1 数据集介绍

`iChallenge-PM`是百度大脑和中山大学中山眼科中心联合举办的iChallenge比赛中，提供的关于病理性近视（Pathologic Myopia，PM）的医疗类数据集，包含1200个受试者的眼底视网膜图片，训练、验证和测试数据集各400张。

- training.zip：包含训练中的图片和标签
- validation.zip：包含验证集的图片
- valid_gt.zip：包含验证集的标签

该数据集是从AI Studio平台中下载的，具体信息如下：

https://pan.baidu.com/s/1yBkVHZw5YFYUdqD2GntRyQ?pwd=2357

![9bf63e5bba326a80175de4736304b2a0](image\9bf63e5bba326a80175de4736304b2a0.png)

#### 1.2 数据集文件结构

数据集中共有三个压缩文件，分别是：

![Snipaste_2025-03-14_17-31-11](image\Snipaste_2025-03-14_17-31-11.png)

#### 2.1 数据标签划分

该眼疾数据集格式有点复杂，这里我对数据集进行了自己的处理，将训练集和验证集写入txt文本里面，分别对应它的图片路径和标签。

```python
import os
import pandas as pd
# 将训练集划分标签
train_dataset = r"F:\SqueezeNet\data\PALM-Training400\PALM-Training400"
train_list = []
label_list = []


train_filenames = os.listdir(train_dataset)

for name in train_filenames:
    filepath = os.path.join(train_dataset, name)
    train_list.append(filepath)
    if name[0] == 'N' or name[0] == 'H':
        label = 0
        label_list.append(label)
    elif name[0] == 'P':
        label = 1
        label_list.append(label)
    else:
        raise('Error dataset!')


with open('F:/SqueezeNet/train.txt', 'w', encoding='UTF-8') as f:
    i = 0
    for train_img in train_list:
        f.write(str(train_img) + ' ' +str(label_list[i]))
        i += 1
        f.write('\n')
# 将验证集划分标签
valid_dataset = r"F:\SqueezeNet\data\PALM-Validation400"
valid_filenames = os.listdir(valid_dataset)
valid_label = r"F:\SqueezeNet\data\PALM-Validation-GT\PM_Label_and_Fovea_Location.xlsx"
data = pd.read_excel(valid_label)
valid_data = data[['imgName', 'Label']].values.tolist()

with open('F:/SqueezeNet/valid.txt', 'w', encoding='UTF-8') as f:
    for valid_img in valid_data:
        f.write(str(valid_dataset) + '/' + valid_img[0] + ' ' + str(valid_img[1]))
        f.write('\n')
12345678910111213141516171819202122232425262728293031323334353637383940
```

#### 2.2 数据预处理

这里采用到的数据预处理，主要有调整图像大小、随机翻转、归一化等。

```python
import os.path
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import transforms

transform_BZ = transforms.Normalize(
    mean=[0.5, 0.5, 0.5],
    std=[0.5, 0.5, 0.5]
)


class LoadData(Dataset):
    def __init__(self, txt_path, train_flag=True):
        self.imgs_info = self.get_images(txt_path)
        self.train_flag = train_flag

        self.train_tf = transforms.Compose([
            transforms.Resize(224),  # 调整图像大小为224x224
            transforms.RandomHorizontalFlip(),  # 随机左右翻转图像
            transforms.RandomVerticalFlip(),  # 随机上下翻转图像
            transforms.ToTensor(),  # 将PIL Image或numpy.ndarray转换为tensor，并归一化到[0,1]之间
            transform_BZ  # 执行某些复杂变换操作
        ])
        self.val_tf = transforms.Compose([
            transforms.Resize(224),  # 调整图像大小为224x224
            transforms.ToTensor(),  # 将PIL Image或numpy.ndarray转换为tensor，并归一化到[0,1]之间
            transform_BZ  # 执行某些复杂变换操作
        ])

    def get_images(self, txt_path):
        with open(txt_path, 'r', encoding='utf-8') as f:
            imgs_info = f.readlines()
            imgs_info = list(map(lambda x: x.strip().split(' '), imgs_info))
        return imgs_info

    def padding_black(self, img):
        w, h = img.size
        scale = 224. / max(w, h)
        img_fg = img.resize([int(x) for x in [w * scale, h * scale]])
        size_fg = img_fg.size
        size_bg = 224
        img_bg = Image.new("RGB", (size_bg, size_bg))
        img_bg.paste(img_fg, ((size_bg - size_fg[0]) // 2,
                              (size_bg - size_fg[1]) // 2))

        img = img_bg
        return img

    def __getitem__(self, index):
        img_path, label = self.imgs_info[index]

        img_path = os.path.join('', img_path)
        img = Image.open(img_path)
        img = img.convert("RGB")
        img = self.padding_black(img)
        if self.train_flag:
            img = self.train_tf(img)
        else:
            img = self.val_tf(img)
        label = int(label)
        return img, label

    def __len__(self):
        return len(self.imgs_info)
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364
```

#### 2.3 构建模型

```python
import torch
import torch.nn as nn
import torch.nn.init as init


class Fire(nn.Module):

    def __init__(self, inplanes, squeeze_planes,
                 expand1x1_planes, expand3x3_planes):
        super(Fire, self).__init__()
        self.inplanes = inplanes
        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)
        self.squeeze_activation = nn.ReLU(inplace=True)
        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,
                                   kernel_size=1)
        self.expand1x1_activation = nn.ReLU(inplace=True)
        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,
                                   kernel_size=3, padding=1)
        self.expand3x3_activation = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.squeeze_activation(self.squeeze(x))
        return torch.cat([
            self.expand1x1_activation(self.expand1x1(x)),
            self.expand3x3_activation(self.expand3x3(x))
        ], 1)


class SqueezeNet(nn.Module):

    def __init__(self, version='1_0', num_classes=1000):
        super(SqueezeNet, self).__init__()
        self.num_classes = num_classes
        if version == '1_0':
            self.features = nn.Sequential(
                nn.Conv2d(3, 96, kernel_size=7, stride=2),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
                Fire(96, 16, 64, 64),
                Fire(128, 16, 64, 64),
                Fire(128, 32, 128, 128),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
                Fire(256, 32, 128, 128),
                Fire(256, 48, 192, 192),
                Fire(384, 48, 192, 192),
                Fire(384, 64, 256, 256),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
                Fire(512, 64, 256, 256),
            )
        elif version == '1_1':
            self.features = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, stride=2),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
                Fire(64, 16, 64, 64),
                Fire(128, 16, 64, 64),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
                Fire(128, 32, 128, 128),
                Fire(256, 32, 128, 128),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
                Fire(256, 48, 192, 192),
                Fire(384, 48, 192, 192),
                Fire(384, 64, 256, 256),
                Fire(512, 64, 256, 256),
            )
        else:
            # FIXME: Is this needed? SqueezeNet should only be called from the
            # FIXME: squeezenet1_x() functions
            # FIXME: This checking is not done for the other models
            raise ValueError("Unsupported SqueezeNet version {version}:"
                             "1_0 or 1_1 expected".format(version=version))

        # Final convolution is initialized differently from the rest
        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            final_conv,
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if m is final_conv:
                    init.normal_(m.weight, mean=0.0, std=0.01)
                else:
                    init.kaiming_uniform_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return torch.flatten(x, 1)

1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495
```

#### 2.4 开始训练

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from model import SqueezeNet
import torchsummary
from dataloader import LoadData
import copy

device = "cuda:0" if torch.cuda.is_available() else "cpu"
print("Using {} device".format(device))

model = SqueezeNet(num_classes=2).to(device)
# print(model)
#print(torchsummary.summary(model, (3, 224, 224), 1))


# 加载训练集和验证集
train_data = LoadData(r"F:\SqueezeNet\train.txt", True)
train_dl = torch.utils.data.DataLoader(train_data, batch_size=16, pin_memory=True,
                                           shuffle=True, num_workers=0)
test_data = LoadData(r"F:\SqueezeNet\valid.txt", True)
test_dl = torch.utils.data.DataLoader(test_data, batch_size=16, pin_memory=True,
                                           shuffle=True, num_workers=0)


# 编写训练函数
def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)  # 训练集的大小
    num_batches = len(dataloader)  # 批次数目, (size/batch_size，向上取整)
    print('num_batches:', num_batches)
    train_loss, train_acc = 0, 0  # 初始化训练损失和正确率

    for X, y in dataloader:  # 获取图片及其标签
        X, y = X.to(device), y.to(device)
        # 计算预测误差
        pred = model(X)  # 网络输出
        loss = loss_fn(pred, y)  # 计算网络输出和真实值之间的差距，targets为真实值，计算二者差值即为损失

        # 反向传播
        optimizer.zero_grad()  # grad属性归零
        loss.backward()  # 反向传播
        optimizer.step()  # 每一步自动更新

        # 记录acc与loss
        train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()
        train_loss += loss.item()

    train_acc /= size
    train_loss /= num_batches

    return train_acc, train_loss

# 编写验证函数
def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)  # 测试集的大小
    num_batches = len(dataloader)  # 批次数目, (size/batch_size，向上取整)
    test_loss, test_acc = 0, 0

    # 当不进行训练时，停止梯度更新，节省计算内存消耗
    with torch.no_grad():
        for imgs, target in dataloader:
            imgs, target = imgs.to(device), target.to(device)

            # 计算loss
            target_pred = model(imgs)
            loss = loss_fn(target_pred, target)

            test_loss += loss.item()
            test_acc += (target_pred.argmax(1) == target).type(torch.float).sum().item()

    test_acc /= size
    test_loss /= num_batches

    return test_acc, test_loss




# 开始训练

epochs = 20

train_loss = []
train_acc = []
test_loss = []
test_acc = []

best_acc = 0  # 设置一个最佳准确率，作为最佳模型的判别指标


loss_function = nn.CrossEntropyLoss()  # 定义损失函数
optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 定义Adam优化器

for epoch in range(epochs):

    model.train()
    epoch_train_acc, epoch_train_loss = train(train_dl, model, loss_function, optimizer)

    model.eval()
    epoch_test_acc, epoch_test_loss = test(test_dl, model, loss_function)

    # 保存最佳模型到 best_model
    if epoch_test_acc > best_acc:
        best_acc = epoch_test_acc
        best_model = copy.deepcopy(model)

    train_acc.append(epoch_train_acc)
    train_loss.append(epoch_train_loss)
    test_acc.append(epoch_test_acc)
    test_loss.append(epoch_test_loss)

    # 获取当前的学习率
    lr = optimizer.state_dict()['param_groups'][0]['lr']

    template = ('Epoch:{:2d}, Train_acc:{:.1f}%, Train_loss:{:.3f}, Test_acc:{:.1f}%, Test_loss:{:.3f}, Lr:{:.2E}')
    print(template.format(epoch + 1, epoch_train_acc * 100, epoch_train_loss,
                          epoch_test_acc * 100, epoch_test_loss, lr))

# 保存最佳模型到文件中
PATH = './best_model.pth'  # 保存的参数文件名
torch.save(best_model.state_dict(), PATH)

print('Done')
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123
```

![d427d9bb041d471abb947f75032a3b5d](image\d427d9bb041d471abb947f75032a3b5d.png)

#### 2.5 结果可视化

```python
import matplotlib.pyplot as plt
#隐藏警告
import warnings
warnings.filterwarnings("ignore")               #忽略警告信息
plt.rcParams['font.sans-serif']    = ['SimHei'] # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False      # 用来正常显示负号
plt.rcParams['figure.dpi']         = 100        #分辨率

epochs_range = range(epochs)

plt.figure(figsize=(12, 3))
plt.subplot(1, 2, 1)

plt.plot(epochs_range, train_acc, label='Training Accuracy')
plt.plot(epochs_range, test_acc, label='Test Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Test Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_loss, label='Training Loss')
plt.plot(epochs_range, test_loss, label='Test Loss')
plt.legend(loc='upper right')
plt.title('Training and Test Loss')
plt.show()
123456789101112131415161718192021222324
```

可视化结果如下：
![65442aa0c32ee9fdbc6ffeedada3f161](image\65442aa0c32ee9fdbc6ffeedada3f161.png)
可以自行调整学习率以及batch_size，这里我的超参数并没有调整。

### 三、数据集个体预测

```python
import matplotlib.pyplot as plt
from PIL import Image
from torchvision.transforms import transforms
from model import SqueezeNet
import torch

data_transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Resize((224, 224)),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

img = Image.open("F:\SqueezeNet\data\PALM-Validation400\V0008.jpg")
plt.imshow(img)
img = data_transform(img)
img = torch.unsqueeze(img, dim=0)
name = ['非病理性近视', '病理性近视']
model_weight_path = r"F:\SqueezeNet\best_model.pth"
model = SqueezeNet(num_classes=2)
model.load_state_dict(torch.load(model_weight_path))
model.eval()
with torch.no_grad():
    output = torch.squeeze(model(img))

    predict = torch.softmax(output, dim=0)
    # 获得最大可能性索引
    predict_cla = torch.argmax(predict).numpy()
    print('索引为', predict_cla)
print('预测结果为：{},置信度为: {}'.format(name[predict_cla], predict[predict_cla].item()))
plt.show()
1234567891011121314151617181920212223242526272829
索引为 1
预测结果为：病理性近视,置信度为: 0.9768268465995789
12
```

![a2049751a39dc7cd492389fd0f280748](image\a2049751a39dc7cd492389fd0f280748.png)

## 一、理论基础

## 1.前言

SqueezeNet算法，顾名思义，Squeeze的中文意思是压缩和挤压的意思，所以我们通过算法的名字就可以猜想到，该算法一定是通过解压模型来降低模型参数量的。当然任何算法的改进都是在原先的基础上提升精度或者降低模型参数，因此该算法的主要目的就是在于降低模型参数量的同时保持模型精度。

随着CNN卷积神经网络的研究发展，越来越多的模型被研发出来，而为了提高模型的精度，深层次的模型例如AlexNet和ResNet等得到了大家的广泛认可。但是由于应用场景的需求，许多模型因为参数量太大的原因而无法满足实际的应用场景需求，例如自动驾驶等技术。所以人们开始把注意力放在了轻量级模型上面，

SqueezeNet模型也因此“应时而生”。

SqueezeNet论文中对轻量级模型的优点做了以下几点总结：

● 更高效的分布式训练。服务器间的通信是分布式CNN训练可扩展性的限制因素。对于分布式数据并行训练，通信开销与模型中参数的数量成正比。简而言之，小模型训练更快，因为需要更少的交流。

● 在向客户导出新模型时减少开销。在自动驾驶方面，特斯拉(Tesla)等公司会定期将新车型从服务器复制到客户的汽车上。这种做法通常被称为无线更新。《消费者报告》发现，随着最近的无线更新，特斯拉Autopilot半自动驾驶功能的安全性得到了逐步提高(《消费者报告》，2016年)。然而，今天典型的CNN/DNN模型的无线更新可能需要大量的数据传输。使用AlexNet，这将需要从服务器到汽车的240MB通信。较小的模型需要较少的通信，使频繁更新更可行。

● 可行的FPGA和嵌入式部署。fpga的片内存储器通常小于10MB1，没有片外存储器或存储器。对于推理，足够小的模型可以直接存储在FPGA上，而不会受到内存带宽的瓶颈(Qiu et al .， 2016)，而视频帧则实时通过FPGA。此外，当在专用集成电路(ASIC)上部署cnn时，足够小的模型可以直接存储在芯片上，而更小的模型可以使ASIC适合更小的芯片。

## 2.设计理念

减少模型参数的方法以往就有，一个明智的方法是采用现有的CNN模型并以有损方式压缩它。近年来围绕模型压缩的主题已经出现了一个研究社区，并且已经报道了几种方法。Denton等人的一种相当直接的方法是将奇异值分解(SVD)应用于预训练的CNN模型。Han等人开发了网络剪枝（Network Pruning），从预训练模型开始，将低于某阈值的参数替换为零，形成稀疏矩阵，最后在稀疏CNN上进行几次迭代训练。Han等人将网络修剪与量化(至8位或更少)和霍夫曼编码相结合，扩展了他们的工作，创建了一种称为深度压缩（Deep Compression）的方法，并进一步设计了一种称为EIE的硬件加速器，该加速器直接在压缩模型上运行，实现了显著的加速和节能。

而SqueezeNet算法的主要策略也是压缩策略，在模型压缩上，总共使用了三种方法策略，具体我们下面讲述。

### 2.1 CNN微架构（CNN MicroArchitecture）

同时，SqueezeNet论文作者考虑到随着设计深度CNN卷积神经网络的趋势，手动选择每层滤波器的尺寸变得非常麻烦。所以为了解决这个问题，网上已经有提出了由具有特定固定组织的多个卷积层组成的各种高级构建块或者模块。例如，GoogleNet的论文提出了Inception模块，它由许多不同维度的过滤器组成，通常包括1×11×1和3×33×3，有时候会加上5×55×5，有时加上1×31×3和3×13×1，可能还会加上额外的特设层，形成一个完整的网络。我们上一篇讲解的WideResNet算法中也曾讲解到一个类似的块，即残差块。SqueezeNet论文作者将这种各个模块的特定组织和维度统称为CNN微架构。

![Snipaste_2025-03-14_17-15-15](image\Snipaste_2025-03-14_17-15-15.png)

### 2.2 CNN宏架构（CNN MacroArchitecture）

CNN微架构指的是单独的层和模块，而CNN宏架构可以定义为多个模块的系统级组织，形成端到端的CNN体系结构。也许在最近的文献中，最广泛研究的CNN宏观架构主题是网络中深度(即层数)的影响。如VGG12-19层在ImageNet-1k数据集上产生更高的精度。选择跨多层或模块的连接是CNN宏观架构研究的一个新兴领域。例如残差网络（ResNet）和高速公路网络（Highway Network）中都建议采用跳过多层的连接，比如将第3层的激活附加地连接到第6层的激活，我们把这种连接称为旁路连接。ResNet的作者提供了一个34层CNN的A/B比较，有和没有旁路连接（即跳跃连接），实验对比发现添加旁路连接可将ImageNet前5名的精度提高2个百分点。

### 2.3 模型网络设计探索过程

论文作者认为，神经网络(包括深度神经网络DNN和卷积神经网络CNN)有很大的设计空间，有许多微体系结构、宏观体系结构、求解器和其他超参数的选择。很自然，社区想要获得关于这些因素如何影响神经网络准确性的直觉(即设计空间的形状)。神经网络的设计空间探索(DSE)的大部分工作都集中在开发自动化方法来寻找提供更高精度的神经网络架构。这些自动化的DSE方法包括贝叶斯优化(Snoek et al, 2012)、模拟退火(Ludermir et al, 2006)、随机搜索(Bergstra & Bengio, 2012)和遗传算法(Stanley & Miikkulainen, 2002)。值得赞扬的是，这些论文中的每一篇都提供了一个案例，其中提议的DSE方法产生了一个与代表性基线相比具有更高精度的NN架构。然而，这些论文并没有试图提供关于神经网络设计空间形状的直觉。在SqueezeNet论文的后面，作者避开了自动化的方法——相反，作者重构CNN，这样就可以进行有原则的a /B比较，以研究CNN架构决策如何影响模型的大小和准确性。

### 2.4 结构设计策略

SqueezeNet算法的主要目标是构建具有很少参数的CNN架构，同时保证具有其它模型也有的精度。为了实现这一目标，作者总共采用了三种策略来设计CNN架构，具体如下：

**策略1：** 将 3×33×3卷积替换成 1×11×1卷积：通过这一步，一个卷积操作的参数数量减少了99倍；

**策略2：** 减少3×33×3卷积的通道数：一个3×33×3卷积的计算量是3×3×�×�3×3×*M*×*N*（其中M，N分别是输入特征图和输出特征图的通道数），作者认为这样一个计算量过于庞大，因此希望尽可能地将M和N减少以减少参数数量。

**策略3：** 在网络的后期进行下采样，这样卷积层就有了大的激活图。在卷积网络中，每个卷积层产生一个输出激活图，其空间分辨率至少为1x1，通常比1x1大得多。这些激活图的高度和宽度由:(1)输入数据的大小(例如256x256图像)和(2)在CNN体系结构中下采样层的选择。

其中策略1和策略2是关于明智地减少CNN中参数的数量，同时试图保持准确性。策略3是关于在有限的参数预算下最大化准确性。接下来，我们描述Fire模块，这是我们的CNN架构的构建块，使我们能够成功地采用策略1、2和3。

### 2.5 Fire模块

Fire模块包括：挤压卷积层（squeeze convolution），输入扩展层（expand），其中有1×11×1和3×33×3卷积滤波器的混合。在挤压卷积层中只有1×11×1卷积滤波器，而扩展层中混合有1×11×1和3×33×3卷积滤波器。同时该模块中引入了三个调节维度的超参数：

● �1�1*s*1*x*1: squeeze 中 1×11×1卷积滤波器个数 ；

● �1�1*e*1*x*1: expand 中 1×11×1 卷积滤波器个数 ；

● �3�3*e*3*x*3: expand 中 3×33×3 卷积滤波器个数 ；

![Snipaste_2025-03-14_17-16-24](image\Snipaste_2025-03-14_17-16-24.png)

![ba9993ccb8894b3dbf90b4fec460eab8affb7b6f986848ef93a7ea43015b30c4](image\ba9993ccb8894b3dbf90b4fec460eab8affb7b6f986848ef93a7ea43015b30c4.png)

![Snipaste_2025-03-14_17-17-50](image\Snipaste_2025-03-14_17-17-50.png)

## 3.网络结构

![3dfecc9244ef411c80f34fa1701325414deb1a8e20934985849aedb72270b579](image\3dfecc9244ef411c80f34fa1701325414deb1a8e20934985849aedb72270b579.png)

● 左图：SqueezeNet ；

● 中图：带简单旁路的 SqueezeNet ；

● 右图：带复杂旁路的 SqueezeNet ；

由图1-2中的左图我们可以看出，SqueezeNet是先从一个独立的卷积层（Conv1）开始，然后经过8个Fire模块（fire2-9），最后以一个卷积层（Conv10）结束。从网络的开始到结束，我们逐渐增加每个fire模块的过滤器数量。在conv1、fire4、fire8和conv10层之后，SqueezeNet以2步长执行最大池化;这些相对较晚的池放置是根据2.4中的策略3。

而简单旁路架构在模块3、5、7和9周围增加了旁路连接，要求这些模块在输入和输出之间学习残差函数。与ResNet一样，为了实现绕过Fire3的连接，将Fire4的输入设置为(Fire2的输出+ Fire3的输出)，其中+运算符是元素加法。这改变了应用于这些Fire模块参数的正则化，并且，根据ResNet，可以提高最终的准确性或训练完整模型的能力。

虽然简单的旁路“只是一根线”，但复杂的旁路包含1x1卷积层的旁路，其滤波器数量设置等于所需的输出通道数量。

**注意：在简单的情况下，输入通道的数量和输出通道的数量必须相同。因此，只有一半的Fire模块可以进行简单的旁路连接，如图2中图所示。当不能满足“相同数量的通道”要求时，我们使用复杂的旁路连接，如图1-2右侧所示。复杂的旁路连接会向模型中添加额外的参数，而简单的旁路连接则不会。**

**完整的SqueezeNet架构如下图所示：**

![19bc13f13fb0464c92428842040eb495b29722d6eb864e2dac90060743c8ab4f](image\19bc13f13fb0464c92428842040eb495b29722d6eb864e2dac90060743c8ab4f.png)

## 4.评估分析

比较SqueezeNet和不同模型压缩方法结果图如下：

![4ef622b5e33648a6931fa394321008e72dc00c44e2a245daa593776e75a6f634](image\4ef622b5e33648a6931fa394321008e72dc00c44e2a245daa593776e75a6f634.png)

微架构下挤压比率（SR）对模型尺寸和精度的影响以及扩展层中3×33×3过滤器的比例对模型尺寸和精度的影响实验对比图如下所示：

![4265a8f5950245f6a51874adfd06aa0edd604871529145108793b55e03d681a6](image\4265a8f5950245f6a51874adfd06aa0edd604871529145108793b55e03d681a6.png)

宏架构中三种结构（普通架构、简单旁支架构和复杂旁支架构）精度对比图如下：

![4dd5e2427886449cb7be15955ccba677bc1ca1985d9640aaabc9194dff9c540d](image\4dd5e2427886449cb7be15955ccba677bc1ca1985d9640aaabc9194dff9c540d.png)

## 二、实战

------

## 1.数据预处理

- 导入相关库

In [ ]

```
import paddle
import paddle.nn as nn
import paddle.nn.functional as F
import numpy as np
import math
import random
import os
from paddle.io import Dataset  # 导入Datasrt库
import paddle.vision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd
```

In [ ]

```
!pip install openpyxl -i https://pypi.org/simple
```

In [ ]

```
# 解压数据集
!unzip /home/aistudio/data/data23828/training.zip -d /home/aistudio/work/dataset
!unzip /home/aistudio/data/data23828/valid_gt.zip -d /home/aistudio/work/dataset
!unzip /home/aistudio/data/data23828/validation.zip -d /home/aistudio/work/dataset
!unzip /home/aistudio/work/dataset/PALM-Training400/PALM-Training400-Annotation-D&F.zip -d /home/aistudio/work/dataset/PALM-Training400
!unzip /home/aistudio/work/dataset/PALM-Training400/PALM-Training400-Annotation-Lession.zip -d /home/aistudio/work/dataset/PALM-Training400
!unzip /home/aistudio/work/dataset/PALM-Training400/PALM-Training400.zip -d /home/aistudio/work/dataset/PALM-Training400
```

- 数据类别可视化

In [2]

```
import warnings
import matplotlib.font_manager as font_manager
warnings.filterwarnings("ignore")
# 查看数据集各类数量
datapath = "/home/aistudio/work/dataset/PALM-Training400/PALM-Training400"

font_path  = '/home/aistudio/fonts/simfang.ttf'  # 将路径替换为simfang字体的路径
font_prop = font_manager.FontProperties(fname=font_path)
# 初始化计数器
count_H = 0
count_N = 0
count_P = 0

# 遍历数据集文件夹
for filename in os.listdir(datapath):
    # 检查文件名前缀并增加相应计数器的值
    if filename.startswith('H'):
        count_H += 1
    elif filename.startswith('N'):
        count_N += 1
    elif filename.startswith('P'):
        count_P += 1

# 打印各类数据集的数量
print(f'高度近视眼睛的数据集数量: {count_H}')
print(f'正常眼睛的数据集数量: {count_N}')
print(f'病理性近视眼睛的数据集数量: {count_P}')
# 设置类标签和数量
labels = ['H', 'N', 'P']
counts = [count_H, count_N, count_P]

# 创建柱状图
plt.bar(labels, counts)


# 设置标签和标题的字体为DejaVu Sans
plt.xlabel('类别', fontproperties=font_prop)
plt.ylabel('数量', fontproperties=font_prop)
plt.title('数据类别数量', fontproperties=font_prop)

# 显示图形
plt.show()
高度近视眼睛的数据集数量: 26
正常眼睛的数据集数量: 161
病理性近视眼睛的数据集数量: 213
```

![下载 (81)](image\下载 (81).png)

```
<Figure size 640x480 with 1 Axes>
```

- 数据集类别划分

In [6]

```
# 将训练集划分标签
train_dataset = "/home/aistudio/work/dataset/PALM-Training400/PALM-Training400"
train_list = []
label_list = []


train_filenames = os.listdir(train_dataset)

for name in train_filenames:
    filepath = os.path.join(train_dataset, name)
    train_list.append(filepath)
    if name[0] == 'N' or name[0] == 'H':
        label = 0
        label_list.append(label)
    elif name[0] == 'P':
        label = 1
        label_list.append(label)
    else:
        raise('Error dataset!')


with open('/home/aistudio/work/train.txt','w',encoding='UTF-8') as f:
    i = 0
    for train_img in train_list:
        f.write(str(train_img) + ' ' +str(label_list[i]))
        i += 1
        f.write('\n')
# 将验证集划分标签
valid_dataset = "/home/aistudio/work/dataset/PALM-Validation400"
valid_filenames = os.listdir(valid_dataset)
valid_label = "/home/aistudio/work/dataset/PALM-Validation-GT/PM_Label_and_Fovea_Location.xlsx"
data = pd.read_excel(valid_label)
valid_data = data[['imgName', 'Label']].values.tolist()

with open('/home/aistudio/work/valid.txt','w',encoding='UTF-8') as f:
    for valid_img in valid_data:
        f.write(str(valid_dataset) + '/' + valid_img[0] + ' ' + str(valid_img[1]))
        f.write('\n')
```

## 2.数据读取

In [3]

```
transform_BZ = transforms.Normalize(
    mean=[0.5, 0.5, 0.5],
    std=[0.5, 0.5, 0.5]
)

class LoadData(Dataset):
    def __init__(self, txt_path, train_flag=True):
        self.imgs_info = self.get_images(txt_path)
        self.train_flag = train_flag

        self.train_tf = transforms.Compose([
            transforms.Resize(224),                  # 调整图像大小为224x224
            transforms.RandomHorizontalFlip(),       #  随机左右翻转图像
            transforms.RandomVerticalFlip(),         # 随机上下翻转图像
            transforms.ToTensor(),                   # 将 PIL 图像转换为张量
            transform_BZ                             # 执行某些复杂变换操作
        ])
        self.val_tf = transforms.Compose([
            transforms.Resize(224),                  # 调整图像大小为224x224
            transforms.ToTensor(),                   # 将 PIL 图像转换为张量
            transform_BZ                             # 执行某些复杂变换操作
        ])

    def get_images(self, txt_path):
        with open(txt_path, 'r', encoding='utf-8') as f:
            imgs_info = f.readlines()
            imgs_info = list(map(lambda x: x.strip().split(' '), imgs_info))
        return imgs_info

    def padding_black(self, img):
        w, h = img.size
        scale = 224. / max(w, h)
        img_fg = img.resize([int(x) for x in [w * scale, h * scale]])
        size_fg = img_fg.size
        size_bg = 224
        img_bg = Image.new("RGB", (size_bg, size_bg))
        img_bg.paste(img_fg, ((size_bg - size_fg[0]) // 2,
                              (size_bg - size_fg[1]) // 2))

        img = img_bg
        return img

    def __getitem__(self, index):
        img_path, label = self.imgs_info[index]
        
        img_path = os.path.join('',img_path)
        img = Image.open(img_path)
        img = img.convert("RGB")
        img = self.padding_black(img)
        if self.train_flag:
            img = self.train_tf(img)
        else:
            img = self.val_tf(img)
        label = int(label)
        return img, label

    def __len__(self):
        return len(self.imgs_info)
```

In [5]

```
train_data = LoadData("/home/aistudio/work/train.txt", True)

valid_data = LoadData("/home/aistudio/work/valid.txt", True)
#数据读取
train_loader = paddle.io.DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = paddle.io.DataLoader(valid_data, batch_size=64, shuffle=True)
```

## 3.模型构建

In [6]

```
class Fire(nn.Layer):

    def __init__(self, inplanes, squeeze_planes,
                 expand1x1_planes, expand3x3_planes):
        super(Fire, self).__init__()
        self.inplanes = inplanes
        self.squeeze = nn.Conv2D(inplanes, squeeze_planes, kernel_size=1)
        self.squeeze_activation = nn.ReLU()
        self.expand1x1 = nn.Conv2D(squeeze_planes, expand1x1_planes,
                                   kernel_size=1)
        self.expand1x1_activation = nn.ReLU()
        self.expand3x3 = nn.Conv2D(squeeze_planes, expand3x3_planes,
                                   kernel_size=3, padding=1)
        self.expand3x3_activation = nn.ReLU()

    def forward(self, x):
        x = self.squeeze_activation(self.squeeze(x))
        return paddle.concat([
            self.expand1x1_activation(self.expand1x1(x)),
            self.expand3x3_activation(self.expand3x3(x))
        ], 1)

class SqueezeNet(nn.Layer):

    def __init__(self, version='1_0', num_classes=1000):
        super(SqueezeNet, self).__init__()
        self.num_classes = num_classes
        if version == '1_0':
            self.features = nn.Sequential(
                nn.Conv2D(3, 96, kernel_size=7, stride=2),
                nn.ReLU(),
                nn.MaxPool2D(kernel_size=3, stride=2, ceil_mode=True),
                Fire(96, 16, 64, 64),
                Fire(128, 16, 64, 64),
                Fire(128, 32, 128, 128),
                nn.MaxPool2D(kernel_size=3, stride=2, ceil_mode=True),
                Fire(256, 32, 128, 128),
                Fire(256, 48, 192, 192),
                Fire(384, 48, 192, 192),
                Fire(384, 64, 256, 256),
                nn.MaxPool2D(kernel_size=3, stride=2, ceil_mode=True),
                Fire(512, 64, 256, 256),
            )
        elif version == '1_1':
            self.features = nn.Sequential(
                nn.Conv2D(3, 64, kernel_size=3, stride=2),
                nn.ReLU(),
                nn.MaxPool2D(kernel_size=3, stride=2, ceil_mode=True),
                Fire(64, 16, 64, 64),
                Fire(128, 16, 64, 64),
                nn.MaxPool2D(kernel_size=3, stride=2, ceil_mode=True),
                Fire(128, 32, 128, 128),
                Fire(256, 32, 128, 128),
                nn.MaxPool2D(kernel_size=3, stride=2, ceil_mode=True),
                Fire(256, 48, 192, 192),
                Fire(384, 48, 192, 192),
                Fire(384, 64, 256, 256),
                Fire(512, 64, 256, 256),
            )
        else:
            # FIXME: Is this needed? SqueezeNet should only be called from the
            # FIXME: squeezenet1_x() functions
            # FIXME: This checking is not done for the other models
            raise ValueError("Unsupported SqueezeNet version {version}:"
                             "1_0 or 1_1 expected".format(version=version))

        # Final convolution is initialized differently from the rest
        final_conv = nn.Conv2D(512, self.num_classes, kernel_size=1)
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            final_conv,
            nn.ReLU(),
            nn.AdaptiveAvgPool2D((1, 1))
        )



    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return paddle.flatten(x, 1)
```

## 4.实例化模型

In [7]

```
import paddle
model = SqueezeNet("1_0", num_classes=4)
params_info = paddle.summary(model,(1, 3, 224, 224))
print(params_info)



W0812 15:21:12.231475   400 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0812 15:21:12.236858   400 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
-------------------------------------------------------------------------------
   Layer (type)         Input Shape          Output Shape         Param #    
===============================================================================
     Conv2D-1        [[1, 3, 224, 224]]   [1, 96, 109, 109]       14,208     
      ReLU-1        [[1, 96, 109, 109]]   [1, 96, 109, 109]          0       
    MaxPool2D-1     [[1, 96, 109, 109]]    [1, 96, 54, 54]           0       
     Conv2D-2        [[1, 96, 54, 54]]     [1, 16, 54, 54]         1,552     
      ReLU-2         [[1, 16, 54, 54]]     [1, 16, 54, 54]           0       
     Conv2D-3        [[1, 16, 54, 54]]     [1, 64, 54, 54]         1,088     
      ReLU-3         [[1, 64, 54, 54]]     [1, 64, 54, 54]           0       
     Conv2D-4        [[1, 16, 54, 54]]     [1, 64, 54, 54]         9,280     
      ReLU-4         [[1, 64, 54, 54]]     [1, 64, 54, 54]           0       
      Fire-1         [[1, 96, 54, 54]]     [1, 128, 54, 54]          0       
     Conv2D-5        [[1, 128, 54, 54]]    [1, 16, 54, 54]         2,064     
      ReLU-5         [[1, 16, 54, 54]]     [1, 16, 54, 54]           0       
     Conv2D-6        [[1, 16, 54, 54]]     [1, 64, 54, 54]         1,088     
      ReLU-6         [[1, 64, 54, 54]]     [1, 64, 54, 54]           0       
     Conv2D-7        [[1, 16, 54, 54]]     [1, 64, 54, 54]         9,280     
      ReLU-7         [[1, 64, 54, 54]]     [1, 64, 54, 54]           0       
      Fire-2         [[1, 128, 54, 54]]    [1, 128, 54, 54]          0       
     Conv2D-8        [[1, 128, 54, 54]]    [1, 32, 54, 54]         4,128     
      ReLU-8         [[1, 32, 54, 54]]     [1, 32, 54, 54]           0       
     Conv2D-9        [[1, 32, 54, 54]]     [1, 128, 54, 54]        4,224     
      ReLU-9         [[1, 128, 54, 54]]    [1, 128, 54, 54]          0       
     Conv2D-10       [[1, 32, 54, 54]]     [1, 128, 54, 54]       36,992     
      ReLU-10        [[1, 128, 54, 54]]    [1, 128, 54, 54]          0       
      Fire-3         [[1, 128, 54, 54]]    [1, 256, 54, 54]          0       
    MaxPool2D-2      [[1, 256, 54, 54]]    [1, 256, 27, 27]          0       
     Conv2D-11       [[1, 256, 27, 27]]    [1, 32, 27, 27]         8,224     
      ReLU-11        [[1, 32, 27, 27]]     [1, 32, 27, 27]           0       
     Conv2D-12       [[1, 32, 27, 27]]     [1, 128, 27, 27]        4,224     
      ReLU-12        [[1, 128, 27, 27]]    [1, 128, 27, 27]          0       
     Conv2D-13       [[1, 32, 27, 27]]     [1, 128, 27, 27]       36,992     
      ReLU-13        [[1, 128, 27, 27]]    [1, 128, 27, 27]          0       
      Fire-4         [[1, 256, 27, 27]]    [1, 256, 27, 27]          0       
     Conv2D-14       [[1, 256, 27, 27]]    [1, 48, 27, 27]        12,336     
      ReLU-14        [[1, 48, 27, 27]]     [1, 48, 27, 27]           0       
     Conv2D-15       [[1, 48, 27, 27]]     [1, 192, 27, 27]        9,408     
      ReLU-15        [[1, 192, 27, 27]]    [1, 192, 27, 27]          0       
     Conv2D-16       [[1, 48, 27, 27]]     [1, 192, 27, 27]       83,136     
      ReLU-16        [[1, 192, 27, 27]]    [1, 192, 27, 27]          0       
      Fire-5         [[1, 256, 27, 27]]    [1, 384, 27, 27]          0       
     Conv2D-17       [[1, 384, 27, 27]]    [1, 48, 27, 27]        18,480     
      ReLU-17        [[1, 48, 27, 27]]     [1, 48, 27, 27]           0       
     Conv2D-18       [[1, 48, 27, 27]]     [1, 192, 27, 27]        9,408     
      ReLU-18        [[1, 192, 27, 27]]    [1, 192, 27, 27]          0       
     Conv2D-19       [[1, 48, 27, 27]]     [1, 192, 27, 27]       83,136     
      ReLU-19        [[1, 192, 27, 27]]    [1, 192, 27, 27]          0       
      Fire-6         [[1, 384, 27, 27]]    [1, 384, 27, 27]          0       
     Conv2D-20       [[1, 384, 27, 27]]    [1, 64, 27, 27]        24,640     
      ReLU-20        [[1, 64, 27, 27]]     [1, 64, 27, 27]           0       
     Conv2D-21       [[1, 64, 27, 27]]     [1, 256, 27, 27]       16,640     
      ReLU-21        [[1, 256, 27, 27]]    [1, 256, 27, 27]          0       
     Conv2D-22       [[1, 64, 27, 27]]     [1, 256, 27, 27]       147,712    
      ReLU-22        [[1, 256, 27, 27]]    [1, 256, 27, 27]          0       
      Fire-7         [[1, 384, 27, 27]]    [1, 512, 27, 27]          0       
    MaxPool2D-3      [[1, 512, 27, 27]]    [1, 512, 13, 13]          0       
     Conv2D-23       [[1, 512, 13, 13]]    [1, 64, 13, 13]        32,832     
      ReLU-23        [[1, 64, 13, 13]]     [1, 64, 13, 13]           0       
     Conv2D-24       [[1, 64, 13, 13]]     [1, 256, 13, 13]       16,640     
      ReLU-24        [[1, 256, 13, 13]]    [1, 256, 13, 13]          0       
     Conv2D-25       [[1, 64, 13, 13]]     [1, 256, 13, 13]       147,712    
      ReLU-25        [[1, 256, 13, 13]]    [1, 256, 13, 13]          0       
      Fire-8         [[1, 512, 13, 13]]    [1, 512, 13, 13]          0       
     Dropout-1       [[1, 512, 13, 13]]    [1, 512, 13, 13]          0       
     Conv2D-26       [[1, 512, 13, 13]]     [1, 4, 13, 13]         2,052     
      ReLU-26         [[1, 4, 13, 13]]      [1, 4, 13, 13]           0       
AdaptiveAvgPool2D-1   [[1, 4, 13, 13]]       [1, 4, 1, 1]            0       
===============================================================================
Total params: 737,476
Trainable params: 737,476
Non-trainable params: 0
-------------------------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 89.22
Params size (MB): 2.81
Estimated Total Size (MB): 92.61
-------------------------------------------------------------------------------

{'total_params': 737476, 'trainable_params': 737476}
```

## 5.开始训练

In [8]

```
epoch_num = 40 #训练轮数
learning_rate = 0.0001 #学习率


val_acc_history = []
val_loss_history = []
bestacc=0

def train(model):
    print('start training ... ')
    # turn into training mode
    model.train()

    opt = paddle.optimizer.Adam(learning_rate=learning_rate,
                                parameters=model.parameters())

    for epoch in range(epoch_num):
        acc_train = []
        for batch_id, data in enumerate(train_loader()):
            x_data = data[0]
            y_data = paddle.to_tensor(data[1],dtype="int64")
            y_data = paddle.unsqueeze(y_data, 1)
            logits = model(x_data)
            loss = F.cross_entropy(logits, y_data)
            acc = paddle.metric.accuracy(logits, y_data)
            acc_train.append(acc.numpy())
            if batch_id % 100 == 0:
                print("epoch: {}, batch_id: {}, loss is: {}".format(epoch, batch_id, loss.numpy()))
                avg_acc = np.mean(acc_train)
                print("[train] accuracy: {}".format(avg_acc))
            loss.backward()
            opt.step()
            opt.clear_grad()
        
        # evaluate model after one epoch
        model.eval()
        accuracies = []
        losses = []
        for batch_id, data in enumerate(test_loader()):
            x_data = data[0]
            y_data = paddle.to_tensor(data[1],dtype="int64")
            y_data = paddle.unsqueeze(y_data, 1)

            logits = model(x_data)
            loss = F.cross_entropy(logits, y_data)
            acc = paddle.metric.accuracy(logits, y_data)
            accuracies.append(acc.numpy())
            losses.append(loss.numpy())

        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)
        print("[test] accuracy/loss: {}/{}".format(avg_acc, avg_loss))
        val_acc_history.append(avg_acc)
        val_loss_history.append(avg_loss)
        if bestacc<acc:
            bestacc=acc
            paddle.save(net.state_dict(), "resnet1_net.pdparams")
        model.train()


epoch: 38, batch_id: 0, loss is: [0.25577748]
[train] accuracy: 0.921875
[test] accuracy/loss: 0.9709821343421936/0.09465523809194565
epoch: 39, batch_id: 0, loss is: [0.11098497]
[train] accuracy: 0.953125
[test] accuracy/loss: 0.9575892686843872/0.13277693092823029
```

## 6.结果可视化

In [9]

```
import matplotlib.pyplot as plt
#隐藏警告
import warnings
warnings.filterwarnings("ignore")               #忽略警告信息

epochs_range = range(epoch_num)

plt.figure(figsize=(12, 3))
plt.subplot(1, 2, 1)

plt.plot(epochs_range, val_acc_history, label='Val Accuracy')
plt.legend(loc='lower right')
plt.title('Val Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, val_loss_history, label='Val Loss')
plt.legend(loc='upper right')
plt.title('Val Loss')
plt.show()
```

![下载 (82)](image\下载 (82).png)

## 总结

SqueezeNet是一种轻量级卷积神经网络，其设计目标是在保持高准确率的同时，尽量减小模型的大小和计算资源消耗。下面是关于SqueezeNet的总结：

1. 轻量级设计：SqueezeNet采用了一种特殊的结构，即“Fire模块”，通过使用较少的参数来提取丰富的特征。这使得SqueezeNet相对于其他深层网络而言具有更小的模型大小。
2. 参数压缩：SqueezeNet通过使用1x1卷积核来减小参数数量，同时使用通道压缩来减小计算量。这样的设计使得SqueezeNet在计算资源受限的环境中表现出色。
3. 高准确率：尽管SqueezeNet是一种轻量级网络，但它在保持模型小型化的同时，仍能提供相对较高的准确率。通过合理的设计，SqueezeNet能够有效地提取和利用图像的特征信息。
4. 适用场景：由于其小型化的特点，SqueezeNet特别适合在资源有限的环境下使用，如移动设备和嵌入式系统。它可以实现图像分类、目标检测和图像分割等计算密集型任务。

总的来说，SqueezeNet是一种在保持高准确率的同时尽量减小模型大小和计算资源消耗的轻量级网络。它的设计和参数压缩策略使其成为在资源受限环境下进行图像处理任务的有力选择。













































