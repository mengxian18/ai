# 大模型微调

## 1.大模型微调的前沿技术与应用

### 1-为什么需要微调 大模型

1. 预训练成本高（65B 需要780GB 显存） 
2. 提示工程有天花板（token 上限与推理成本） 
3. 基础模型缺少特定领域数据 
4. 数据安全和隐私 
5. 个性化服务需要私有化的微调大模型

### 2-大模型微调开源框架与工具

![Snipaste_2025-03-18_15-23-35](image\Snipaste_2025-03-18_15-23-35.png)

![Snipaste_2025-03-18_15-27-35](image\Snipaste_2025-03-18_15-27-35.png)

###  3-未来趋势

1. **新架构的探索**：随着 Transformer可能被更 高效或更适应性强的架 构所取代，研究者将探 索新的神经网络架构， 以提升处理速度、降低 计算成本，并提高模型 的泛化能力。 
2. **模型压缩和优化**：为了 应对日益增长的计算需 求，模型压缩和优化技 术将成为研究的热点， 以减少模型的大小和提 高运行效率。 
3. **自动化模型设计** （AutoML）：自动化 机器学习将在大模型设 计中发挥更大的作用， 尤其是在寻找替代 Transformer的新架构 方面。 
4. **跨模态学习**：跨模态学 习，即同时处理文本、 图像和声音等不同类型 数据的能力，将成为大 模型发展的一个关键方 向。 
5. **更广泛的应用范围**：随 着新架构的出现和技术 的进步，大模型将在更 多领域（如健康医疗、 金融等）得到应用

### 4-挑战

- • **架构创新的复杂性**：设计能够超越Transformer的新架构将面 临巨大的技术挑战，特别是在保持或提高效率和效果的同时减 少计算资源需求。
-  • **适应新架构的微调技术**：随着基础架构的变化，现有的微调 技术可能需要重大调整或重新设计，以适应新的模型架构。
-  • **模型可解释性**：新的架构可能会带来更复杂的模型内部结构， 这可能会进一步加剧模型可解释性和透明度的问题。 
- • **迁移学习的挑战**：新架构可能会使得从旧模型到新模型的迁 移变得更加困难，特别是在保留已有知识和经验方面。
-  • **伦理和社会责任**：新架构可能会在不同程度上放大或缓解目 前模型的偏见和不平等问题，如何确保技术的公正性和负责任 使用将持续是一个挑战。

## 2.AI 大模型四阶技术总览

### 1-发展阶段

![Snipaste_2025-03-18_15-37-21](image\Snipaste_2025-03-18_15-37-21.png)

### 2-技术阶段

![Snipaste_2025-03-18_15-46-18](image\Snipaste_2025-03-18_15-46-18.png)

### 3-基于GPT 的Prompt 技巧最佳实践

• **角色设定**：擅于使用 System 给GPT设定角色和任务，如“哲学大师”；

• **指令注入**：在 System 中注入常驻任务指令，如“主题创作”； 

• **问题拆解**：将复杂问题拆解成的子问题，分步骤执行，如：Debug 和多任务；

• **分层设计**：创作长篇内容，分层提问，先概览再章节，最后补充细节，如：小说生成； 

• **编程思维**：将prompt当做编程语言，主动设计变量、模板和正文，如：评估模型输出质量；

• **Few-Shot**：基于样例的prompt设计，规范推理路径和输出样式，如：构造训练数据；

使用 Prompt 构造标注数据

使用 Prompt 帮助 Debug

### 4-预训练语言模型的三种网络架构

![Snipaste_2025-03-18_15-54-52](image\Snipaste_2025-03-18_15-54-52.png)



## 3.大语言模型技术发展与演进

![Snipaste_2025-03-18_15-59-02](image\Snipaste_2025-03-18_15-59-02.png)

统计语言模型、两个问题：1）参数空间太大；2）模型过于稀疏；

NNLM 普遍采用 RNN / LSTM 

两个问题： 1. 长距离依赖（梯度消失） 2. 计算效率（RNN 难以并行）

**注意力机制**的特点和优势

1. 注意力机制有助于克服循环神经网络（RNNs）的一些挑战，例如输 入序列长度增加时性能下降和顺序处理输入导致的计算效率低下。 
2.  在自然语言处理（NLP）、计算机视觉（Computer Vision）、跨模 态任务和推荐系统等多个领域中，注意力机制已成为多项任务中的最 先进模型，取得了显著的性能提升。
3.   注意力机制不仅可以提高主要任务的性能，还具有其他优势。它们被 广泛用于提高神经网络的可解释性，帮助解释模型的决策过程，使得 原本被认为是黑盒模型的神经网络变得更易解释。这对于人们对机器 学习模型的公平性、可追溯性和透明度的关注具有重要意义。

### 1-机器学习分类

![Snipaste_2025-03-18_16-08-07](image\Snipaste_2025-03-18_16-08-07.png)

### 2-BERT价值

![Snipaste_2025-03-18_16-19-58](image\Snipaste_2025-03-18_16-19-58.png)

### 3-BERT VS GPT

![Snipaste_2025-03-18_16-21-05](image\Snipaste_2025-03-18_16-21-05.png)

![Snipaste_2025-03-18_16-21-24](image\Snipaste_2025-03-18_16-21-24.png)

## 4-大模型微调技术揭秘-PEFT

BERT 验证可行：预训练+下游任务微调（2018） 两个问题： 1）预训练成本高； 2）新任务需要重新微调；

Adapter Tuning: 开启大模型 PEFT (2019）

### 1.Adapter 核心技术解读

Adapter 嵌入 Transformer 网络： 

• 在两个FNN层厚增加Adapter层

• Adapter 内部学习降维后特征，减少参数 

•使用skip-connection，最差退化为 identity 

• 提升微调效率和稳定性，可复用性增强

### 2.Prefix Tuning 核心技术解读

Prefix 嵌入 Transformer 网络： 

- • 在预训练Transformer 前增加Prefix 模块 
- • 仅训练Prefix，冻结Transformer 全部参数 
- • 降低GPU算力和训练时间成本 
- • 特别适合于那些参数数量庞大的模型，如 GPT-3，使微调这些模型成为可能。 

Prefix：任务特定的指令集”，引导模型生成 特定任务的输出。

### 3.Prompt Tuning 主要贡献与训练方法：

- • 直观性：Prompt tuning 使用直观的语言提示来引 导模型，使其更易于理解和操作。
-  • 适用性：这种方法特别适用于那些预训练模型已经 掌握了大量通用知识的情况，通过简单的提示就能 激发特定的响应。
-  • 微调成本低：prompt tuning 可以在微调时减少所 需的计算资源，同时保持良好的性能。

1. 设计提示：根据任务选择硬提示（固定文本）或 软提示（可训练向量）作为输入。 
2.  融入输入：硬提示直接加入文本，软提示作为向 量加入序列。 
3.  训练过程：硬提示下全面微调模型；软提示下只 调整提示向量，其他参数不变。 
4. 执行任务：训练后模型用于NLP任务（如问答、摘 要），输出由提示引导

### 4.P-Tuning v1: 解决人工设计Prompt 的问题(2021)

 P-Tuning 的创新之处在于将提示（Prompt）转化为 可学习的嵌入层（Embedding Layer），但直接对 嵌入层参数进行优化时面临两大挑战： 

1.离散性（Discreteness）：已经通过预训练优化过 的正常语料嵌入层与直接对输入提示嵌入进行随机 初始化训练相比，可能会导致后者陷入局部最优解。 

2.关联性（Association）：这种方法难以有效捕捉 提示嵌入之间的相互关系。

### 5.P-Tuning 和 Prefix-Tuning 主要区别在于：

 • Prefix Tuning 类似于模仿指令，通过在模型开头 加入额外的嵌入（embedding），而P-Tuning  的嵌入位置更为灵活。 

• Prefix Tuning 在每个注意力层增加前缀嵌入来引 入额外参数，并用多层感知机（MLP）进行初始 化；相比之下，P-Tuning 仅在输入时加入嵌入， 并通过长短期记忆网络（LSTM）加MLP进行初 始化。

### 6.P-Tuning 在小模型上性能不佳。

 P-Tuning v2 旨在使提示调整（Prompt Tuning）在不同规模 的预训练模型上，针对各种下游任务都能达到类似全面微调 （Fine-tuning）的效果。 之前的方法在以下两方面有所限制： 

• 模型规模差异：在大型预训练模型中，Prompt Tuning 和 P-Tuning 能取得与全面微调相似的效果，但在参数较少 的模型上则表现不佳。 

• 任务类型差异：无论是Prompt Tuning 还是P-Tuning， 在序列标注任务上的表现都较差。

## 5-大模型微调技术揭秘-LoRA

为了使微调更加高效，LoRA的方法是通过低秩分解将权重 更新表示为两个较小的矩阵（称为更新矩阵）。这些新矩阵可以在适应新数据的同时保持整体变化数量较少进行训 练。

原始权重矩阵保持冻结状态，并且不再接受任何进一步的 调整。最终结果是通过将原始权重和适应后的权重进行组 合得到

### 1.LoRA 核心技术

![Snipaste_2025-03-18_17-29-47](image\Snipaste_2025-03-18_17-29-47.png)

### 2.LoRA 相比Adapter 方法的优势

![Snipaste_2025-03-18_17-34-05](image\Snipaste_2025-03-18_17-34-05.png)

### 3.LoRA 相比Soft Prompts 方法的优势

![Snipaste_2025-03-18_17-34-50](image\Snipaste_2025-03-18_17-34-50.png)

### 4.LoRA 核心思想： • 对下游任务增量训练小模型

## 6-大模型开发工具库 Hugging Face Transformers

![Snipaste_2025-03-19_10-35-46](image\Snipaste_2025-03-19_10-35-46.png)

### 1.Hugging Face Transformers 库独特价值

 1.丰富的预训练模型：提供广泛的预训练模型，如BERT、GPT、T5等，适用于各种NLP任务。

2. 易于使用：设计注重易用性，使得即使没有深厚机器学习背景的开发者也能快速上手。 
3.  最新研究成果的快速集成：经常更新，包含最新的研究成果和模型。 
4. 强大的社区支持：活跃的社区不断更新和维护库，提供技术支持和新功能。 
5.  跨框架兼容性：支持多种深度学习框架，如PyTorch、TensorFlow，提供灵活选择。 
6.  高度灵活和可定制化：允许用户根据需求定制和调整模型，进行微调或应用于特定任务。 
7. 广泛的应用范围：适用于从文本分类到语言生成等多种NLP应用，以及其他模态的扩展。

使用 Pipelines 快速实践大模型

使用 Pipelines 实现智能问答语音识别图像分类

