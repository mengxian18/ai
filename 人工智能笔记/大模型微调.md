# 大模型微调

## 1.大模型微调的前沿技术与应用

### 1-为什么需要微调 大模型

1. 预训练成本高（65B 需要780GB 显存） 
2. 提示工程有天花板（token 上限与推理成本） 
3. 基础模型缺少特定领域数据 
4. 数据安全和隐私 
5. 个性化服务需要私有化的微调大模型

### 2-大模型微调开源框架与工具

![Snipaste_2025-03-18_15-23-35](image\Snipaste_2025-03-18_15-23-35.png)

![Snipaste_2025-03-18_15-27-35](image\Snipaste_2025-03-18_15-27-35.png)

###  3-未来趋势

1. **新架构的探索**：随着 Transformer可能被更 高效或更适应性强的架 构所取代，研究者将探 索新的神经网络架构， 以提升处理速度、降低 计算成本，并提高模型 的泛化能力。 
2. **模型压缩和优化**：为了 应对日益增长的计算需 求，模型压缩和优化技 术将成为研究的热点， 以减少模型的大小和提 高运行效率。 
3. **自动化模型设计** （AutoML）：自动化 机器学习将在大模型设 计中发挥更大的作用， 尤其是在寻找替代 Transformer的新架构 方面。 
4. **跨模态学习**：跨模态学 习，即同时处理文本、 图像和声音等不同类型 数据的能力，将成为大 模型发展的一个关键方 向。 
5. **更广泛的应用范围**：随 着新架构的出现和技术 的进步，大模型将在更 多领域（如健康医疗、 金融等）得到应用

### 4-挑战

- • **架构创新的复杂性**：设计能够超越Transformer的新架构将面 临巨大的技术挑战，特别是在保持或提高效率和效果的同时减 少计算资源需求。
-  • **适应新架构的微调技术**：随着基础架构的变化，现有的微调 技术可能需要重大调整或重新设计，以适应新的模型架构。
-  • **模型可解释性**：新的架构可能会带来更复杂的模型内部结构， 这可能会进一步加剧模型可解释性和透明度的问题。 
- • **迁移学习的挑战**：新架构可能会使得从旧模型到新模型的迁 移变得更加困难，特别是在保留已有知识和经验方面。
-  • **伦理和社会责任**：新架构可能会在不同程度上放大或缓解目 前模型的偏见和不平等问题，如何确保技术的公正性和负责任 使用将持续是一个挑战。

## 2.AI 大模型四阶技术总览

### 1-发展阶段

![Snipaste_2025-03-18_15-37-21](image\Snipaste_2025-03-18_15-37-21.png)

### 2-技术阶段

![Snipaste_2025-03-18_15-46-18](image\Snipaste_2025-03-18_15-46-18.png)

### 3-基于GPT 的Prompt 技巧最佳实践

• **角色设定**：擅于使用 System 给GPT设定角色和任务，如“哲学大师”；

• **指令注入**：在 System 中注入常驻任务指令，如“主题创作”； 

• **问题拆解**：将复杂问题拆解成的子问题，分步骤执行，如：Debug 和多任务；

• **分层设计**：创作长篇内容，分层提问，先概览再章节，最后补充细节，如：小说生成； 

• **编程思维**：将prompt当做编程语言，主动设计变量、模板和正文，如：评估模型输出质量；

• **Few-Shot**：基于样例的prompt设计，规范推理路径和输出样式，如：构造训练数据；

使用 Prompt 构造标注数据

使用 Prompt 帮助 Debug

### 4-预训练语言模型的三种网络架构

![Snipaste_2025-03-18_15-54-52](image\Snipaste_2025-03-18_15-54-52.png)



## 3.大语言模型技术发展与演进

![Snipaste_2025-03-18_15-59-02](image\Snipaste_2025-03-18_15-59-02.png)

统计语言模型、两个问题：1）参数空间太大；2）模型过于稀疏；

NNLM 普遍采用 RNN / LSTM 

两个问题： 1. 长距离依赖（梯度消失） 2. 计算效率（RNN 难以并行）

**注意力机制**的特点和优势

1. 注意力机制有助于克服循环神经网络（RNNs）的一些挑战，例如输 入序列长度增加时性能下降和顺序处理输入导致的计算效率低下。 
2.  在自然语言处理（NLP）、计算机视觉（Computer Vision）、跨模 态任务和推荐系统等多个领域中，注意力机制已成为多项任务中的最 先进模型，取得了显著的性能提升。
3.   注意力机制不仅可以提高主要任务的性能，还具有其他优势。它们被 广泛用于提高神经网络的可解释性，帮助解释模型的决策过程，使得 原本被认为是黑盒模型的神经网络变得更易解释。这对于人们对机器 学习模型的公平性、可追溯性和透明度的关注具有重要意义。

### 1-机器学习分类

![Snipaste_2025-03-18_16-08-07](image\Snipaste_2025-03-18_16-08-07.png)

### 2-BERT价值

![Snipaste_2025-03-18_16-19-58](image\Snipaste_2025-03-18_16-19-58.png)

### 3-BERT VS GPT

![Snipaste_2025-03-18_16-21-05](image\Snipaste_2025-03-18_16-21-05.png)

![Snipaste_2025-03-18_16-21-24](image\Snipaste_2025-03-18_16-21-24.png)

## 4-大模型微调技术揭秘-PEFT

BERT 验证可行：预训练+下游任务微调（2018） 两个问题： 1）预训练成本高； 2）新任务需要重新微调；

Adapter Tuning: 开启大模型 PEFT (2019）

### 1.Adapter 核心技术解读

Adapter 嵌入 Transformer 网络： 

• 在两个FNN层厚增加Adapter层

• Adapter 内部学习降维后特征，减少参数 

•使用skip-connection，最差退化为 identity 

• 提升微调效率和稳定性，可复用性增强

### 2.Prefix Tuning 核心技术解读

Prefix 嵌入 Transformer 网络： 

- • 在预训练Transformer 前增加Prefix 模块 
- • 仅训练Prefix，冻结Transformer 全部参数 
- • 降低GPU算力和训练时间成本 
- • 特别适合于那些参数数量庞大的模型，如 GPT-3，使微调这些模型成为可能。 

Prefix：任务特定的指令集”，引导模型生成 特定任务的输出。

### 3.Prompt Tuning 主要贡献与训练方法：

- • 直观性：Prompt tuning 使用直观的语言提示来引 导模型，使其更易于理解和操作。
-  • 适用性：这种方法特别适用于那些预训练模型已经 掌握了大量通用知识的情况，通过简单的提示就能 激发特定的响应。
-  • 微调成本低：prompt tuning 可以在微调时减少所 需的计算资源，同时保持良好的性能。

1. 设计提示：根据任务选择硬提示（固定文本）或 软提示（可训练向量）作为输入。 
2.  融入输入：硬提示直接加入文本，软提示作为向 量加入序列。 
3.  训练过程：硬提示下全面微调模型；软提示下只 调整提示向量，其他参数不变。 
4. 执行任务：训练后模型用于NLP任务（如问答、摘 要），输出由提示引导

### 4.P-Tuning v1: 解决人工设计Prompt 的问题(2021)

 P-Tuning 的创新之处在于将提示（Prompt）转化为 可学习的嵌入层（Embedding Layer），但直接对 嵌入层参数进行优化时面临两大挑战： 

1.离散性（Discreteness）：已经通过预训练优化过 的正常语料嵌入层与直接对输入提示嵌入进行随机 初始化训练相比，可能会导致后者陷入局部最优解。 

2.关联性（Association）：这种方法难以有效捕捉 提示嵌入之间的相互关系。

### 5.P-Tuning 和 Prefix-Tuning 主要区别在于：

 • Prefix Tuning 类似于模仿指令，通过在模型开头 加入额外的嵌入（embedding），而P-Tuning  的嵌入位置更为灵活。 

• Prefix Tuning 在每个注意力层增加前缀嵌入来引 入额外参数，并用多层感知机（MLP）进行初始 化；相比之下，P-Tuning 仅在输入时加入嵌入， 并通过长短期记忆网络（LSTM）加MLP进行初 始化。

### 6.P-Tuning 在小模型上性能不佳。

 P-Tuning v2 旨在使提示调整（Prompt Tuning）在不同规模 的预训练模型上，针对各种下游任务都能达到类似全面微调 （Fine-tuning）的效果。 之前的方法在以下两方面有所限制： 

• 模型规模差异：在大型预训练模型中，Prompt Tuning 和 P-Tuning 能取得与全面微调相似的效果，但在参数较少 的模型上则表现不佳。 

• 任务类型差异：无论是Prompt Tuning 还是P-Tuning， 在序列标注任务上的表现都较差。

## 5-大模型微调技术揭秘-LoRA

为了使微调更加高效，LoRA的方法是通过低秩分解将权重 更新表示为两个较小的矩阵（称为更新矩阵）。这些新矩阵可以在适应新数据的同时保持整体变化数量较少进行训 练。

原始权重矩阵保持冻结状态，并且不再接受任何进一步的 调整。最终结果是通过将原始权重和适应后的权重进行组 合得到

### 1.LoRA 核心技术

![Snipaste_2025-03-18_17-29-47](image\Snipaste_2025-03-18_17-29-47.png)

### 2.LoRA 相比Adapter 方法的优势

![Snipaste_2025-03-18_17-34-05](image\Snipaste_2025-03-18_17-34-05.png)

### 3.LoRA 相比Soft Prompts 方法的优势

![Snipaste_2025-03-18_17-34-50](image\Snipaste_2025-03-18_17-34-50.png)

### 4.LoRA 核心思想： • 对下游任务增量训练小模型

## 6-大模型开发工具库 Hugging Face Transformers

![Snipaste_2025-03-19_10-35-46](image\Snipaste_2025-03-19_10-35-46.png)

### 1.Hugging Face Transformers 库独特价值

 1.丰富的预训练模型：提供广泛的预训练模型，如BERT、GPT、T5等，适用于各种NLP任务。

2. 易于使用：设计注重易用性，使得即使没有深厚机器学习背景的开发者也能快速上手。 
3.  最新研究成果的快速集成：经常更新，包含最新的研究成果和模型。 
4. 强大的社区支持：活跃的社区不断更新和维护库，提供技术支持和新功能。 
5.  跨框架兼容性：支持多种深度学习框架，如PyTorch、TensorFlow，提供灵活选择。 
6.  高度灵活和可定制化：允许用户根据需求定制和调整模型，进行微调或应用于特定任务。 
7. 广泛的应用范围：适用于从文本分类到语言生成等多种NLP应用，以及其他模态的扩展。

使用 Pipelines 快速实践大模型

使用 Pipelines 实现智能问答语音识别图像分类

# 量化

## 量化在模型训练中的应用

- **数据量化**：在机器学习和大模型训练中，数据是基础。量化数据意味着将原始数据进行标准化、归一化等处理。例如，将图像数据的像素值归一化到 0 - 1 区间，或者将文本数据通过词向量等方式转化为数字向量，以便模型能够更好地处理和学习。这样做可以加速模型收敛，提高训练效率，同时减少数据的存储空间和传输带宽。
- **模型参数量化**：模型参数通常以浮点数形式存储和计算，但可以将其量化为低精度的数据类型，如整数或定点数。例如，将 32 位浮点数的模型参数量化为 8 位整数。这不仅可以减少模型的存储空间，还能加快模型的推理速度，降低计算能耗。在训练过程中，也可以采用量化训练的方法，即在保持模型精度的前提下，使用低精度数据进行计算，以提高训练效率。

## 量化在模型评估中的应用

- **性能指标量化**：通过一系列量化的指标来评估模型的性能。常见的指标有准确率、召回率、F1 值等用于分类任务，均方误差（MSE）、平均绝对误差（MAE）等用于回归任务。以图像分类模型为例，准确率是指模型正确分类的图像数量与总图像数量的比值，通过这个量化指标可以直观地了解模型的分类性能。这些指标能够帮助开发者准确衡量模型的优劣，以便进行优化和改进。
- **模型复杂度量化**：模型复杂度也是一个重要的量化方面，例如通过计算模型的参数数量、神经网络的层数、计算量（如浮点运算次数 FLOPs）等来衡量。一般来说，参数数量越多、层数越深、计算量越大，模型的复杂度越高。量化模型复杂度有助于在模型性能和资源消耗之间进行权衡，选择合适复杂度的模型以适应不同的应用场景。

## 量化在模型优化中的应用

- **超参数调整**：超参数是在模型训练之前需要设定的参数，如学习率、正则化系数、迭代次数等。通过量化的方法来调整超参数，例如采用网格搜索、随机搜索或更高级的启发式搜索算法（如遗传算法、模拟退火算法等），在超参数空间中搜索最优的参数组合。这些搜索算法通过量化评估模型在不同超参数组合下的性能，来找到使模型性能最优的超参数设置。
- **模型压缩**：除了参数量化外，还可以通过其他量化方法进行模型压缩，如剪枝。剪枝是指去除模型中不重要的连接或参数，将其量化为零。通过对模型进行剪枝，可以在不显著影响模型性能的前提下，减少模型的存储空间和计算量，提高模型的运行效率。同时，还可以结合量化后的参数存储和计算，进一步优化模型。

### 量化在大模型中的挑战与解决方案

- 挑战
  - **精度损失**：量化过程中由于数据精度的降低，可能导致模型精度下降，尤其是在大模型中，微小的精度损失可能会对整体性能产生较大影响。
  - **复杂的量化策略**：大模型结构复杂，不同层和参数对量化的敏感度不同，需要设计复杂的量化策略来平衡精度和压缩率。
  - **硬件适配问题**：现有的硬件设备可能不完全支持低精度计算，导致量化后的模型在实际运行中无法充分发挥优势，或者需要额外的硬件支持来实现高效的量化计算。
- 解决方案
  - **改进量化算法**：研究和开发更先进的量化算法，如混合精度量化，即对不同的模型参数采用不同的量化精度，对敏感参数采用较高精度，对相对不敏感参数采用较低精度，以在保证精度的同时实现较好的压缩效果。
  - **量化感知训练**：在模型训练过程中考虑量化因素，通过特殊的训练方法使模型适应量化后的环境，减少精度损失。例如，在训练过程中模拟量化误差，让模型学习如何在存在量化误差的情况下保持性能。
  - **硬件优化**：推动硬件厂商开发支持低精度计算的专用硬件，如支持量化计算的图形处理单元（GPU）或专用集成电路（ASIC），以提高量化模型的运行效率，充分发挥量化的优势。





# DeepSeek-R1-7b全量微调（SFT）技术教程

微调（Fine-tuning）是一种典型的大模型（LLM）后训练技术，通过特定领域的数据对预训练模型的参数进行调整，使其适应新任务或领域。模型本身的权重被修改，从而内化新知识。特别适用于医疗、法律、教育等垂直领域的大模型应用。大模型微调包括有监督微调（SFT）和参数高效微调（PEFT）两种方式。有监督微调（SFT）SFT一般需要对预训练模型所有参数进行更新，所以也叫全参数微调、全量微调。

微调（Fine-tuning）是一种典型的大模型（LLM）后训练技术，通过特定领域的数据对预训练模型的参数进行调整，使其适应新任务或领域。模型本身的权重被修改，从而内化新知识。特别适用于医疗、法律、教育等垂直领域的大模型应用。

大模型微调包括有监督微调（SFT）和参数高效微调（PEFT）两种方式。

## 有监督微调（SFT）

SFT一般需要对预训练模型所有参数进行更新，所以也叫全参数微调、全量微调。SFT一般需要较多的高质量微调数据，对算力要求也非常高，一个7b的模型，全量微调通常需要参数量16~20倍的GPU显存，也就是说至少需要两张80G的A100显卡才能训的动。但优点是模型能够真正内化领域知识，训练效果也会相对较优。

假设一个7b的模型，模型参数用fp16存储，需要7*2=14G的显存；梯度如果用fp32存储，需要7*4=28G显存；然后是优化器状态，比如Adam优化器需要保存动量与方差，同样采用fp32存储，则需要7*4*2=56G显存；同时考虑训练过程中激活值的内存占用（取决于训练的batchsize和序列长度），可能需要14~20G左右的显存，除此之外PyTorch训练框架的缓存和日志也需要占用一部分内存，假设10G左右。

那么全量微调的显存计算公式为：

参数（2倍）+梯度（4倍）+优化器（8倍）+激活值（2倍）+框架开销（1.5倍）≈ 17.5倍

## 参数高效微调（PEFT）

鉴于全量微调对数据要求比较高，并且训练成本昂贵，所以PEFT方法受到了广泛的欢迎。PEFT旨在通过训练少量参数来使得模型适配下游任务，最典型的PEFT方法是LoRA（低秩适配器）。

LoRA训练时，不改变模型原有的参数权重，而是在权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用来模拟参数的变化量。如下图所示，将可训练参数矩阵ΔW低秩分解为矩阵A和B，矩阵A通过高斯函数初始化，矩阵B通过零初始化。

一个7b的模型，采用LoRA训练通常只需要两张24G的3090或4090显卡即可，并且对数据量要求不高，几百条数据即可开训，半小时内即可训练完成，非常高效。并且LoRA是一种可插拔式的适配器模型，当我们想要实现不同的模型风格效果时，在基础模型不变的情况下，可以训练多个LoRA与基础模型切换和适配，非常灵活。但缺点就是训练效果不如全量微调，还极有可能使得模型能力退化。

SFT vs. PEFT

SFT与PEFT的综合对比如下表所示：

![c898f515f7b40be15c829906a56c97d9](image\c898f515f7b40be15c829906a56c97d9.png)

![61d69dbe2b29762bf146b67e8d56588b](image\61d69dbe2b29762bf146b67e8d56588b.png)

## LLaMA-Factory

LLama-Factory 是一个基于 Hugging Face Transformers 构建的开源大语言模型微调框架，专注于简化大模型的高效训练与适配流程。其核心目标是通过模块化设计降低技术门槛，支持用户快速针对垂直场景定制专属模型。因为LLama-Factory简单易上手，并且配有图形化的Web UI界面，所以本文主要基于LLaMA-Factory框架进行[DeepSeek](https://link.csdn.net/?target=https%3A%2F%2Fdownload.csdn.net%2Fdownload%2FMr_Roki%2F90383108%3Flogin%3Dfrom_csdn)-R1-7b模型的全参数微调。

数据集
在模型和框架固定的情况下，其实数据集才是最大的困难。DeepSeek-R1是一个推理（Reasoning）模型，要对其进行微调需要高质量的带有思维链（CoT）的数据集，但对于垂类推理大模型的微调，构建数据集通常是成本最高、可控性最差的一个环节。 
本文我们使用ModelScope平台

## 微调参数讲解

![011cd0ebf3a849d3b9b8fd2d7c412fcb](image\011cd0ebf3a849d3b9b8fd2d7c412fcb.png)

详细的参数解释如下

> 1. **模型基础信息**：
> 2. 1. `model_name_or_path: Qwen2.5-7B-Instruct`：指定了要微调的基础模型的路径或名称。这里表明基础模型是存放在Qwen2.5-7B-Instruct路径下的`qwen`模型。
>    2. `quantization_bit: 4`：表示将模型的权重进行量化的位数为 4 位。量化是一种压缩技术，可以减少模型的存储和计算需求，但可能会对模型的精度产生一定影响。通过将权重量化到 4 位，可以在一定程度上提高模型的运行效率。
> 3. **微调方法**：
> 4. 1. `stage: sft`：这里的`sft`代表“Supervised Fine-Tuning”（有监督微调），即使用有标注的数据对模型进行微调训练。
>    2. `do_train: true`：表示要进行训练操作。
>    3. `finetuning_type: lora`：使用“LoRA”（Low-Rank Adaptation）方法进行微调。LoRA 是一种高效的微调技术，它通过在原始模型的基础上添加一些低秩矩阵来实现对模型的微调，从而减少了训练的参数数量和计算成本。
>    4. `lora_target: all`：表示对模型的所有部分都应用 LoRA 微调。
> 5. **数据集相关**：
> 6. 1. `dataset: identity`：指定了使用的数据集`identity`
>    2. `template: qwen`：指定了数据的模板或格式与`qwen`模型相匹配。这有助于将数据集转换为适合模型输入的格式。
>    3. `cutoff_len: 1024`：设置输入文本的截断长度为 1024。如果输入文本超过这个长度，会被截断以适应模型的处理能力。
>    4. `max_samples: 1000`：限制数据集中使用的最大样本数量为 1000。这可能是出于训练时间或资源限制的考虑。
>    5. `overwrite_cache: true`：表示如果缓存存在，将覆盖缓存。这意味着每次运行时都会重新处理数据集，而不是使用之前缓存的数据。
>    6. `preprocessing_num_workers: 16`：指定了用于数据预处理的工作进程数为 16。增加工作进程数可以加快数据预处理的速度，但也会消耗更多的系统资源。
> 7. **输出设置**：
> 8. 1. `output_dir: saves/qwen-7b/lora/sft`：指定了微调后的模型输出路径。训练后的模型将保存在`saves/qwen-7b/lora/sft`文件夹中。
>    2. `logging_steps: 10`：表示每 10 步记录一次训练日志，以便跟踪训练过程中的指标和进度。
>    3. `save_steps: 500`：每 500 步保存一次模型的中间状态，以便在训练过程中可以随时恢复或检查模型的进展。
>    4. `plot_loss: true`：表示绘制训练过程中的损失曲线，以便直观地观察模型的训练效果。
>    5. `overwrite_output_dir: true`：如果输出目录已经存在，将覆盖该目录。这确保了每次训练的结果都会保存在指定的输出目录中。
> 9. **训练参数**：
> 10. 1. `per_device_train_batch_size: 1`：每个设备上的训练批次大小为 1。这意味着每次只处理一个样本进行训练，通常在资源有限或模型较大时使用较小的批次大小。
>     2. `gradient_accumulation_steps: 8`：梯度累积的步数为 8。梯度累积是一种技术，通过多次前向传播和反向传播累积梯度，然后再进行一次参数更新，以等效于使用较大的批次大小进行训练。
>     3. `learning_rate: 1.0e-4`：学习率为 0.0001。学习率决定了模型在训练过程中参数更新的步长，较小的学习率可能导致训练速度较慢，但可以提高模型的稳定性和准确性。
>     4. `num_train_epochs: 3.0`：训练的轮数为 3 轮。一轮是指对整个数据集进行一次完整的遍历。
>     5. `lr_scheduler_type: cosine`：使用余弦退火（cosine annealing）的学习率调度策略。这种策略可以在训练过程中逐渐降低学习率，有助于提高模型的收敛速度和性能。
>     6. `warmup_ratio: 0.1`：热身比例为 0.1。在训练开始时，使用较小的学习率进行热身，然后逐渐增加到指定的学习率，以帮助模型更好地适应训练。
>     7. `bf16: true`：表示使用 BF16（Brain Floating Point 16）数据类型进行训练。BF16 是一种半精度浮点数格式，可以减少内存占用和计算时间，但可能会对精度产生一定影响。
>     8. `ddp_timeout: 180000000`：分布式数据并行（DDP）的超时时间为 180000000 毫秒（约 50 小时）。这是在分布式训练中等待其他进程的最长时间，如果超过这个时间，训练将被终止。
> 11. **评估设置**：
> 12. 1. `val_size: 0.1`：将数据集的 10%作为验证集。在训练过程中，会使用验证集来评估模型的性能，以便及时调整训练策略。
>     2. `per_device_eval_batch_size: 1`：每个设备上的评估批次大小为 1。与训练批次大小类似，评估时每次只处理一个样本。
>     3. `eval_strategy: steps`：按照指定的步数进行评估。这意味着在训练过程中，每隔一定的步数就会对模型进行一次评估。
>     4. `eval_steps: 500`：每 500 步进行一次评估。这与`eval_strategy`配合使用，确定了评估的频率。

从上面的配置文件中可以看到，本次微调的数据集是 identity。修改原有的json数据，**就可以微调一个属于你自己的大模型。**

https://blog.csdn.net/python12222_/article/details/143182461

### 梯度消失

- **定义**：在神经网络反向传播过程中，梯度值随着网络层数的加深而不断减小，最终趋近于 0，导致靠近输入层的神经元参数更新缓慢甚至停止更新，使得模型难以学习到数据的特征。
- **产生原因**：主要与激活函数和网络结构有关。以 Sigmoid 函数为例，其导数范围在 (0, 0.25]，当使用 Sigmoid 函数作为激活函数且网络层数较多时，经过多层连乘，梯度会变得极小。从网络结构角度，链式法则下的梯度计算，若每一层的梯度都小于 1，多层传播后梯度就会趋近于 0。
- **影响**：模型训练时间大幅增加，因为靠近输入层的参数难以更新，无法有效学习数据特征。模型的预测准确率难以提升，无法充分挖掘数据中的复杂信息，泛化能力差。
- **解决方法**：选择合适的激活函数，如 ReLU 函数，其在正向传播时，当输入大于 0，输出等于输入，导数为 1，能有效避免梯度消失。使用残差网络（ResNet），通过短路连接（shortcut connection）让梯度可以直接跨层传递，增强梯度的传播能力。

### 梯度爆炸

- **定义**：与梯度消失相反，在反向传播时，梯度值不断增大，导致参数更新幅度过大，模型变得不稳定，甚至无法收敛。
- **产生原因**：同样与激活函数和网络结构有关。某些激活函数的导数在特定情况下可能较大，多层连乘后使梯度不断放大。网络权重初始化时，如果权重值过大，也容易引发梯度爆炸。
- **影响**：模型参数更新异常，导致模型训练过程振荡，难以收敛到最优解。训练过程中损失值会突然增大，模型出现过拟合现象，在测试集上表现很差。
- **解决方法**：采用梯度裁剪技术，设置一个阈值，当梯度值超过该阈值时，对梯度进行裁剪，使其保持在合理范围内。合理初始化网络权重，如使用 Xavier 初始化或 He 初始化方法，根据网络结构和激活函数来确定合适的初始权重值。

- 学习率相关参数
  - `learning_rate: 1.0e - 4`：学习率决定了模型在训练过程中参数更新的步长。如果学习率过大，可能导致参数更新时梯度变化过于剧烈，从而引发梯度爆炸；而学习率过小，则可能使梯度在传播过程中逐渐变小，导致梯度消失，模型训练速度过慢。
  - `lr_scheduler_type: cosine`：学习率调度策略采用余弦退火。这种策略可以在训练过程中逐渐降低学习率，有助于避免梯度爆炸，同时在训练后期能让模型更精细地调整参数，防止因学习率一直较大而使梯度在后期出现异常增大或消失的情况。
  - `warmup_ratio: 0.1`：热身比例为 0.1，即在训练开始时，使用较小的学习率进行热身，然后逐渐增加到指定的学习率。热身过程可以让模型在初期以较小的梯度进行训练，避免一开始就因学习率较大而导致梯度爆炸，同时也有助于模型更好地适应训练，减少梯度消失的可能性。
- 数据类型参数
  - `bf16: true`：使用 BF16（Brain Floating Point 16）数据类型进行训练。BF16 是一种半精度浮点数格式，虽然可以减少内存占用和计算时间，但可能会对精度产生一定影响。在某些情况下，精度的降低可能会导致梯度计算出现偏差，进而影响梯度的传播和更新，增加梯度消失或爆炸的风险。不过，如果模型和数据适合这种数据类型，也可以在一定程度上提高训练效率，避免因长时间训练或大量计算导致的梯度问题。
- 训练超参数
  - `per_device_train_batch_size: 1`和`gradient_accumulation_steps: 8`：批次大小和梯度累积步数会影响梯度的计算和更新。较小的批次大小意味着每次计算的梯度可能不够准确，而通过梯度累积可以在一定程度上弥补这一点。但如果批次大小过小且累积步数不合理，可能导致梯度的估计不准确，从而引发梯度消失或爆炸。例如，如果批次大小太小，模型可能无法学习到数据中的有效特征，导致梯度不稳定；而累积步数过多，可能使梯度在累积过程中出现异常增长或衰减。
  - `num_train_epochs: 3.0`：训练轮数也与梯度问题有关。如果训练轮数过多，模型可能会过度拟合，导致梯度在后期出现不稳定的情况，甚至可能引发梯度爆炸或消失。而训练轮数过少，模型可能还没有充分学习到数据的特征，梯度也可能没有达到稳定状态。

量化参数`quantization_bit: 4`对梯度消失和梯度爆炸也可能有间接影响。量化将模型的权重量化到 4 位，减少了模型的存储和计算需求，但可能会损失一定的精度。这可能会影响梯度的计算和传播，使得梯度在更新过程中出现偏差，从而在一定程度上增加了梯度消失或爆炸的可能性。

## 模型微调的方式

**1. 按训练目标分类**

### **有监督微调（Supervised Fine-Tuning, SFT）**

有监督微调是最常见的微调方式，适用于任务明确且具有标注数据的情况。通过使用人工标注的高质量数据对，模型能够学习特定任务所需的知识，从而在指定任务上提供准确的输出。

- 适用场景：数据充足且任务目标明确的情况，如文本分类、情感分析、命名实体识别等。

**数据示例：**

```json
[
  {
    "instruction": "作为法律顾问，请回答以下问题：",  // 可选指令模板
    "input": "如果我在公司被解雇，我有权获得多少赔偿？",
    "output": "根据《劳动合同法》第47条..."
  },
  {
    "input": "签订购房合同后，开发商违约怎么办？",
    "output": "您可以依据《合同法》要求..."
  }
]
```

### **指令微调（Instruction Tuning）**

指令微调旨在增强模型理解并执行不同指令的能力。通过指令-输出对的训练，使模型能够更好地遵循人类指令，提高其在多个任务上的泛化能力。

- 适用场景：需要提升模型的泛化性和指令理解能力，如聊天机器人、自动化任务、智能问答等。

**数据示例：**

```json
[
  {
    "instruction": "用简单的语言解释量子力学。",
    "input": "",//可省略
    "output": "量子力学是研究微小粒子行为的科学..."
  },
  {
    "instruction": "将以下句子翻译成法语。",
    "input": "你好，今天天气不错。",
    "output": "Bonjour, il fait beau aujourd'hui."
  }
]
```

### **对齐方法（RLHF/DPO）**

通过人类反馈（如 Reinforcement Learning from Human Feedback, RLHF）或直接偏好优化（Direct Preference Optimization, DPO），调整模型的输出，使其更符合人类价值观，提高安全性和一致性。

- 适用场景：需要控制模型输出的安全性或风格一致性，如客服机器人、儿童内容生成、内容审核等。

**RLHF数据示例：**

```json
[
  {
    "prompt": "请写一篇关于气候变化的文章。",
    "chosen": "气候变化是人类面临的最紧迫问题之一。科学研究表明...",
    "rejected": "气候变化是媒体夸大的骗局，无需在意。",
    "score": {"chosen": 5, "rejected": 1},  // 可选：标注人工评分
    "reject_reason": "否定科学共识"          // 可选：标注拒绝原因
  }
]
```

rejected 回答应明确包含安全性或价值观问题（如危险步骤、歧视性内容），而非仅是质量差异

**DPO数据示例：**

```prolog
[
  {
    "prompt": "如何回应‘女性不适合学理科’的观点？",
    "chosen": "性别不应限制个人发展，许多女性科学家取得了卓越成就。",
    "rejected": "女性的逻辑思维确实比男性差，这是客观事实。"
  }
]
```

### **多任务学习**

通过同时优化多个相关任务，提升模型的泛化能力，使其能够高效处理多种任务。通过损失函数动态调整不同任务的训练权重。

- 适用场景：任务之间存在关联性，适用于智能助理、语音识别、情感分析等任务。

**数据示例：**

```prolog
[
  {
    "task": "情感分析",
    "input": "这款手机的电池寿命太短了，太失望了。",
    "output": "负面",
  },
  {
    "task": "文本摘要",
    "input": "近日，某科技公司发布了一款新产品...",
    "output": "某科技公司发布新品"
  }
]
```

**2. 按参数更新策略分类**

### **全参数微调（Full Fine-Tuning）**

所有模型参数都参与训练，通常需要大量计算资源，适用于数据充足、计算资源充足的情况。

- 技术代表：常规SFT、RLHF（如ChatGPT的训练方式）。

**数据示例：**

```json
{
  "model": "GPT-3",
  "trainable_parameters": "100%",
  "dataset": "500K法律文本对",
  "fine_tuning_method": "全参数微调"
}
```

### **部分冻结微调（Partial Fine-Tuning）**

仅训练模型的部分层，如冻结底层参数，仅更新高层参数，降低计算开销。

- 技术代表：如BERT冻结前8层，仅训练后4层。

**数据示例：**

```json
{
  "model": "BERT",
  "trainable_layers": "最后4层",
  "frozen_layers": "前8层",
  "fine_tuning_method": "部分冻结微调"
}
```

### **参数高效微调（PEFT）**

仅更新少量的参数，通常通过结构化方法（如LoRA、Adapter）减少计算需求，并在低资源环境下实现高效微调。

- 技术代表：LoRA（低秩适配）、Adapter（插入小网络）。

**LoRA的特点：**

- 只调整部分参数（如低秩矩阵分解）。
- 降低计算和内存开销。
- 适合快速微调，尤其在资源受限时。

**adapter的特点：**

- 插入额外的 Adapter 层
- 降低计算和内存开销。（仅训练 Adapter 层和可独立存储 Adapter 层）
- 多任务学习、迁移学习。

最后比较下：

- Adapter 插入额外的小型可训练模块，适用于多任务和迁移学习。
- LoRA 通过低秩矩阵分解，调整少量关键参数，适用于快速微调。
- 如果需要在 多个任务间切换，Adapter 更合适；如果只是对单个任务高效微调，LoRA 更优。

